{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df1f71a",
   "metadata": {},
   "source": [
    "# 파이토치\n",
    "### 이미지 처리에 특화\n",
    "### CUDA 버전 너무 높으면 다른 의존성과 충돌 - 낮은 것이 안정적일 수 있음\n",
    "### 훈련과 추론 data간에 차원을 맞춰야 함(학습 : 모델 4차원  -> 추론 : 4차원 입력)\n",
    "##### https://pytorch.org/get-started/locally/\n",
    "##### cmd : uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126(cuda 버전)\n",
    "##### cmd : nvidia-smi(cuda 버전확인)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca981840",
   "metadata": {},
   "source": [
    "### 1. 기본 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ab9e307a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "20262995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0+cu126'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "106d7bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor라는 type == array \n",
    "torch.tensor(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ff792038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "575f908b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "21a8de1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8bcf0a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1,2,3,4,5])\n",
    "torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eceb3252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 5) # 1로 채운 3 x 5개의 data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cdfebfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(512, 512) # 이미지 : 1(흰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eacd3a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(512, 512) # 이미지 : 0(검)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "27663c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(5) # 단위행렬(대각선의 요소가 1 나머지는 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a5e0fb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5eccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7, 9])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0d5a106e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3705, 0.9748, 0.9446, 0.5988, 0.1571])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f8275706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6161, 0.5458, 0.3332],\n",
       "         [0.2866, 0.1125, 0.1595],\n",
       "         [0.4735, 0.1178, 0.5041]],\n",
       "\n",
       "        [[0.0918, 0.1126, 0.3282],\n",
       "         [0.4008, 0.9338, 0.5264],\n",
       "         [0.4327, 0.8287, 0.0964]],\n",
       "\n",
       "        [[0.3693, 0.9160, 0.8084],\n",
       "         [0.3003, 0.8294, 0.2425],\n",
       "         [0.4915, 0.1954, 0.9136]]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a7436bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련과 추론 data간에 차원을 맞춰야 함\n",
    "data = torch.rand(3, 3, 3) # 3*3*3 = 27\n",
    "data.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f55073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8438, 0.1055, 0.7333, 0.8939, 0.3684, 0.4597, 0.8943, 0.3518, 0.6823],\n",
       "        [0.9227, 0.0083, 0.8861, 0.2400, 0.8006, 0.9530, 0.9965, 0.0996, 0.9467],\n",
       "        [0.2374, 0.6591, 0.1932, 0.5844, 0.4461, 0.2534, 0.3255, 0.1613, 0.8221]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(9, 3) # 9*3 = 27\n",
    "data.view(3, 9) # 3*9 = 27(행렬 전환 - 이미지 반전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6e9f9317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8438, 0.1055, 0.7333, 0.8939, 0.3684, 0.4597, 0.8943, 0.3518, 0.6823,\n",
       "        0.9227, 0.0083, 0.8861, 0.2400, 0.8006, 0.9530, 0.9965, 0.0996, 0.9467,\n",
       "        0.2374, 0.6591, 0.1932, 0.5844, 0.4461, 0.2534, 0.3255, 0.1613, 0.8221])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(27) # 3차원을 1차원으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f053d9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8438, 0.1055, 0.7333, 0.8939, 0.3684, 0.4597, 0.8943, 0.3518, 0.6823],\n",
       "        [0.9227, 0.0083, 0.8861, 0.2400, 0.8006, 0.9530, 0.9965, 0.0996, 0.9467],\n",
       "        [0.2374, 0.6591, 0.1932, 0.5844, 0.4461, 0.2534, 0.3255, 0.1613, 0.8221]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(3, -1) # 3행으로 하고 열은 자동으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "949b2eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8438, 0.1055, 0.7333, 0.8939, 0.3684, 0.4597, 0.8943, 0.3518, 0.6823,\n",
       "        0.9227, 0.0083, 0.8861, 0.2400, 0.8006, 0.9530, 0.9965, 0.0996, 0.9467,\n",
       "        0.2374, 0.6591, 0.1932, 0.5844, 0.4461, 0.2534, 0.3255, 0.1613, 0.8221])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(-1) # 1차원으로 변환(주로 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "05be3ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape # 모양"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4f46397b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dim() # 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a61e9034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype # 타입(중요)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520fca00",
   "metadata": {},
   "source": [
    "### 2. 하드웨어 설정(중요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b280f141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "50301c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4070 Laptop GPU'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de609f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "80243b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([1,2,3])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6e20dc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to('cuda') # 1. gpu에 data 올리기(model/data 모두 같은 공간에서 처리해야 함 - 파라미터 적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0b9682e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = torch.tensor([1,2,3]).cuda() # 2. gpu에 data 올리기\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "95805c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.to('cpu') # gpu에서 cpu로 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8c071615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6], device='cuda:0')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cuda() + data1 # 같은 공간에 옮겨서 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fa5f059f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.cuda()\n",
    "data - data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45203d",
   "metadata": {},
   "source": [
    "### 3. 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "62e3037e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([[1,2,3],[4,5,6]])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "97fd0650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "56e87c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([1, 2, 3]),\n",
       "indices=tensor([0, 0, 0]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min(dim=0) # 열(0)기준으로 작은 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "48679b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([1, 4]),\n",
       "indices=tensor([0, 0]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min(dim=1) # 행(1)기준으로 작은 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "df4d7faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([4, 5, 6]),\n",
       "indices=tensor([1, 1, 1]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max(dim=0) # 열(0)기준으로 큰 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e4a3b4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([3, 6]),\n",
       "indices=tensor([2, 2]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max(dim=1) # 행(1)기준으로 큰 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f68bef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e4fdb553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6adcdced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f704af9",
   "metadata": {},
   "source": [
    "### 4. 차원 편집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "36ee22af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(3, 128, 128) # 3차원 이미지\n",
    "image = image.view(1, 3, 128, 128) # 4차원 이미지\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c711eabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(3, 128, 128) \n",
    "image = image.unsqueeze(dim=0) # 맨 앞 차원 추가\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bae875af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.rand(1, 128, 128)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f4b76c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.squeeze() # 맨 앞 차원 제거\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0a79d",
   "metadata": {},
   "source": [
    "### 5. 모델 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f33f609f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/boston.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa077bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "X_train = df.iloc[:, :13].values\n",
    "\n",
    "# X_train = torch.tensor(X_train)\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "\n",
    "y_train  = df.iloc[:, -1].values # 마지막 값만 \n",
    "y_train = torch.FloatTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bfcb6e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=13, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn # 모델 생성에 필요한 함수 포함\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "# 모델 학습\n",
    "# 13 => 100 => ReLU => 50 => ReLU => 1\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 100),\n",
    "    nn.ReLU(), # 활성화 함수(비선형)\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 1)\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "894cf3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 0.weight\n",
      "Shape: torch.Size([100, 13])\n",
      "Values: Parameter containing:\n",
      "tensor([[-0.0805,  0.0714,  0.0090,  ..., -0.1843,  0.1688, -0.2331],\n",
      "        [ 0.0196,  0.0245,  0.0665,  ...,  0.2508,  0.1881, -0.0655],\n",
      "        [-0.1478,  0.1086,  0.1481,  ...,  0.1826,  0.1640, -0.1714],\n",
      "        ...,\n",
      "        [-0.1706,  0.2769,  0.1478,  ...,  0.1223,  0.2247, -0.2108],\n",
      "        [ 0.0190, -0.1167,  0.1926,  ...,  0.0661,  0.1722,  0.0315],\n",
      "        [ 0.0911,  0.2089,  0.1088,  ..., -0.1541,  0.1831,  0.1338]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name: 0.bias\n",
      "Shape: torch.Size([100])\n",
      "Values: Parameter containing:\n",
      "tensor([-0.1589, -0.0800,  0.0492,  0.0925, -0.1918, -0.2740,  0.1307, -0.1000,\n",
      "        -0.2202, -0.2457,  0.1739, -0.1071, -0.2271,  0.1189, -0.1098,  0.0338,\n",
      "        -0.0639,  0.1841, -0.1588, -0.2070, -0.2122,  0.0736, -0.2313, -0.1823,\n",
      "         0.2396,  0.2396, -0.2178,  0.2696,  0.0789, -0.1825,  0.1207,  0.0231,\n",
      "         0.0430, -0.1686,  0.1504, -0.0743, -0.1859,  0.0250,  0.0365, -0.1154,\n",
      "        -0.1000, -0.1359,  0.2618,  0.0204, -0.0232, -0.1360, -0.1123, -0.0772,\n",
      "         0.2329, -0.1097,  0.1675, -0.2267, -0.2037, -0.0615,  0.0594,  0.2433,\n",
      "         0.0424,  0.0117, -0.2336, -0.1045,  0.0297, -0.2094, -0.2383,  0.1428,\n",
      "        -0.1325,  0.1357, -0.1885, -0.2094,  0.1440, -0.1825,  0.0782, -0.0196,\n",
      "        -0.2452, -0.2516, -0.0869,  0.0743, -0.1067,  0.0072,  0.2126,  0.0092,\n",
      "        -0.1064, -0.1270,  0.0975, -0.1825, -0.0413, -0.2021,  0.1571,  0.1338,\n",
      "         0.2378, -0.0300, -0.0114,  0.2582,  0.1851,  0.2423, -0.2551, -0.1804,\n",
      "        -0.1735, -0.1849,  0.0334, -0.0711], requires_grad=True)\n",
      "\n",
      "Name: 2.weight\n",
      "Shape: torch.Size([50, 100])\n",
      "Values: Parameter containing:\n",
      "tensor([[ 0.0868, -0.0576,  0.0261,  ...,  0.0219,  0.0166, -0.0774],\n",
      "        [ 0.0258, -0.0735,  0.0126,  ..., -0.0854,  0.0434,  0.0587],\n",
      "        [-0.0310, -0.0905, -0.0100,  ..., -0.0108,  0.0880,  0.0267],\n",
      "        ...,\n",
      "        [-0.0075, -0.0056,  0.0836,  ..., -0.0046,  0.0785, -0.0374],\n",
      "        [ 0.0983,  0.0645, -0.0077,  ..., -0.0342,  0.0353,  0.0431],\n",
      "        [-0.0927, -0.0328, -0.0988,  ...,  0.0323,  0.0143,  0.0600]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name: 2.bias\n",
      "Shape: torch.Size([50])\n",
      "Values: Parameter containing:\n",
      "tensor([ 0.0454, -0.0404, -0.0306,  0.0354, -0.0021,  0.0692, -0.0640, -0.0160,\n",
      "        -0.0165,  0.0706,  0.0319, -0.0830, -0.0578,  0.0416,  0.0717, -0.0172,\n",
      "        -0.0241, -0.0571,  0.0511,  0.0837,  0.0578, -0.0297,  0.0101, -0.0541,\n",
      "         0.0069,  0.0985,  0.0942, -0.0295,  0.0821, -0.0867,  0.0525, -0.0302,\n",
      "         0.0884, -0.0790,  0.0745, -0.0115,  0.0150, -0.0688, -0.0725,  0.0204,\n",
      "         0.0701, -0.0466, -0.0532,  0.0502, -0.0029,  0.0160,  0.0467, -0.0778,\n",
      "        -0.0919, -0.0577], requires_grad=True)\n",
      "\n",
      "Name: 4.weight\n",
      "Shape: torch.Size([1, 50])\n",
      "Values: Parameter containing:\n",
      "tensor([[ 0.0900, -0.1131,  0.0330, -0.1324,  0.0194, -0.0526,  0.0828,  0.0335,\n",
      "         -0.0577,  0.1269,  0.0048, -0.1096, -0.0464, -0.1059,  0.0353, -0.1272,\n",
      "          0.0445, -0.0122, -0.1246,  0.0003, -0.0534,  0.0615,  0.0911,  0.0792,\n",
      "          0.0817, -0.1105, -0.0501,  0.1191,  0.0496,  0.0771, -0.0121, -0.0447,\n",
      "         -0.1315,  0.0285,  0.0604, -0.1035,  0.0223,  0.0885, -0.0720,  0.0498,\n",
      "         -0.0376,  0.0659, -0.1123, -0.0917,  0.0290, -0.0889,  0.0362,  0.0944,\n",
      "         -0.0108,  0.0077]], requires_grad=True)\n",
      "\n",
      "Name: 4.bias\n",
      "Shape: torch.Size([1])\n",
      "Values: Parameter containing:\n",
      "tensor([0.0531], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Values: {param}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "287de76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  86.6841812133789\n",
      "loss :  77.77656555175781\n",
      "loss :  71.03170776367188\n",
      "loss :  68.79390716552734\n",
      "loss :  67.28589630126953\n",
      "loss :  67.9419174194336\n",
      "loss :  68.44976043701172\n",
      "loss :  68.2663803100586\n",
      "loss :  68.07633972167969\n",
      "loss :  66.90066528320312\n",
      "loss :  65.7442398071289\n",
      "loss :  64.81673431396484\n",
      "loss :  63.66332244873047\n",
      "loss :  62.95154571533203\n",
      "loss :  62.73629379272461\n",
      "loss :  62.57203674316406\n",
      "loss :  62.57960510253906\n",
      "loss :  62.69703674316406\n",
      "loss :  62.55058670043945\n",
      "loss :  62.32500076293945\n",
      "loss :  62.040401458740234\n",
      "loss :  61.53632354736328\n",
      "loss :  61.0667610168457\n",
      "loss :  60.667259216308594\n",
      "loss :  60.253440856933594\n",
      "loss :  60.00657272338867\n",
      "loss :  59.85639572143555\n",
      "loss :  59.70282745361328\n",
      "loss :  59.63091278076172\n",
      "loss :  59.527000427246094\n",
      "loss :  59.36253356933594\n",
      "loss :  59.21091079711914\n",
      "loss :  58.98773956298828\n",
      "loss :  58.75733184814453\n",
      "loss :  58.558250427246094\n",
      "loss :  58.34531784057617\n",
      "loss :  58.19394302368164\n",
      "loss :  58.0656852722168\n",
      "loss :  57.94000244140625\n",
      "loss :  57.842411041259766\n",
      "loss :  57.711734771728516\n",
      "loss :  57.576297760009766\n",
      "loss :  57.42442321777344\n",
      "loss :  57.249176025390625\n",
      "loss :  57.088417053222656\n",
      "loss :  56.91769027709961\n",
      "loss :  56.76620864868164\n",
      "loss :  56.621612548828125\n",
      "loss :  56.47978591918945\n",
      "loss :  56.344970703125\n",
      "loss :  56.19432067871094\n",
      "loss :  56.04341506958008\n",
      "loss :  55.881099700927734\n",
      "loss :  55.721797943115234\n",
      "loss :  55.56663131713867\n",
      "loss :  55.413116455078125\n",
      "loss :  55.2655029296875\n",
      "loss :  55.1153450012207\n",
      "loss :  54.967979431152344\n",
      "loss :  54.814208984375\n",
      "loss :  54.659454345703125\n",
      "loss :  54.49704360961914\n",
      "loss :  54.337493896484375\n",
      "loss :  54.17451858520508\n",
      "loss :  54.01266860961914\n",
      "loss :  53.8497200012207\n",
      "loss :  53.68802261352539\n",
      "loss :  53.52292251586914\n",
      "loss :  53.354026794433594\n",
      "loss :  53.18052291870117\n",
      "loss :  53.0042724609375\n",
      "loss :  52.8272819519043\n",
      "loss :  52.6491813659668\n",
      "loss :  52.46999740600586\n",
      "loss :  52.28961181640625\n",
      "loss :  52.10624313354492\n",
      "loss :  51.92058563232422\n",
      "loss :  51.73118209838867\n",
      "loss :  51.53943634033203\n",
      "loss :  51.34544372558594\n",
      "loss :  51.150264739990234\n",
      "loss :  50.95281219482422\n",
      "loss :  50.75437927246094\n",
      "loss :  50.55238723754883\n",
      "loss :  50.34722137451172\n",
      "loss :  50.13966751098633\n",
      "loss :  49.92835235595703\n",
      "loss :  49.714351654052734\n",
      "loss :  49.496620178222656\n",
      "loss :  49.27444839477539\n",
      "loss :  49.04722595214844\n",
      "loss :  48.81587600708008\n",
      "loss :  48.58173370361328\n",
      "loss :  48.342430114746094\n",
      "loss :  48.09720993041992\n",
      "loss :  47.8476676940918\n",
      "loss :  47.59452438354492\n",
      "loss :  47.33803939819336\n",
      "loss :  47.081878662109375\n",
      "loss :  46.82488250732422\n",
      "loss :  46.563724517822266\n",
      "loss :  46.29700469970703\n",
      "loss :  46.02851867675781\n",
      "loss :  45.75707244873047\n",
      "loss :  45.48148727416992\n",
      "loss :  45.202579498291016\n",
      "loss :  44.92261505126953\n",
      "loss :  44.64005661010742\n",
      "loss :  44.35317611694336\n",
      "loss :  44.06380844116211\n",
      "loss :  43.77069854736328\n",
      "loss :  43.4760627746582\n",
      "loss :  43.178871154785156\n",
      "loss :  42.87594985961914\n",
      "loss :  42.5692253112793\n",
      "loss :  42.26140213012695\n",
      "loss :  41.96300506591797\n",
      "loss :  41.6686897277832\n",
      "loss :  41.406009674072266\n",
      "loss :  41.28475570678711\n",
      "loss :  41.51824188232422\n",
      "loss :  41.83417892456055\n",
      "loss :  40.84917068481445\n",
      "loss :  39.97658157348633\n",
      "loss :  40.508338928222656\n",
      "loss :  39.953243255615234\n",
      "loss :  39.24526596069336\n",
      "loss :  39.618499755859375\n",
      "loss :  38.97026824951172\n",
      "loss :  38.62580871582031\n",
      "loss :  38.77933120727539\n",
      "loss :  38.094242095947266\n",
      "loss :  38.039424896240234\n",
      "loss :  37.9091911315918\n",
      "loss :  37.36188507080078\n",
      "loss :  37.40106964111328\n",
      "loss :  37.07449722290039\n",
      "loss :  36.70392990112305\n",
      "loss :  36.69096755981445\n",
      "loss :  36.297611236572266\n",
      "loss :  36.035377502441406\n",
      "loss :  35.978424072265625\n",
      "loss :  35.615352630615234\n",
      "loss :  35.36752700805664\n",
      "loss :  35.25801467895508\n",
      "loss :  34.93621826171875\n",
      "loss :  34.675872802734375\n",
      "loss :  34.543582916259766\n",
      "loss :  34.26007843017578\n",
      "loss :  33.96985626220703\n",
      "loss :  33.810001373291016\n",
      "loss :  33.60207748413086\n",
      "loss :  33.324195861816406\n",
      "loss :  33.13815689086914\n",
      "loss :  32.99904251098633\n",
      "loss :  32.77432632446289\n",
      "loss :  32.52131652832031\n",
      "loss :  32.32747268676758\n",
      "loss :  32.1514778137207\n",
      "loss :  31.95089340209961\n",
      "loss :  31.744930267333984\n",
      "loss :  31.56406021118164\n",
      "loss :  31.40227508544922\n",
      "loss :  31.225788116455078\n",
      "loss :  31.028919219970703\n",
      "loss :  30.83353614807129\n",
      "loss :  30.655960083007812\n",
      "loss :  30.496978759765625\n",
      "loss :  30.343814849853516\n",
      "loss :  30.188655853271484\n",
      "loss :  30.03138542175293\n",
      "loss :  29.873947143554688\n",
      "loss :  29.721647262573242\n",
      "loss :  29.581148147583008\n",
      "loss :  29.457319259643555\n",
      "loss :  29.360727310180664\n",
      "loss :  29.312089920043945\n",
      "loss :  29.34092140197754\n",
      "loss :  29.463184356689453\n",
      "loss :  29.543203353881836\n",
      "loss :  29.368730545043945\n",
      "loss :  28.78900146484375\n",
      "loss :  28.331167221069336\n",
      "loss :  28.336061477661133\n",
      "loss :  28.538944244384766\n",
      "loss :  28.503093719482422\n",
      "loss :  28.092334747314453\n",
      "loss :  27.77250099182129\n",
      "loss :  27.801361083984375\n",
      "loss :  27.91927146911621\n",
      "loss :  27.7921199798584\n",
      "loss :  27.472646713256836\n",
      "loss :  27.277301788330078\n",
      "loss :  27.302139282226562\n",
      "loss :  27.33881378173828\n",
      "loss :  27.202207565307617\n",
      "loss :  26.965532302856445\n",
      "loss :  26.82685661315918\n",
      "loss :  26.82773208618164\n",
      "loss :  26.827939987182617\n"
     ]
    }
   ],
   "source": [
    "optim = Adam(model.parameters(), lr=0.001) # 최적화(탐색 크기 및 속도)함수 선택\n",
    "criterion = nn.MSELoss() # 손실함수 선택\n",
    "epochs = 200 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    optim.zero_grad() # 기울기 값을 0으로 초기화(누적 방지)\n",
    "    \n",
    "    y_pred = model(X_train) # 예측\n",
    "    \n",
    "    loss = criterion(y_pred, y_train.view(-1, 1)) # loss 값 계산\n",
    "    \n",
    "    loss.backward() # 역전파\n",
    "    \n",
    "    optim.step() # 가중치 업데이트(다음 진행)\n",
    "    \n",
    "    print('loss : ', loss.item()) # loss 값 확인(계속 감소하면 epoch 증가 고려)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e67e3c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.1006,  0.0876,  0.0139,  ..., -0.1730,  0.1771, -0.2250],\n",
       "                      [ 0.0084,  0.0392,  0.0677,  ...,  0.2574,  0.1922, -0.0617],\n",
       "                      [-0.1491,  0.1210,  0.1543,  ...,  0.1914,  0.1705, -0.1639],\n",
       "                      ...,\n",
       "                      [-0.1831,  0.2893,  0.1500,  ...,  0.1290,  0.2292, -0.2064],\n",
       "                      [-0.0207, -0.0945,  0.2033,  ...,  0.0848,  0.1846,  0.0475],\n",
       "                      [ 0.0924,  0.2207,  0.1152,  ..., -0.1455,  0.1909,  0.1412]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.1442, -0.0709,  0.0597,  0.0925, -0.2023, -0.2706,  0.1244, -0.1004,\n",
       "                      -0.2067, -0.2560,  0.1649, -0.1019, -0.2089,  0.1189, -0.0834,  0.0254,\n",
       "                      -0.0745,  0.1803, -0.1687, -0.1973, -0.2122,  0.0911, -0.2279, -0.1823,\n",
       "                       0.2351,  0.2284, -0.1590,  0.2718,  0.0551, -0.1096,  0.1248,  0.0330,\n",
       "                       0.0353, -0.1686,  0.1504, -0.0707, -0.1859,  0.0250,  0.0271, -0.1154,\n",
       "                      -0.1134, -0.1359,  0.2759,  0.0298, -0.0378, -0.1474, -0.1202, -0.0772,\n",
       "                       0.2329, -0.1097,  0.1541, -0.2394, -0.2154, -0.0615,  0.0487,  0.2433,\n",
       "                       0.0522,  0.0117, -0.2350, -0.1056,  0.0317, -0.2188, -0.2383,  0.1353,\n",
       "                      -0.1090,  0.1357, -0.2023, -0.2214,  0.1440, -0.1926,  0.0672, -0.0291,\n",
       "                      -0.2524, -0.2612, -0.0869,  0.0899, -0.0617, -0.0051,  0.2126,  0.0092,\n",
       "                      -0.0979, -0.1200,  0.0975, -0.1893, -0.0413, -0.2120,  0.1679,  0.1462,\n",
       "                       0.2287, -0.0282, -0.0068,  0.2428,  0.1735,  0.2423, -0.2571, -0.1728,\n",
       "                      -0.1653, -0.1760,  0.0570, -0.0615])),\n",
       "             ('2.weight',\n",
       "              tensor([[ 0.0817, -0.0628,  0.0210,  ...,  0.0219,  0.0166, -0.0826],\n",
       "                      [ 0.0188, -0.0805,  0.0056,  ..., -0.0854,  0.0434,  0.0517],\n",
       "                      [-0.0310, -0.0905, -0.0100,  ..., -0.0108,  0.0880,  0.0267],\n",
       "                      ...,\n",
       "                      [-0.0075, -0.0056,  0.0836,  ..., -0.0046,  0.0785, -0.0374],\n",
       "                      [ 0.0897,  0.0560, -0.0164,  ..., -0.0428,  0.0267,  0.0345],\n",
       "                      [-0.0927, -0.0328, -0.0988,  ...,  0.0323,  0.0143,  0.0600]])),\n",
       "             ('2.bias',\n",
       "              tensor([ 0.0403, -0.0475, -0.0306,  0.0247,  0.0081,  0.0579, -0.0640, -0.0053,\n",
       "                      -0.0278,  0.0706,  0.0319, -0.0920, -0.0578,  0.0356,  0.0824, -0.0234,\n",
       "                      -0.0134, -0.0571,  0.0409,  0.0874,  0.0509, -0.0187,  0.0211, -0.0430,\n",
       "                       0.0069,  0.0905,  0.0942, -0.0347,  0.0821, -0.0548,  0.0525, -0.0302,\n",
       "                       0.0884,  0.0447,  0.0745, -0.0214,  0.0150, -0.0805, -0.0839,  0.0207,\n",
       "                       0.0609, -0.0466, -0.0532,  0.0394,  0.0072,  0.0062,  0.0410, -0.0778,\n",
       "                      -0.1005, -0.0577])),\n",
       "             ('4.weight',\n",
       "              tensor([[ 0.0848, -0.1060,  0.0330, -0.1233,  0.0216, -0.0455,  0.0828,  0.0367,\n",
       "                       -0.0472,  0.1269,  0.0048, -0.1005, -0.0464, -0.0999,  0.0379, -0.1212,\n",
       "                        0.0463, -0.0122, -0.1146,  0.0044, -0.0474,  0.0648,  0.0919,  0.0816,\n",
       "                        0.0817, -0.1011, -0.0501,  0.1144,  0.0496,  0.0625, -0.0121, -0.0447,\n",
       "                       -0.1315,  0.0827,  0.0604, -0.0938,  0.0223,  0.0768, -0.0655,  0.0431,\n",
       "                       -0.0289,  0.0659, -0.1123, -0.0826,  0.0285, -0.0855,  0.0304,  0.0944,\n",
       "                       -0.0034,  0.0077]])),\n",
       "             ('4.bias', tensor([0.0644]))])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "77c2d7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  43.22514724731445\n",
      "loss :  124.25946807861328\n",
      "loss :  133.8849334716797\n",
      "loss :  96.34406280517578\n",
      "loss :  96.43588256835938\n",
      "loss :  61.28706741333008\n",
      "loss :  94.13752746582031\n",
      "loss :  104.20698547363281\n",
      "loss :  98.22016906738281\n",
      "loss :  41.66969299316406\n",
      "loss :  39.087249755859375\n",
      "loss :  96.01577758789062\n",
      "loss :  113.0343246459961\n",
      "loss :  97.66802978515625\n",
      "loss :  39.473846435546875\n",
      "loss :  49.48249435424805\n",
      "loss :  94.46086883544922\n",
      "loss :  93.47787475585938\n",
      "loss :  99.20034790039062\n",
      "loss :  42.14739990234375\n",
      "loss :  45.523319244384766\n",
      "loss :  92.48186492919922\n",
      "loss :  112.20755004882812\n",
      "loss :  95.83422088623047\n",
      "loss :  32.702144622802734\n",
      "loss :  39.57686233520508\n",
      "loss :  93.03163146972656\n",
      "loss :  110.5262222290039\n",
      "loss :  93.8543930053711\n",
      "loss :  41.547908782958984\n",
      "loss :  44.547149658203125\n",
      "loss :  91.92891693115234\n",
      "loss :  103.3098373413086\n",
      "loss :  94.24685668945312\n",
      "loss :  38.37900161743164\n",
      "loss :  42.073997497558594\n",
      "loss :  92.34278869628906\n",
      "loss :  111.3625717163086\n",
      "loss :  94.66858673095703\n",
      "loss :  34.83001708984375\n",
      "loss :  41.88148880004883\n",
      "loss :  92.0809326171875\n",
      "loss :  105.01656341552734\n",
      "loss :  94.77812194824219\n",
      "loss :  38.90059280395508\n",
      "loss :  44.550724029541016\n",
      "loss :  91.88037109375\n",
      "loss :  105.23649597167969\n",
      "loss :  94.47549438476562\n",
      "loss :  36.262325286865234\n",
      "loss :  41.96953201293945\n",
      "loss :  91.80243682861328\n",
      "loss :  109.20953369140625\n",
      "loss :  94.21640014648438\n",
      "loss :  36.4377326965332\n",
      "loss :  42.473995208740234\n",
      "loss :  91.3572006225586\n",
      "loss :  106.4875717163086\n",
      "loss :  94.14268493652344\n",
      "loss :  37.87294006347656\n",
      "loss :  42.73283767700195\n",
      "loss :  91.23284149169922\n",
      "loss :  108.4155502319336\n",
      "loss :  94.31344604492188\n",
      "loss :  35.9133186340332\n",
      "loss :  42.101985931396484\n",
      "loss :  91.16373443603516\n",
      "loss :  107.67105865478516\n",
      "loss :  94.45514678955078\n",
      "loss :  36.800819396972656\n",
      "loss :  43.06187438964844\n",
      "loss :  91.17247009277344\n",
      "loss :  106.86844635009766\n",
      "loss :  94.6027603149414\n",
      "loss :  36.24418640136719\n",
      "loss :  42.5779914855957\n",
      "loss :  91.14373779296875\n",
      "loss :  107.95052337646484\n",
      "loss :  94.67346954345703\n",
      "loss :  36.072391510009766\n",
      "loss :  42.67174530029297\n",
      "loss :  91.1432113647461\n",
      "loss :  107.09564971923828\n",
      "loss :  94.65594482421875\n",
      "loss :  36.51959991455078\n",
      "loss :  42.8243293762207\n",
      "loss :  91.14666748046875\n",
      "loss :  107.44834899902344\n",
      "loss :  94.76956176757812\n",
      "loss :  36.09925079345703\n",
      "loss :  42.687774658203125\n",
      "loss :  91.18826293945312\n",
      "loss :  107.0690689086914\n",
      "loss :  94.79000091552734\n",
      "loss :  36.477783203125\n",
      "loss :  42.98961639404297\n",
      "loss :  91.20291137695312\n",
      "loss :  106.8448486328125\n",
      "loss :  94.77503204345703\n",
      "loss :  36.37693786621094\n",
      "loss :  42.81955337524414\n",
      "loss :  91.195556640625\n",
      "loss :  107.21487426757812\n",
      "loss :  94.69729614257812\n",
      "loss :  36.42108154296875\n",
      "loss :  42.80517578125\n",
      "loss :  91.16578674316406\n",
      "loss :  107.16455841064453\n",
      "loss :  94.622802734375\n",
      "loss :  36.519622802734375\n",
      "loss :  42.78118133544922\n",
      "loss :  91.15010070800781\n",
      "loss :  107.38118743896484\n",
      "loss :  94.6089859008789\n",
      "loss :  36.41246795654297\n",
      "loss :  42.741065979003906\n",
      "loss :  91.14281463623047\n",
      "loss :  107.30209350585938\n",
      "loss :  94.59699249267578\n",
      "loss :  36.485740661621094\n",
      "loss :  42.79899215698242\n",
      "loss :  91.14038848876953\n",
      "loss :  107.29257202148438\n",
      "loss :  94.60948944091797\n",
      "loss :  36.425235748291016\n",
      "loss :  42.76649856567383\n",
      "loss :  91.13699340820312\n",
      "loss :  107.33112335205078\n",
      "loss :  94.60594940185547\n",
      "loss :  36.436161041259766\n",
      "loss :  42.783199310302734\n",
      "loss :  91.13734436035156\n",
      "loss :  107.28250885009766\n",
      "loss :  94.636962890625\n",
      "loss :  36.411781311035156\n",
      "loss :  42.820106506347656\n",
      "loss :  91.14488220214844\n",
      "loss :  107.1502456665039\n",
      "loss :  94.65924072265625\n",
      "loss :  36.442161560058594\n",
      "loss :  42.854122161865234\n",
      "loss :  91.14688110351562\n",
      "loss :  107.1293716430664\n",
      "loss :  94.66173553466797\n",
      "loss :  36.43718719482422\n",
      "loss :  42.83266067504883\n",
      "loss :  91.14299011230469\n",
      "loss :  107.18952178955078\n",
      "loss :  94.65225982666016\n",
      "loss :  36.43255615234375\n",
      "loss :  42.83521270751953\n",
      "loss :  91.13249206542969\n",
      "loss :  107.17242431640625\n",
      "loss :  94.62447357177734\n",
      "loss :  36.441558837890625\n",
      "loss :  42.83009719848633\n",
      "loss :  91.13025665283203\n",
      "loss :  107.1745834350586\n",
      "loss :  94.63715362548828\n",
      "loss :  36.41292953491211\n",
      "loss :  42.849815368652344\n",
      "loss :  91.13240051269531\n",
      "loss :  107.04574584960938\n",
      "loss :  94.64131164550781\n",
      "loss :  36.462379455566406\n",
      "loss :  42.870628356933594\n",
      "loss :  91.13872528076172\n",
      "loss :  107.06837463378906\n",
      "loss :  94.66355895996094\n",
      "loss :  36.411861419677734\n",
      "loss :  42.870025634765625\n",
      "loss :  91.1449203491211\n",
      "loss :  106.96328735351562\n",
      "loss :  94.6629638671875\n",
      "loss :  36.47256088256836\n",
      "loss :  42.905025482177734\n",
      "loss :  91.14187622070312\n",
      "loss :  106.97566223144531\n",
      "loss :  94.6489486694336\n",
      "loss :  36.435211181640625\n",
      "loss :  42.875526428222656\n",
      "loss :  91.14131927490234\n",
      "loss :  106.95381164550781\n",
      "loss :  94.64297485351562\n",
      "loss :  36.464019775390625\n",
      "loss :  42.9005012512207\n",
      "loss :  91.13287353515625\n",
      "loss :  106.91374969482422\n",
      "loss :  94.59847259521484\n",
      "loss :  36.50862503051758\n",
      "loss :  42.84494400024414\n",
      "loss :  91.13597869873047\n",
      "loss :  107.08206939697266\n",
      "loss :  94.64070129394531\n",
      "loss :  36.396697998046875\n",
      "loss :  42.90564727783203\n",
      "loss :  91.13005065917969\n",
      "loss :  106.72852325439453\n",
      "loss :  94.57333374023438\n",
      "loss :  36.60124969482422\n",
      "loss :  42.88119888305664\n",
      "loss :  91.12891387939453\n",
      "loss :  107.13542175292969\n",
      "loss :  94.63720703125\n",
      "loss :  36.30698013305664\n",
      "loss :  42.843353271484375\n",
      "loss :  91.12663269042969\n",
      "loss :  106.74046325683594\n",
      "loss :  94.58285522460938\n",
      "loss :  36.60668182373047\n",
      "loss :  42.93593215942383\n",
      "loss :  91.12759399414062\n",
      "loss :  106.98766326904297\n",
      "loss :  94.6460189819336\n",
      "loss :  36.30049133300781\n",
      "loss :  42.867122650146484\n",
      "loss :  91.11808013916016\n",
      "loss :  106.68904876708984\n",
      "loss :  94.52767944335938\n",
      "loss :  36.6524658203125\n",
      "loss :  42.873382568359375\n",
      "loss :  91.11503601074219\n",
      "loss :  107.19055938720703\n",
      "loss :  94.62798309326172\n",
      "loss :  36.21501922607422\n",
      "loss :  42.834922790527344\n",
      "loss :  91.11774444580078\n",
      "loss :  106.54552459716797\n",
      "loss :  94.55174255371094\n",
      "loss :  36.69111633300781\n",
      "loss :  42.95647048950195\n",
      "loss :  91.12230682373047\n",
      "loss :  107.03172302246094\n",
      "loss :  94.64937591552734\n",
      "loss :  36.19921112060547\n",
      "loss :  42.84226989746094\n",
      "loss :  91.10647583007812\n",
      "loss :  106.53614807128906\n",
      "loss :  94.46410369873047\n",
      "loss :  36.79386520385742\n",
      "loss :  42.86573791503906\n",
      "loss :  91.10001373291016\n",
      "loss :  107.35189819335938\n",
      "loss :  94.62690734863281\n",
      "loss :  36.071956634521484\n",
      "loss :  42.801300048828125\n",
      "loss :  91.11109161376953\n",
      "loss :  106.27600860595703\n",
      "loss :  94.52413177490234\n",
      "loss :  36.83919906616211\n",
      "loss :  43.02875900268555\n",
      "loss :  91.11508178710938\n",
      "loss :  107.02349853515625\n",
      "loss :  94.65220642089844\n",
      "loss :  36.074981689453125\n",
      "loss :  42.77195739746094\n",
      "loss :  91.10750579833984\n",
      "loss :  106.47074890136719\n",
      "loss :  94.49223327636719\n",
      "loss :  36.77522277832031\n",
      "loss :  42.96406555175781\n",
      "loss :  91.10787200927734\n",
      "loss :  107.016845703125\n",
      "loss :  94.66193389892578\n",
      "loss :  36.067020416259766\n",
      "loss :  42.82838821411133\n",
      "loss :  91.10720825195312\n",
      "loss :  106.28389739990234\n",
      "loss :  94.48573303222656\n",
      "loss :  36.799381256103516\n",
      "loss :  42.93724822998047\n",
      "loss :  91.10963439941406\n",
      "loss :  107.13164520263672\n",
      "loss :  94.66777801513672\n",
      "loss :  36.010948181152344\n",
      "loss :  42.8365364074707\n",
      "loss :  91.09925842285156\n",
      "loss :  106.05512237548828\n",
      "loss :  94.41108703613281\n",
      "loss :  36.99736785888672\n",
      "loss :  42.90996551513672\n",
      "loss :  91.0989761352539\n",
      "loss :  107.41693115234375\n",
      "loss :  94.65314483642578\n",
      "loss :  35.856475830078125\n",
      "loss :  42.74186325073242\n",
      "loss :  91.09847259521484\n",
      "loss :  105.92532348632812\n",
      "loss :  94.44913482666016\n",
      "loss :  37.07843017578125\n",
      "loss :  43.05751419067383\n",
      "loss :  91.11182403564453\n",
      "loss :  107.13113403320312\n",
      "loss :  94.70489501953125\n",
      "loss :  35.85898208618164\n",
      "loss :  42.781166076660156\n",
      "loss :  91.08879852294922\n",
      "loss :  105.87437438964844\n",
      "loss :  94.32343292236328\n",
      "loss :  37.25893783569336\n",
      "loss :  42.879547119140625\n",
      "loss :  91.11566162109375\n",
      "loss :  107.6529312133789\n",
      "loss :  94.81314849853516\n",
      "loss :  35.64129638671875\n",
      "loss :  42.85625457763672\n",
      "loss :  91.12167358398438\n",
      "loss :  105.04719543457031\n",
      "loss :  94.46885681152344\n",
      "loss :  37.6124267578125\n",
      "loss :  43.164913177490234\n",
      "loss :  91.11922454833984\n",
      "loss :  107.67313385009766\n",
      "loss :  94.7567367553711\n",
      "loss :  35.52074432373047\n",
      "loss :  42.582733154296875\n",
      "loss :  91.07225036621094\n",
      "loss :  105.55461883544922\n",
      "loss :  94.22273254394531\n",
      "loss :  37.79682159423828\n",
      "loss :  42.9788703918457\n",
      "loss :  91.11649322509766\n",
      "loss :  107.89681243896484\n",
      "loss :  94.92170715332031\n",
      "loss :  35.48326110839844\n",
      "loss :  42.811492919921875\n",
      "loss :  91.11478424072266\n",
      "loss :  104.4796371459961\n",
      "loss :  94.40165710449219\n",
      "loss :  38.336639404296875\n",
      "loss :  43.19213104248047\n",
      "loss :  91.1444091796875\n",
      "loss :  108.25162506103516\n",
      "loss :  94.94232177734375\n",
      "loss :  35.26643753051758\n",
      "loss :  42.550437927246094\n",
      "loss :  91.07598114013672\n",
      "loss :  104.66852569580078\n",
      "loss :  94.20710754394531\n",
      "loss :  38.71131896972656\n",
      "loss :  43.12535095214844\n",
      "loss :  91.12735748291016\n",
      "loss :  108.57149505615234\n",
      "loss :  95.12168884277344\n",
      "loss :  35.254920959472656\n",
      "loss :  42.646453857421875\n",
      "loss :  91.10507202148438\n",
      "loss :  103.7798843383789\n",
      "loss :  94.35884094238281\n",
      "loss :  39.51777267456055\n",
      "loss :  43.3143310546875\n",
      "loss :  91.15467071533203\n",
      "loss :  109.01304626464844\n",
      "loss :  95.20589447021484\n",
      "loss :  35.0604133605957\n",
      "loss :  42.402015686035156\n",
      "loss :  91.08784484863281\n",
      "loss :  103.7645492553711\n",
      "loss :  94.27558135986328\n",
      "loss :  39.931373596191406\n",
      "loss :  43.39556884765625\n",
      "loss :  91.12555694580078\n",
      "loss :  109.13260650634766\n",
      "loss :  95.28101348876953\n",
      "loss :  35.071163177490234\n",
      "loss :  42.32889175415039\n",
      "loss :  91.10274505615234\n",
      "loss :  103.46253204345703\n",
      "loss :  94.35424041748047\n",
      "loss :  40.36162185668945\n",
      "loss :  43.557464599609375\n",
      "loss :  91.13533020019531\n",
      "loss :  109.0600357055664\n",
      "loss :  95.25981140136719\n",
      "loss :  35.017974853515625\n",
      "loss :  42.20142364501953\n",
      "loss :  91.09685516357422\n",
      "loss :  103.646484375\n",
      "loss :  94.2703628540039\n",
      "loss :  40.42660903930664\n",
      "loss :  43.555908203125\n",
      "loss :  91.11199951171875\n",
      "loss :  109.00453186035156\n",
      "loss :  95.24516296386719\n",
      "loss :  34.985504150390625\n",
      "loss :  42.17591857910156\n",
      "loss :  91.1053237915039\n",
      "loss :  103.60578155517578\n",
      "loss :  94.31307220458984\n",
      "loss :  40.32115173339844\n",
      "loss :  43.642906188964844\n",
      "loss :  91.10076141357422\n",
      "loss :  108.71328735351562\n",
      "loss :  95.15283966064453\n",
      "loss :  34.88629913330078\n",
      "loss :  42.10408020019531\n",
      "loss :  91.09239196777344\n",
      "loss :  103.96868133544922\n",
      "loss :  94.24769592285156\n",
      "loss :  39.85911560058594\n",
      "loss :  43.60763168334961\n",
      "loss :  91.08698272705078\n",
      "loss :  108.28556060791016\n",
      "loss :  95.0548324584961\n",
      "loss :  34.98300552368164\n",
      "loss :  42.19464111328125\n",
      "loss :  91.09725189208984\n",
      "loss :  104.0686264038086\n",
      "loss :  94.2364501953125\n",
      "loss :  39.520225524902344\n",
      "loss :  43.53289794921875\n",
      "loss :  91.0757064819336\n",
      "loss :  108.1042251586914\n",
      "loss :  94.91222381591797\n",
      "loss :  34.96482849121094\n",
      "loss :  42.184268951416016\n",
      "loss :  91.07178497314453\n",
      "loss :  104.44950103759766\n",
      "loss :  94.13379669189453\n",
      "loss :  39.012821197509766\n",
      "loss :  43.39006805419922\n",
      "loss :  91.06893157958984\n",
      "loss :  107.86912536621094\n",
      "loss :  94.88114929199219\n",
      "loss :  35.063255310058594\n",
      "loss :  42.33944320678711\n",
      "loss :  91.07266235351562\n",
      "loss :  104.27167510986328\n",
      "loss :  94.2057113647461\n",
      "loss :  38.846736907958984\n",
      "loss :  43.366580963134766\n",
      "loss :  91.07986450195312\n",
      "loss :  107.74287414550781\n",
      "loss :  94.87693786621094\n",
      "loss :  35.071285247802734\n",
      "loss :  42.38795471191406\n",
      "loss :  91.06206512451172\n",
      "loss :  104.2902603149414\n",
      "loss :  94.18282318115234\n",
      "loss :  38.633846282958984\n",
      "loss :  43.31231689453125\n",
      "loss :  91.05841064453125\n",
      "loss :  107.71048736572266\n",
      "loss :  94.80490112304688\n",
      "loss :  35.092472076416016\n",
      "loss :  42.352535247802734\n",
      "loss :  91.04840087890625\n",
      "loss :  104.43685913085938\n",
      "loss :  94.12936401367188\n",
      "loss :  38.53879928588867\n",
      "loss :  43.22283935546875\n",
      "loss :  91.06913757324219\n",
      "loss :  107.67692565917969\n",
      "loss :  94.85070037841797\n",
      "loss :  35.120750427246094\n",
      "loss :  42.467674255371094\n",
      "loss :  91.06070709228516\n",
      "loss :  104.04940795898438\n",
      "loss :  94.21336364746094\n",
      "loss :  38.65097427368164\n",
      "loss :  43.30319595336914\n",
      "loss :  91.08462524414062\n",
      "loss :  107.65265655517578\n",
      "loss :  94.8526382446289\n",
      "loss :  35.080238342285156\n",
      "loss :  42.41809844970703\n",
      "loss :  91.0488510131836\n",
      "loss :  104.11723327636719\n",
      "loss :  94.14891052246094\n",
      "loss :  38.64004898071289\n",
      "loss :  43.259986877441406\n",
      "loss :  91.0608901977539\n",
      "loss :  107.70381164550781\n",
      "loss :  94.81407165527344\n",
      "loss :  35.09698486328125\n",
      "loss :  42.38520431518555\n",
      "loss :  91.04010772705078\n",
      "loss :  104.10969543457031\n",
      "loss :  94.11366271972656\n",
      "loss :  38.69463348388672\n",
      "loss :  43.211368560791016\n",
      "loss :  91.08660125732422\n",
      "loss :  107.71817016601562\n",
      "loss :  94.92887115478516\n",
      "loss :  35.108062744140625\n",
      "loss :  42.50288009643555\n",
      "loss :  91.05847930908203\n",
      "loss :  103.62293243408203\n",
      "loss :  94.18810272216797\n",
      "loss :  39.00783157348633\n",
      "loss :  43.274654388427734\n",
      "loss :  91.08531951904297\n",
      "loss :  107.99308776855469\n",
      "loss :  94.9218521118164\n",
      "loss :  35.02741622924805\n",
      "loss :  42.350677490234375\n",
      "loss :  91.04270935058594\n",
      "loss :  103.66751861572266\n",
      "loss :  94.1010513305664\n",
      "loss :  39.21019744873047\n",
      "loss :  43.26743698120117\n",
      "loss :  91.08863067626953\n",
      "loss :  108.04158782958984\n",
      "loss :  95.0025863647461\n",
      "loss :  35.030067443847656\n",
      "loss :  42.38482666015625\n",
      "loss :  91.07118225097656\n",
      "loss :  103.24837493896484\n",
      "loss :  94.2361831665039\n",
      "loss :  39.51496124267578\n",
      "loss :  43.44908142089844\n",
      "loss :  91.07799530029297\n",
      "loss :  108.01210021972656\n",
      "loss :  94.90186309814453\n",
      "loss :  34.884002685546875\n",
      "loss :  42.14669418334961\n",
      "loss :  91.03723907470703\n",
      "loss :  103.88815307617188\n",
      "loss :  94.00421142578125\n",
      "loss :  39.28166198730469\n",
      "loss :  43.269649505615234\n",
      "loss :  91.08098602294922\n",
      "loss :  107.87504577636719\n",
      "loss :  95.03260803222656\n",
      "loss :  35.077842712402344\n",
      "loss :  42.42512130737305\n",
      "loss :  91.10975646972656\n",
      "loss :  102.96564483642578\n",
      "loss :  94.32522583007812\n",
      "loss :  39.60315704345703\n",
      "loss :  43.56029510498047\n",
      "loss :  91.11026000976562\n",
      "loss :  107.7397689819336\n",
      "loss :  94.85700988769531\n",
      "loss :  34.810176849365234\n",
      "loss :  42.10398483276367\n",
      "loss :  91.0408706665039\n",
      "loss :  103.99281311035156\n",
      "loss :  94.01526641845703\n",
      "loss :  38.992671966552734\n",
      "loss :  43.36941909790039\n",
      "loss :  91.03844451904297\n",
      "loss :  107.45257568359375\n",
      "loss :  94.7744369506836\n",
      "loss :  35.01905059814453\n",
      "loss :  42.24040603637695\n",
      "loss :  91.03852844238281\n",
      "loss :  103.85320281982422\n",
      "loss :  94.06010437011719\n",
      "loss :  38.969600677490234\n",
      "loss :  43.25291061401367\n",
      "loss :  91.07179260253906\n",
      "loss :  107.47692108154297\n",
      "loss :  94.87255859375\n",
      "loss :  34.998592376708984\n",
      "loss :  42.394222259521484\n",
      "loss :  91.0426025390625\n",
      "loss :  103.47918701171875\n",
      "loss :  94.12721252441406\n",
      "loss :  38.80835723876953\n",
      "loss :  43.309776306152344\n",
      "loss :  91.0519790649414\n",
      "loss :  107.46808624267578\n",
      "loss :  94.79549407958984\n",
      "loss :  34.93004608154297\n",
      "loss :  42.259803771972656\n",
      "loss :  91.02533721923828\n",
      "loss :  103.77749633789062\n",
      "loss :  94.0265121459961\n",
      "loss :  38.87091827392578\n",
      "loss :  43.21299743652344\n",
      "loss :  91.07184600830078\n",
      "loss :  107.4135971069336\n",
      "loss :  94.90541076660156\n",
      "loss :  35.02287292480469\n",
      "loss :  42.44652557373047\n",
      "loss :  91.04864501953125\n",
      "loss :  103.20259857177734\n",
      "loss :  94.15228271484375\n",
      "loss :  39.01813888549805\n",
      "loss :  43.30284881591797\n",
      "loss :  91.07176971435547\n",
      "loss :  107.5777587890625\n",
      "loss :  94.85123443603516\n",
      "loss :  34.88178634643555\n",
      "loss :  42.25285339355469\n",
      "loss :  91.0298843383789\n",
      "loss :  103.4952163696289\n",
      "loss :  94.03679656982422\n",
      "loss :  38.981895446777344\n",
      "loss :  43.27265548706055\n",
      "loss :  91.06596374511719\n",
      "loss :  107.40972137451172\n",
      "loss :  94.87557220458984\n",
      "loss :  34.973140716552734\n",
      "loss :  42.33091735839844\n",
      "loss :  91.04178619384766\n",
      "loss :  103.2478256225586\n",
      "loss :  94.08580017089844\n",
      "loss :  39.17950439453125\n",
      "loss :  43.28165054321289\n",
      "loss :  91.06938171386719\n",
      "loss :  107.55514526367188\n",
      "loss :  94.84892272949219\n",
      "loss :  34.92686080932617\n",
      "loss :  42.25257873535156\n",
      "loss :  91.03163146972656\n",
      "loss :  103.30863189697266\n",
      "loss :  94.01847839355469\n",
      "loss :  39.18085479736328\n",
      "loss :  43.24408721923828\n",
      "loss :  91.0849609375\n",
      "loss :  107.51627349853516\n",
      "loss :  94.95630645751953\n",
      "loss :  34.96513748168945\n",
      "loss :  42.35683822631836\n",
      "loss :  91.07926177978516\n",
      "loss :  102.79290771484375\n",
      "loss :  94.21426391601562\n",
      "loss :  39.38123321533203\n",
      "loss :  43.480125427246094\n",
      "loss :  91.08199310302734\n",
      "loss :  107.34745025634766\n",
      "loss :  94.78671264648438\n",
      "loss :  34.82086181640625\n",
      "loss :  42.0909538269043\n",
      "loss :  91.0297622680664\n",
      "loss :  103.66938781738281\n",
      "loss :  93.93795013427734\n",
      "loss :  38.94131088256836\n",
      "loss :  43.25233459472656\n",
      "loss :  91.04660034179688\n",
      "loss :  107.16085052490234\n",
      "loss :  94.81101989746094\n",
      "loss :  35.03180694580078\n",
      "loss :  42.31378173828125\n",
      "loss :  91.05986022949219\n",
      "loss :  103.10975646972656\n",
      "loss :  94.12137603759766\n",
      "loss :  39.073673248291016\n",
      "loss :  43.35713577270508\n",
      "loss :  91.06591796875\n",
      "loss :  107.1230697631836\n",
      "loss :  94.70872497558594\n",
      "loss :  34.84436798095703\n",
      "loss :  42.158199310302734\n",
      "loss :  91.02468872070312\n",
      "loss :  103.67884826660156\n",
      "loss :  93.96717834472656\n",
      "loss :  38.54886245727539\n",
      "loss :  43.25112533569336\n",
      "loss :  91.03160858154297\n",
      "loss :  106.82170104980469\n",
      "loss :  94.66207122802734\n",
      "loss :  35.002227783203125\n",
      "loss :  42.27326202392578\n",
      "loss :  91.02558135986328\n",
      "loss :  103.57862091064453\n",
      "loss :  93.99288177490234\n",
      "loss :  38.52590560913086\n",
      "loss :  43.15457534790039\n",
      "loss :  91.04740905761719\n",
      "loss :  106.902099609375\n",
      "loss :  94.68254852294922\n",
      "loss :  34.99312210083008\n",
      "loss :  42.338462829589844\n",
      "loss :  91.02447509765625\n",
      "loss :  103.37828063964844\n",
      "loss :  94.01709747314453\n",
      "loss :  38.45094299316406\n",
      "loss :  43.17910385131836\n",
      "loss :  91.0465087890625\n",
      "loss :  106.84344482421875\n",
      "loss :  94.6814193725586\n",
      "loss :  34.94087219238281\n",
      "loss :  42.30550003051758\n",
      "loss :  91.01580047607422\n",
      "loss :  103.44500732421875\n",
      "loss :  93.98893737792969\n",
      "loss :  38.45151138305664\n",
      "loss :  43.1444091796875\n",
      "loss :  91.05949401855469\n",
      "loss :  106.7846450805664\n",
      "loss :  94.7487564086914\n",
      "loss :  35.02674102783203\n",
      "loss :  42.405967712402344\n",
      "loss :  91.02638244628906\n",
      "loss :  103.08702087402344\n",
      "loss :  94.03203582763672\n",
      "loss :  38.68675994873047\n",
      "loss :  43.15177917480469\n",
      "loss :  91.05278778076172\n",
      "loss :  107.07279968261719\n",
      "loss :  94.70683288574219\n",
      "loss :  34.89971923828125\n",
      "loss :  42.244056701660156\n",
      "loss :  91.01010131835938\n",
      "loss :  103.24882507324219\n",
      "loss :  93.936279296875\n",
      "loss :  38.75104522705078\n",
      "loss :  43.11412811279297\n",
      "loss :  91.07128143310547\n",
      "loss :  107.00820922851562\n",
      "loss :  94.8498764038086\n",
      "loss :  34.99350357055664\n",
      "loss :  42.39683151245117\n",
      "loss :  91.05310821533203\n",
      "loss :  102.5895004272461\n",
      "loss :  94.13387298583984\n",
      "loss :  39.07709884643555\n",
      "loss :  43.325904846191406\n",
      "loss :  91.07009887695312\n",
      "loss :  107.03650665283203\n",
      "loss :  94.7241439819336\n",
      "loss :  34.79788589477539\n",
      "loss :  42.10213088989258\n",
      "loss :  91.01182556152344\n",
      "loss :  103.3542709350586\n",
      "loss :  93.86739349365234\n",
      "loss :  38.87963104248047\n",
      "loss :  43.117088317871094\n",
      "loss :  91.04722595214844\n",
      "loss :  106.97251892089844\n",
      "loss :  94.81280517578125\n",
      "loss :  35.02056121826172\n",
      "loss :  42.33017349243164\n",
      "loss :  91.0458755493164\n",
      "loss :  102.59286499023438\n",
      "loss :  94.09659576416016\n",
      "loss :  39.10494613647461\n",
      "loss :  43.301998138427734\n",
      "loss :  91.06829833984375\n",
      "loss :  106.96439361572266\n",
      "loss :  94.69100189208984\n",
      "loss :  34.77832794189453\n",
      "loss :  42.081199645996094\n",
      "loss :  91.0210952758789\n",
      "loss :  103.26985931396484\n",
      "loss :  93.91026306152344\n",
      "loss :  38.741268157958984\n",
      "loss :  43.22037124633789\n",
      "loss :  91.02641296386719\n",
      "loss :  106.64244842529297\n",
      "loss :  94.64324951171875\n",
      "loss :  34.940185546875\n",
      "loss :  42.18455505371094\n",
      "loss :  91.01898193359375\n",
      "loss :  103.21051025390625\n",
      "loss :  93.93535614013672\n",
      "loss :  38.6213493347168\n",
      "loss :  43.13607406616211\n",
      "loss :  91.03575897216797\n",
      "loss :  106.65296936035156\n",
      "loss :  94.63594055175781\n",
      "loss :  34.877132415771484\n",
      "loss :  42.21793746948242\n",
      "loss :  91.0144271850586\n",
      "loss :  103.18255615234375\n",
      "loss :  93.94804382324219\n",
      "loss :  38.40779113769531\n",
      "loss :  43.14527893066406\n",
      "loss :  91.02696990966797\n",
      "loss :  106.4834976196289\n",
      "loss :  94.60393524169922\n",
      "loss :  34.92401123046875\n",
      "loss :  42.23075485229492\n",
      "loss :  91.00674438476562\n",
      "loss :  103.23661041259766\n",
      "loss :  93.92024230957031\n",
      "loss :  38.389495849609375\n",
      "loss :  43.075069427490234\n",
      "loss :  91.04523468017578\n",
      "loss :  106.45618438720703\n",
      "loss :  94.67027282714844\n",
      "loss :  34.95277404785156\n",
      "loss :  42.33790969848633\n",
      "loss :  91.0204086303711\n",
      "loss :  102.8918228149414\n",
      "loss :  94.00154876708984\n",
      "loss :  38.43495178222656\n",
      "loss :  43.14198684692383\n",
      "loss :  91.03630065917969\n",
      "loss :  106.49413299560547\n",
      "loss :  94.58917236328125\n",
      "loss :  34.8708381652832\n",
      "loss :  42.183467864990234\n",
      "loss :  91.00382232666016\n",
      "loss :  103.25358581542969\n",
      "loss :  93.8770523071289\n",
      "loss :  38.34914016723633\n",
      "loss :  43.03775405883789\n",
      "loss :  91.045166015625\n",
      "loss :  106.39474487304688\n",
      "loss :  94.68989562988281\n",
      "loss :  34.96285629272461\n",
      "loss :  42.354827880859375\n",
      "loss :  91.0334701538086\n",
      "loss :  102.72637176513672\n",
      "loss :  94.03284454345703\n",
      "loss :  38.488712310791016\n",
      "loss :  43.15974426269531\n",
      "loss :  91.03558349609375\n",
      "loss :  106.47760009765625\n",
      "loss :  94.5341567993164\n",
      "loss :  34.81459426879883\n",
      "loss :  42.0946044921875\n",
      "loss :  91.00190734863281\n",
      "loss :  103.42854309082031\n",
      "loss :  93.81695556640625\n",
      "loss :  38.206417083740234\n",
      "loss :  43.00617980957031\n",
      "loss :  91.0234146118164\n",
      "loss :  106.25041961669922\n",
      "loss :  94.5990219116211\n",
      "loss :  35.00231170654297\n",
      "loss :  42.32651901245117\n",
      "loss :  91.0162582397461\n",
      "loss :  102.83853149414062\n",
      "loss :  93.95736694335938\n",
      "loss :  38.40144348144531\n",
      "loss :  43.056053161621094\n",
      "loss :  91.06275939941406\n",
      "loss :  106.38140869140625\n",
      "loss :  94.62371063232422\n",
      "loss :  34.87656021118164\n",
      "loss :  42.26334762573242\n",
      "loss :  91.01348876953125\n",
      "loss :  102.89546203613281\n",
      "loss :  93.9028549194336\n",
      "loss :  38.37633514404297\n",
      "loss :  43.03628921508789\n",
      "loss :  91.0495834350586\n",
      "loss :  106.39826965332031\n",
      "loss :  94.6557388305664\n",
      "loss :  34.887290954589844\n",
      "loss :  42.26639175415039\n",
      "loss :  91.01583862304688\n",
      "loss :  102.71956634521484\n",
      "loss :  93.92900848388672\n",
      "loss :  38.571083068847656\n",
      "loss :  43.072757720947266\n",
      "loss :  91.0548095703125\n",
      "loss :  106.4563980102539\n",
      "loss :  94.64901733398438\n",
      "loss :  34.853858947753906\n",
      "loss :  42.20570373535156\n",
      "loss :  91.01646423339844\n",
      "loss :  102.72211456298828\n",
      "loss :  93.91119384765625\n",
      "loss :  38.51460266113281\n",
      "loss :  43.1124153137207\n",
      "loss :  91.03853607177734\n",
      "loss :  106.33134460449219\n",
      "loss :  94.58447265625\n",
      "loss :  34.83898162841797\n",
      "loss :  42.14027786254883\n",
      "loss :  91.00802612304688\n",
      "loss :  102.93206787109375\n",
      "loss :  93.84745788574219\n",
      "loss :  38.478580474853516\n",
      "loss :  43.02973175048828\n",
      "loss :  91.04561614990234\n",
      "loss :  106.28154754638672\n",
      "loss :  94.62403869628906\n",
      "loss :  34.880496978759766\n",
      "loss :  42.237876892089844\n",
      "loss :  91.0260009765625\n",
      "loss :  102.62801361083984\n",
      "loss :  93.94844055175781\n",
      "loss :  38.473060607910156\n",
      "loss :  43.118316650390625\n",
      "loss :  91.05378723144531\n",
      "loss :  106.1775131225586\n",
      "loss :  94.60382843017578\n",
      "loss :  34.84444046020508\n",
      "loss :  42.17019271850586\n",
      "loss :  91.01790618896484\n",
      "loss :  102.82012176513672\n",
      "loss :  93.87171173095703\n",
      "loss :  38.40201950073242\n",
      "loss :  43.04729080200195\n",
      "loss :  91.04523468017578\n",
      "loss :  106.1578598022461\n",
      "loss :  94.58553314208984\n",
      "loss :  34.886600494384766\n",
      "loss :  42.20217514038086\n",
      "loss :  91.0263671875\n",
      "loss :  102.70171356201172\n",
      "loss :  93.89944458007812\n",
      "loss :  38.42438507080078\n",
      "loss :  43.055484771728516\n",
      "loss :  91.05136108398438\n",
      "loss :  106.15058135986328\n",
      "loss :  94.5741195678711\n",
      "loss :  34.84970474243164\n",
      "loss :  42.16714859008789\n",
      "loss :  91.01917266845703\n",
      "loss :  102.7631607055664\n",
      "loss :  93.86666107177734\n",
      "loss :  38.38016128540039\n",
      "loss :  43.02421951293945\n",
      "loss :  91.04376220703125\n",
      "loss :  106.13749694824219\n",
      "loss :  94.55289459228516\n",
      "loss :  34.828033447265625\n",
      "loss :  42.15700149536133\n",
      "loss :  91.01868438720703\n",
      "loss :  102.78878784179688\n",
      "loss :  93.8536376953125\n",
      "loss :  38.25492858886719\n",
      "loss :  43.00726318359375\n",
      "loss :  91.0501708984375\n",
      "loss :  106.01148223876953\n",
      "loss :  94.5782699584961\n",
      "loss :  34.87920379638672\n",
      "loss :  42.220359802246094\n",
      "loss :  91.02005767822266\n",
      "loss :  102.6313247680664\n",
      "loss :  93.87041473388672\n",
      "loss :  38.33489227294922\n",
      "loss :  42.99464797973633\n",
      "loss :  91.06183624267578\n",
      "loss :  106.07182312011719\n",
      "loss :  94.59852600097656\n",
      "loss :  34.86867904663086\n",
      "loss :  42.225791931152344\n",
      "loss :  91.02030181884766\n",
      "loss :  102.5103530883789\n",
      "loss :  93.85822296142578\n",
      "loss :  38.488216400146484\n",
      "loss :  42.985816955566406\n",
      "loss :  91.06832122802734\n",
      "loss :  106.20330810546875\n",
      "loss :  94.64249420166016\n",
      "loss :  34.808902740478516\n",
      "loss :  42.205448150634766\n",
      "loss :  91.02837371826172\n",
      "loss :  102.33218383789062\n",
      "loss :  93.90772247314453\n",
      "loss :  38.521419525146484\n",
      "loss :  43.09279251098633\n",
      "loss :  91.05902099609375\n",
      "loss :  106.08549499511719\n",
      "loss :  94.5885238647461\n",
      "loss :  34.78574752807617\n",
      "loss :  42.10896301269531\n",
      "loss :  91.01762390136719\n",
      "loss :  102.59835052490234\n",
      "loss :  93.79585266113281\n",
      "loss :  38.55972671508789\n",
      "loss :  42.983314514160156\n",
      "loss :  91.06929779052734\n",
      "loss :  106.11272430419922\n",
      "loss :  94.6733627319336\n",
      "loss :  34.8648796081543\n",
      "loss :  42.24126434326172\n",
      "loss :  91.0495376586914\n",
      "loss :  102.07164764404297\n",
      "loss :  93.96765899658203\n",
      "loss :  38.68315124511719\n",
      "loss :  43.17098617553711\n",
      "loss :  91.05697631835938\n",
      "loss :  106.05658721923828\n",
      "loss :  94.49797058105469\n",
      "loss :  34.66345977783203\n",
      "loss :  41.95969772338867\n",
      "loss :  91.01823425292969\n",
      "loss :  102.96748352050781\n",
      "loss :  93.698974609375\n",
      "loss :  38.31230545043945\n",
      "loss :  42.93049621582031\n",
      "loss :  91.04328918457031\n",
      "loss :  105.90833282470703\n",
      "loss :  94.57388305664062\n",
      "loss :  34.904754638671875\n",
      "loss :  42.23555374145508\n",
      "loss :  91.03266143798828\n",
      "loss :  102.24744415283203\n",
      "loss :  93.88008880615234\n",
      "loss :  38.48807907104492\n",
      "loss :  43.00985336303711\n",
      "loss :  91.0842514038086\n",
      "loss :  106.01219940185547\n",
      "loss :  94.57947540283203\n",
      "loss :  34.76029968261719\n",
      "loss :  42.1470947265625\n",
      "loss :  91.02696990966797\n",
      "loss :  102.34508514404297\n",
      "loss :  93.8239974975586\n",
      "loss :  38.44794464111328\n",
      "loss :  43.03564453125\n",
      "loss :  91.05802154541016\n",
      "loss :  105.90022277832031\n",
      "loss :  94.54356384277344\n",
      "loss :  34.782432556152344\n",
      "loss :  42.10854721069336\n",
      "loss :  91.0235824584961\n",
      "loss :  102.4544448852539\n",
      "loss :  93.79010009765625\n",
      "loss :  38.463016510009766\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "optim = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 200\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for temp in range(len(X_train) // batch_size):\n",
    "        \n",
    "        s = temp * batch_size # 0 * 100\n",
    "        e = s + batch_size    # 100\n",
    "        \n",
    "        X = X_train[s:e]\n",
    "        y = y_train[s:e]\n",
    "    \n",
    "        optim.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        print('loss : ', loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
