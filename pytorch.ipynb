{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df1f71a",
   "metadata": {},
   "source": [
    "# 파이토치\n",
    "### 이미지 처리에 특화\n",
    "### CUDA 버전 너무 높으면 다른 의존성과 충돌 - 낮은 것이 안정적일 수 있음\n",
    "### 훈련과 추론 data간에 차원을 맞춰야 함(학습 : 모델 4차원  -> 추론 : 4차원 입력)\n",
    "##### https://pytorch.org/get-started/locally/\n",
    "##### cmd : uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126(cuda 버전)\n",
    "##### cmd : nvidia-smi(cuda 버전확인)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca981840",
   "metadata": {},
   "source": [
    "### 1. 기본 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9e307a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20262995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0+cu126'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106d7bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor라는 type == array \n",
    "torch.tensor(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff792038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "575f908b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a8de1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bcf0a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1,2,3,4,5])\n",
    "torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eceb3252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 5) # 1로 채운 3 x 5개의 data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdfebfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(512, 512) # 이미지 : 1(흰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eacd3a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(512, 512) # 이미지 : 0(검)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27663c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(5) # 단위행렬(대각선의 요소가 1 나머지는 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5e0fb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc5eccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d5a106e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8031, 0.3610, 0.5579, 0.2391, 0.9281])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8275706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000, 0.7573, 0.8516],\n",
       "         [0.2283, 0.5278, 0.3146],\n",
       "         [0.3244, 0.2240, 0.2109]],\n",
       "\n",
       "        [[0.1333, 0.5523, 0.5743],\n",
       "         [0.4822, 0.6583, 0.1073],\n",
       "         [0.3081, 0.3659, 0.6293]],\n",
       "\n",
       "        [[0.4425, 0.5879, 0.5490],\n",
       "         [0.9255, 0.4956, 0.7592],\n",
       "         [0.7293, 0.4287, 0.3339]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7436bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련과 추론 data간에 차원을 맞춰야 함\n",
    "data = torch.rand(3, 3, 3) # 3*3*3 = 27\n",
    "data.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9f55073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1730, 0.1030, 0.0515, 0.6148, 0.2885, 0.4123, 0.7614, 0.9527, 0.7365],\n",
       "        [0.6878, 0.9188, 0.3300, 0.0539, 0.3904, 0.6056, 0.1930, 0.1274, 0.4462],\n",
       "        [0.0891, 0.5417, 0.7978, 0.9331, 0.6954, 0.4693, 0.5764, 0.6349, 0.5695]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(9, 3) # 9*3 = 27\n",
    "data.view(3, 9) # 3*9 = 27(행렬 전환 - 이미지 반전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e9f9317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1730, 0.1030, 0.0515, 0.6148, 0.2885, 0.4123, 0.7614, 0.9527, 0.7365,\n",
       "        0.6878, 0.9188, 0.3300, 0.0539, 0.3904, 0.6056, 0.1930, 0.1274, 0.4462,\n",
       "        0.0891, 0.5417, 0.7978, 0.9331, 0.6954, 0.4693, 0.5764, 0.6349, 0.5695])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(27) # 3차원을 1차원으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f053d9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1730, 0.1030, 0.0515, 0.6148, 0.2885, 0.4123, 0.7614, 0.9527, 0.7365],\n",
       "        [0.6878, 0.9188, 0.3300, 0.0539, 0.3904, 0.6056, 0.1930, 0.1274, 0.4462],\n",
       "        [0.0891, 0.5417, 0.7978, 0.9331, 0.6954, 0.4693, 0.5764, 0.6349, 0.5695]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(3, -1) # 3행으로 하고 열은 자동으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "949b2eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1730, 0.1030, 0.0515, 0.6148, 0.2885, 0.4123, 0.7614, 0.9527, 0.7365,\n",
       "        0.6878, 0.9188, 0.3300, 0.0539, 0.3904, 0.6056, 0.1930, 0.1274, 0.4462,\n",
       "        0.0891, 0.5417, 0.7978, 0.9331, 0.6954, 0.4693, 0.5764, 0.6349, 0.5695])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(-1) # 1차원으로 변환(주로 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05be3ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape # 모양"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f46397b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dim() # 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a61e9034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype # 타입(중요)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520fca00",
   "metadata": {},
   "source": [
    "### 2. 하드웨어 설정(중요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b280f141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50301c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4070 Laptop GPU'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de609f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80243b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([1,2,3])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e20dc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to('cuda') # 1. gpu에 data 올리기(model/data 모두 같은 공간에서 처리해야 함 - 파라미터 적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b9682e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = torch.tensor([1,2,3]).cuda() # 2. gpu에 data 올리기\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95805c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.to('cpu') # gpu에서 cpu로 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c071615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cuda() + data1 # 같은 공간에 옮겨서 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa5f059f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.cuda()\n",
    "data - data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45203d",
   "metadata": {},
   "source": [
    "### 3. 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62e3037e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([[1,2,3],[4,5,6]])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97fd0650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56e87c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([1, 2, 3]),\n",
       "indices=tensor([0, 0, 0]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min(dim=0) # 열(0)기준으로 작은 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48679b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([1, 4]),\n",
       "indices=tensor([0, 0]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min(dim=1) # 행(1)기준으로 작은 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df4d7faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([4, 5, 6]),\n",
       "indices=tensor([1, 1, 1]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max(dim=0) # 열(0)기준으로 큰 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4a3b4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([3, 6]),\n",
       "indices=tensor([2, 2]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max(dim=1) # 행(1)기준으로 큰 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f68bef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4fdb553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6adcdced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f704af9",
   "metadata": {},
   "source": [
    "### 4. 차원 편집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36ee22af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(3, 128, 128) # 3차원 이미지\n",
    "image = image.view(1, 3, 128, 128) # 4차원 이미지\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c711eabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(3, 128, 128) \n",
    "image = image.unsqueeze(dim=0) # 맨 앞 차원 추가\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bae875af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.rand(1, 128, 128)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4b76c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.squeeze() # 맨 앞 차원 제거\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0a79d",
   "metadata": {},
   "source": [
    "### 5. 모델 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f33f609f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/boston.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa077bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.iloc[:, :13].values\n",
    "\n",
    "# X_train = torch.tensor(X_train)\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "\n",
    "y_train  = df.iloc[:, -1].values # 마지막 값만 \n",
    "y_train = torch.FloatTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bfcb6e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=13, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn # 모델 생성에 필요한 함수 포함\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "# 모델 학습\n",
    "# 13 => 100 => ReLU => 50 => ReLU => 1\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 100),\n",
    "    nn.ReLU(), # 활성화 함수(비선형)\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 1)\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "894cf3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 0.weight\n",
      "Shape: torch.Size([100, 13])\n",
      "Values: Parameter containing:\n",
      "tensor([[-0.2091,  0.1801, -0.1821,  ..., -0.2544, -0.1559, -0.1905],\n",
      "        [-0.2006,  0.0940, -0.2521,  ...,  0.1317,  0.2371,  0.2363],\n",
      "        [ 0.0527,  0.0964, -0.0203,  ..., -0.1674, -0.0820, -0.0912],\n",
      "        ...,\n",
      "        [-0.2192, -0.1853,  0.2737,  ..., -0.0340,  0.0370,  0.1508],\n",
      "        [ 0.1964, -0.1164, -0.0647,  ...,  0.2700,  0.0734, -0.0888],\n",
      "        [ 0.0474, -0.1828, -0.0224,  ...,  0.0627,  0.1649, -0.2285]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name: 0.bias\n",
      "Shape: torch.Size([100])\n",
      "Values: Parameter containing:\n",
      "tensor([ 0.1459,  0.0088,  0.1868, -0.0209,  0.2619,  0.2074,  0.1258,  0.1643,\n",
      "         0.1035,  0.0897,  0.0253, -0.1171,  0.0468,  0.2413, -0.1991,  0.0389,\n",
      "         0.1814,  0.1577, -0.2210, -0.1917,  0.0264,  0.0977, -0.1961, -0.2058,\n",
      "        -0.0060, -0.1403,  0.0568,  0.1599,  0.0784,  0.1910,  0.0623, -0.0247,\n",
      "         0.2218, -0.0663, -0.2038,  0.1276,  0.0043,  0.1705, -0.2034, -0.1053,\n",
      "         0.0514, -0.2738, -0.1932, -0.1104, -0.2768,  0.1833,  0.0329,  0.0962,\n",
      "         0.0945, -0.0323, -0.0565, -0.1591,  0.0898, -0.2012, -0.0575,  0.0663,\n",
      "         0.1718, -0.0206,  0.1127,  0.2770,  0.1924,  0.0803, -0.0263,  0.1036,\n",
      "         0.0693, -0.2407,  0.2005,  0.2432,  0.1805, -0.0058, -0.1221,  0.0052,\n",
      "        -0.2367,  0.1330,  0.2311,  0.1486, -0.0166, -0.1969, -0.0385,  0.0005,\n",
      "         0.1048,  0.0558,  0.2480,  0.1908,  0.2247, -0.0441,  0.0596,  0.0034,\n",
      "        -0.1576, -0.1461, -0.0618,  0.0032,  0.0781, -0.1757, -0.0357, -0.0255,\n",
      "        -0.1387, -0.1074, -0.2008, -0.0305], requires_grad=True)\n",
      "\n",
      "Name: 2.weight\n",
      "Shape: torch.Size([50, 100])\n",
      "Values: Parameter containing:\n",
      "tensor([[ 0.0135, -0.0027, -0.0519,  ...,  0.0427,  0.0403,  0.0689],\n",
      "        [-0.0032,  0.0093,  0.0909,  ...,  0.0200,  0.0866, -0.0109],\n",
      "        [ 0.0267,  0.0879, -0.0142,  ...,  0.0544,  0.0156,  0.0046],\n",
      "        ...,\n",
      "        [-0.0980,  0.0940,  0.0218,  ..., -0.0352, -0.0041, -0.0197],\n",
      "        [-0.0962, -0.0056, -0.0450,  ..., -0.0486,  0.0466,  0.0751],\n",
      "        [ 0.0814, -0.0405,  0.0015,  ..., -0.0440,  0.0274, -0.0349]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name: 2.bias\n",
      "Shape: torch.Size([50])\n",
      "Values: Parameter containing:\n",
      "tensor([-0.0415, -0.0752,  0.0178, -0.0619, -0.0113, -0.0976,  0.0322,  0.0823,\n",
      "        -0.0994, -0.0995, -0.0115, -0.0487, -0.0118, -0.0506,  0.0588,  0.0359,\n",
      "        -0.0784, -0.0709, -0.0086, -0.0447,  0.0134,  0.0132,  0.0868, -0.0818,\n",
      "        -0.0731,  0.0516, -0.0924,  0.0487, -0.0521, -0.0996,  0.0676,  0.0296,\n",
      "        -0.0988,  0.0375, -0.0425, -0.0495, -0.0977, -0.0606, -0.0112, -0.0526,\n",
      "        -0.0732,  0.0348,  0.0938,  0.0418,  0.0437,  0.0182,  0.0384,  0.0182,\n",
      "        -0.0799, -0.0287], requires_grad=True)\n",
      "\n",
      "Name: 4.weight\n",
      "Shape: torch.Size([1, 50])\n",
      "Values: Parameter containing:\n",
      "tensor([[-0.0185,  0.0131, -0.0094,  0.0245,  0.0299, -0.0227,  0.0359, -0.0965,\n",
      "         -0.0371,  0.1370,  0.1077, -0.1078, -0.0263,  0.0609, -0.0184,  0.0262,\n",
      "          0.0997,  0.1220,  0.1131,  0.1275,  0.1264,  0.0193,  0.0024,  0.0804,\n",
      "          0.0554, -0.1363,  0.0215, -0.0996, -0.0760,  0.0677,  0.0561, -0.1301,\n",
      "          0.0676,  0.0726,  0.0594,  0.0769,  0.0849,  0.1324,  0.0052,  0.0104,\n",
      "         -0.0523, -0.0690, -0.1085,  0.0768, -0.0888,  0.0722, -0.1221, -0.1260,\n",
      "         -0.1221, -0.1036]], requires_grad=True)\n",
      "\n",
      "Name: 4.bias\n",
      "Shape: torch.Size([1])\n",
      "Values: Parameter containing:\n",
      "tensor([0.0167], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Values: {param}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "287de76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  250.86962890625\n",
      "loss :  116.08440399169922\n",
      "loss :  80.24451446533203\n",
      "loss :  110.32664489746094\n",
      "loss :  133.69998168945312\n",
      "loss :  124.2442626953125\n",
      "loss :  98.20609283447266\n",
      "loss :  75.53921508789062\n",
      "loss :  66.73704528808594\n",
      "loss :  71.09789276123047\n",
      "loss :  80.3309326171875\n",
      "loss :  85.80316925048828\n",
      "loss :  83.96558380126953\n",
      "loss :  76.66825103759766\n",
      "loss :  68.40853881835938\n",
      "loss :  64.1466064453125\n",
      "loss :  65.68810272216797\n",
      "loss :  70.04893493652344\n",
      "loss :  72.59082794189453\n",
      "loss :  71.11671447753906\n",
      "loss :  66.86531066894531\n",
      "loss :  62.659141540527344\n",
      "loss :  60.77091598510742\n",
      "loss :  61.55466842651367\n",
      "loss :  63.6263313293457\n",
      "loss :  65.06535339355469\n",
      "loss :  64.84686279296875\n",
      "loss :  63.25650405883789\n",
      "loss :  61.46051788330078\n",
      "loss :  60.584739685058594\n",
      "loss :  60.92218017578125\n",
      "loss :  61.793643951416016\n",
      "loss :  62.172183990478516\n",
      "loss :  61.57530975341797\n",
      "loss :  60.341033935546875\n",
      "loss :  59.26536178588867\n",
      "loss :  58.876651763916016\n",
      "loss :  59.15911865234375\n",
      "loss :  59.54909133911133\n",
      "loss :  59.578006744384766\n",
      "loss :  59.17532730102539\n",
      "loss :  58.607757568359375\n",
      "loss :  58.23394775390625\n",
      "loss :  58.188438415527344\n",
      "loss :  58.31864547729492\n",
      "loss :  58.347564697265625\n",
      "loss :  58.116580963134766\n",
      "loss :  57.70576477050781\n",
      "loss :  57.34212875366211\n",
      "loss :  57.18973922729492\n",
      "loss :  57.209659576416016\n",
      "loss :  57.2215461730957\n",
      "loss :  57.08892822265625\n",
      "loss :  56.82992172241211\n",
      "loss :  56.577449798583984\n",
      "loss :  56.42799758911133\n",
      "loss :  56.37110900878906\n",
      "loss :  56.29635238647461\n",
      "loss :  56.138282775878906\n",
      "loss :  55.917850494384766\n",
      "loss :  55.716060638427734\n",
      "loss :  55.594276428222656\n",
      "loss :  55.53086853027344\n",
      "loss :  55.44331359863281\n",
      "loss :  55.29513168334961\n",
      "loss :  55.129356384277344\n",
      "loss :  54.99660110473633\n",
      "loss :  54.902339935302734\n",
      "loss :  54.8017692565918\n",
      "loss :  54.66151809692383\n",
      "loss :  54.49509811401367\n",
      "loss :  54.34520721435547\n",
      "loss :  54.22841262817383\n",
      "loss :  54.11745834350586\n",
      "loss :  53.98529815673828\n",
      "loss :  53.83610534667969\n",
      "loss :  53.69911575317383\n",
      "loss :  53.58041763305664\n",
      "loss :  53.4627571105957\n",
      "loss :  53.3289909362793\n",
      "loss :  53.18544006347656\n",
      "loss :  53.044837951660156\n",
      "loss :  52.91304016113281\n",
      "loss :  52.781044006347656\n",
      "loss :  52.639007568359375\n",
      "loss :  52.489898681640625\n",
      "loss :  52.34538650512695\n",
      "loss :  52.20774459838867\n",
      "loss :  52.06706619262695\n",
      "loss :  51.922607421875\n",
      "loss :  51.777645111083984\n",
      "loss :  51.63282012939453\n",
      "loss :  51.48836135864258\n",
      "loss :  51.34170913696289\n",
      "loss :  51.19477081298828\n",
      "loss :  51.048580169677734\n",
      "loss :  50.90184783935547\n",
      "loss :  50.753353118896484\n",
      "loss :  50.60357666015625\n",
      "loss :  50.45442581176758\n",
      "loss :  50.30801010131836\n",
      "loss :  50.15922164916992\n",
      "loss :  50.00653839111328\n",
      "loss :  49.853355407714844\n",
      "loss :  49.70085906982422\n",
      "loss :  49.5478401184082\n",
      "loss :  49.3930549621582\n",
      "loss :  49.23691177368164\n",
      "loss :  49.08122634887695\n",
      "loss :  48.92404556274414\n",
      "loss :  48.763362884521484\n",
      "loss :  48.602413177490234\n",
      "loss :  48.44306564331055\n",
      "loss :  48.282283782958984\n",
      "loss :  48.11971664428711\n",
      "loss :  47.957923889160156\n",
      "loss :  47.796024322509766\n",
      "loss :  47.63218688964844\n",
      "loss :  47.466487884521484\n",
      "loss :  47.29948425292969\n",
      "loss :  47.13172912597656\n",
      "loss :  46.962562561035156\n",
      "loss :  46.793800354003906\n",
      "loss :  46.624210357666016\n",
      "loss :  46.452117919921875\n",
      "loss :  46.27888107299805\n",
      "loss :  46.10307693481445\n",
      "loss :  45.92624282836914\n",
      "loss :  45.74830627441406\n",
      "loss :  45.56987380981445\n",
      "loss :  45.39258575439453\n",
      "loss :  45.21559143066406\n",
      "loss :  45.0364875793457\n",
      "loss :  44.853981018066406\n",
      "loss :  44.670196533203125\n",
      "loss :  44.486759185791016\n",
      "loss :  44.306610107421875\n",
      "loss :  44.1273078918457\n",
      "loss :  43.948726654052734\n",
      "loss :  43.76948165893555\n",
      "loss :  43.58901596069336\n",
      "loss :  43.408206939697266\n",
      "loss :  43.227413177490234\n",
      "loss :  43.0468864440918\n",
      "loss :  42.864078521728516\n",
      "loss :  42.68095779418945\n",
      "loss :  42.49721908569336\n",
      "loss :  42.3111686706543\n",
      "loss :  42.124088287353516\n",
      "loss :  41.93616485595703\n",
      "loss :  41.746822357177734\n",
      "loss :  41.5578498840332\n",
      "loss :  41.367252349853516\n",
      "loss :  41.17164611816406\n",
      "loss :  40.97324752807617\n",
      "loss :  40.7641716003418\n",
      "loss :  40.542179107666016\n",
      "loss :  40.308284759521484\n",
      "loss :  40.06846618652344\n",
      "loss :  39.821372985839844\n",
      "loss :  39.56455993652344\n",
      "loss :  39.303436279296875\n",
      "loss :  39.04587173461914\n",
      "loss :  38.79014587402344\n",
      "loss :  38.527992248535156\n",
      "loss :  38.22505569458008\n",
      "loss :  37.89937973022461\n",
      "loss :  37.53640365600586\n",
      "loss :  37.15269470214844\n",
      "loss :  36.721500396728516\n",
      "loss :  36.26380157470703\n",
      "loss :  35.80844497680664\n",
      "loss :  35.44269561767578\n",
      "loss :  35.094635009765625\n",
      "loss :  34.79110336303711\n",
      "loss :  34.50944137573242\n",
      "loss :  34.209327697753906\n",
      "loss :  33.932891845703125\n",
      "loss :  33.67108154296875\n",
      "loss :  33.42325210571289\n",
      "loss :  33.1862678527832\n",
      "loss :  32.95313262939453\n",
      "loss :  32.720130920410156\n",
      "loss :  32.48659133911133\n",
      "loss :  32.25142288208008\n",
      "loss :  32.01830291748047\n",
      "loss :  31.79374885559082\n",
      "loss :  31.569849014282227\n",
      "loss :  31.347824096679688\n",
      "loss :  31.118165969848633\n",
      "loss :  30.87468719482422\n",
      "loss :  30.643030166625977\n",
      "loss :  30.445926666259766\n",
      "loss :  30.249156951904297\n",
      "loss :  30.05124282836914\n",
      "loss :  29.85072135925293\n",
      "loss :  29.658174514770508\n",
      "loss :  29.47052574157715\n",
      "loss :  29.279022216796875\n",
      "loss :  29.096298217773438\n"
     ]
    }
   ],
   "source": [
    "optim = Adam(model.parameters(), lr=0.001) # 최적화(탐색 크기 및 속도)함수 선택\n",
    "criterion = nn.MSELoss() # 손실함수 선택\n",
    "epochs = 200 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    optim.zero_grad() # 기울기 값을 0으로 초기화(누적 방지)\n",
    "    \n",
    "    y_pred = model(X_train) # 예측\n",
    "    \n",
    "    loss = criterion(y_pred, y_train.view(-1, 1)) # loss 값 계산\n",
    "    \n",
    "    loss.backward() # 역전파\n",
    "    \n",
    "    optim.step() # 가중치 업데이트(다음 진행)\n",
    "    \n",
    "    print('loss : ', loss.item()) # loss 값 확인(계속 감소하면 epoch 증가 고려)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e67e3c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.2091,  0.1801, -0.1821,  ..., -0.2544, -0.1559, -0.1905],\n",
       "                      [-0.2266,  0.0956, -0.2746,  ...,  0.1059,  0.2424,  0.1277],\n",
       "                      [ 0.1099,  0.1317, -0.0303,  ..., -0.1641, -0.0782, -0.0130],\n",
       "                      ...,\n",
       "                      [-0.1370, -0.1563,  0.3107,  ...,  0.0082,  0.0289,  0.3151],\n",
       "                      [ 0.2634, -0.1181, -0.0482,  ...,  0.2859,  0.0736,  0.0065],\n",
       "                      [ 0.1465, -0.0815, -0.0935,  ...,  0.0403,  0.1714, -0.2920]])),\n",
       "             ('0.bias',\n",
       "              tensor([ 0.1459,  0.0194,  0.1746,  0.0320,  0.2688,  0.2074,  0.1258,  0.1518,\n",
       "                       0.1035,  0.0831,  0.0253, -0.1171,  0.0565,  0.2413, -0.1205,  0.0535,\n",
       "                       0.1650,  0.1784, -0.2208, -0.1836,  0.0513,  0.0977, -0.0165, -0.1976,\n",
       "                       0.0070, -0.1403,  0.0672,  0.1599,  0.0784,  0.1985,  0.0623, -0.0287,\n",
       "                       0.2381, -0.0663, -0.2033,  0.1276, -0.0118,  0.1388, -0.2113, -0.0765,\n",
       "                       0.0514, -0.2605, -0.1991, -0.1039, -0.2642,  0.1833,  0.0329,  0.1038,\n",
       "                       0.0885, -0.0317, -0.0495, -0.1522,  0.0898, -0.1902, -0.0727,  0.0906,\n",
       "                       0.1577, -0.0517,  0.1054,  0.2770,  0.1874,  0.0899, -0.0263,  0.1185,\n",
       "                       0.0860, -0.2450,  0.1875,  0.2359,  0.1805,  0.0893, -0.1254,  0.0033,\n",
       "                      -0.2367,  0.1330,  0.2311,  0.1495, -0.0118, -0.1684, -0.0588, -0.0109,\n",
       "                       0.1361,  0.0379,  0.2389,  0.1732,  0.2133, -0.0406,  0.0472,  0.0034,\n",
       "                      -0.1563, -0.1558, -0.0648,  0.0025,  0.0762,  0.0188, -0.0357, -0.0255,\n",
       "                      -0.1547, -0.1347, -0.2092, -0.0231])),\n",
       "             ('2.weight',\n",
       "              tensor([[ 0.0135, -0.0062, -0.0488,  ...,  0.0481,  0.0395,  0.0705],\n",
       "                      [-0.0032,  0.0093,  0.0836,  ...,  0.0127,  0.0793, -0.0109],\n",
       "                      [ 0.0267,  0.0799, -0.0212,  ...,  0.0471,  0.0079, -0.0028],\n",
       "                      ...,\n",
       "                      [-0.0980,  0.0940,  0.0218,  ..., -0.0352, -0.0041, -0.0197],\n",
       "                      [-0.0962, -0.0056, -0.0450,  ..., -0.0486,  0.0466,  0.0751],\n",
       "                      [ 0.0814, -0.0450,  0.0102,  ..., -0.0428,  0.0341, -0.0521]])),\n",
       "             ('2.bias',\n",
       "              tensor([-0.0516, -0.0826,  0.0100, -0.0622,  0.0035, -0.1060,  0.0414,  0.0723,\n",
       "                      -0.0994, -0.0995,  0.0079, -0.0487, -0.0118, -0.0414,  0.0588,  0.0372,\n",
       "                      -0.0712, -0.0609,  0.0012, -0.0356,  0.0231,  0.0217,  0.0955, -0.0818,\n",
       "                      -0.0731,  0.0516, -0.0924,  0.0487, -0.0608, -0.0900,  0.0773,  0.0296,\n",
       "                      -0.0889,  0.0566, -0.0425, -0.0399, -0.0878, -0.0606, -0.0137, -0.0446,\n",
       "                      -0.0824,  0.0348,  0.0859,  0.0418,  0.0437,  0.0182,  0.0285,  0.0182,\n",
       "                      -0.0799, -0.0405])),\n",
       "             ('4.weight',\n",
       "              tensor([[-0.0190,  0.0057, -0.0019,  0.0296,  0.0395, -0.0147,  0.0401, -0.0978,\n",
       "                       -0.0371,  0.1370,  0.1851, -0.1078, -0.0263,  0.0569, -0.0184,  0.0390,\n",
       "                        0.0982,  0.1313,  0.1165,  0.1594,  0.1287,  0.0182,  0.0126,  0.0796,\n",
       "                        0.0554, -0.1363,  0.0215, -0.0996, -0.0759,  0.0714,  0.0614, -0.1301,\n",
       "                        0.0842,  0.0799,  0.0594,  0.0811,  0.0978,  0.1324,  0.0300,  0.0113,\n",
       "                       -0.0609, -0.0690, -0.1012,  0.0768, -0.0888,  0.0722, -0.1246, -0.1260,\n",
       "                       -0.1221, -0.1033]])),\n",
       "             ('4.bias', tensor([0.0265]))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77c2d7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\walker\\code\\deep-learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:616: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  64.26487731933594\n",
      "loss :  172.74830627441406\n",
      "loss :  148.2445068359375\n",
      "loss :  162.296142578125\n",
      "loss :  175.07603454589844\n",
      "loss :  54.00504684448242\n",
      "loss :  119.8480453491211\n",
      "loss :  156.4344024658203\n",
      "loss :  117.05152130126953\n",
      "loss :  52.5182991027832\n",
      "loss :  50.335609436035156\n",
      "loss :  122.49315643310547\n",
      "loss :  122.99473571777344\n",
      "loss :  108.62812042236328\n",
      "loss :  59.907474517822266\n",
      "loss :  59.29724884033203\n",
      "loss :  116.50807189941406\n",
      "loss :  101.72052001953125\n",
      "loss :  112.54727172851562\n",
      "loss :  58.339962005615234\n",
      "loss :  46.801998138427734\n",
      "loss :  110.93460083007812\n",
      "loss :  128.19705200195312\n",
      "loss :  102.21803283691406\n",
      "loss :  44.220767974853516\n",
      "loss :  42.42521286010742\n",
      "loss :  110.5284194946289\n",
      "loss :  125.22572326660156\n",
      "loss :  99.1503677368164\n",
      "loss :  54.29301071166992\n",
      "loss :  44.86939239501953\n",
      "loss :  107.14364624023438\n",
      "loss :  112.08338165283203\n",
      "loss :  101.037841796875\n",
      "loss :  52.762359619140625\n",
      "loss :  43.9246940612793\n",
      "loss :  107.7282485961914\n",
      "loss :  121.2330322265625\n",
      "loss :  100.89179992675781\n",
      "loss :  41.113868713378906\n",
      "loss :  43.0611686706543\n",
      "loss :  108.56468200683594\n",
      "loss :  115.94367218017578\n",
      "loss :  101.9253921508789\n",
      "loss :  45.05275344848633\n",
      "loss :  46.49665451049805\n",
      "loss :  107.47052001953125\n",
      "loss :  108.97016143798828\n",
      "loss :  102.78227233886719\n",
      "loss :  44.57306671142578\n",
      "loss :  45.678497314453125\n",
      "loss :  107.55997467041016\n",
      "loss :  114.322021484375\n",
      "loss :  102.3583755493164\n",
      "loss :  40.30607223510742\n",
      "loss :  44.4047966003418\n",
      "loss :  107.18932342529297\n",
      "loss :  113.19668579101562\n",
      "loss :  101.41728973388672\n",
      "loss :  43.90077209472656\n",
      "loss :  45.626155853271484\n",
      "loss :  106.1021957397461\n",
      "loss :  111.28158569335938\n",
      "loss :  101.21070098876953\n",
      "loss :  42.750431060791016\n",
      "loss :  44.91801834106445\n",
      "loss :  106.29574584960938\n",
      "loss :  113.50518798828125\n",
      "loss :  101.23574829101562\n",
      "loss :  40.825130462646484\n",
      "loss :  44.815162658691406\n",
      "loss :  105.8658218383789\n",
      "loss :  111.44241333007812\n",
      "loss :  101.06598663330078\n",
      "loss :  42.5750846862793\n",
      "loss :  45.44862365722656\n",
      "loss :  105.34767150878906\n",
      "loss :  111.08357238769531\n",
      "loss :  100.83811950683594\n",
      "loss :  41.3304443359375\n",
      "loss :  44.824222564697266\n",
      "loss :  105.28325653076172\n",
      "loss :  112.0105209350586\n",
      "loss :  100.51821899414062\n",
      "loss :  41.21672821044922\n",
      "loss :  44.848915100097656\n",
      "loss :  104.76220703125\n",
      "loss :  111.00037384033203\n",
      "loss :  100.27628326416016\n",
      "loss :  41.78287887573242\n",
      "loss :  44.854366302490234\n",
      "loss :  104.48368835449219\n",
      "loss :  111.45050811767578\n",
      "loss :  100.08698272705078\n",
      "loss :  40.788818359375\n",
      "loss :  44.54035186767578\n",
      "loss :  104.28813171386719\n",
      "loss :  111.36957550048828\n",
      "loss :  99.94925689697266\n",
      "loss :  40.992618560791016\n",
      "loss :  44.71214294433594\n",
      "loss :  103.88534545898438\n",
      "loss :  110.81017303466797\n",
      "loss :  99.87513732910156\n",
      "loss :  40.765472412109375\n",
      "loss :  44.651798248291016\n",
      "loss :  103.67616271972656\n",
      "loss :  110.96389770507812\n",
      "loss :  99.82337188720703\n",
      "loss :  40.2320671081543\n",
      "loss :  44.6302375793457\n",
      "loss :  103.39791107177734\n",
      "loss :  110.5481185913086\n",
      "loss :  99.74562072753906\n",
      "loss :  40.33882141113281\n",
      "loss :  44.73311233520508\n",
      "loss :  103.08656311035156\n",
      "loss :  110.38143157958984\n",
      "loss :  99.61759948730469\n",
      "loss :  40.029598236083984\n",
      "loss :  44.628570556640625\n",
      "loss :  102.84258270263672\n",
      "loss :  110.36863708496094\n",
      "loss :  99.49771881103516\n",
      "loss :  39.90946960449219\n",
      "loss :  44.61638641357422\n",
      "loss :  102.5424575805664\n",
      "loss :  110.18621063232422\n",
      "loss :  99.35464477539062\n",
      "loss :  39.805877685546875\n",
      "loss :  44.56602478027344\n",
      "loss :  102.28135681152344\n",
      "loss :  110.16869354248047\n",
      "loss :  99.20433044433594\n",
      "loss :  39.65500259399414\n",
      "loss :  44.539791107177734\n",
      "loss :  102.0132827758789\n",
      "loss :  109.9642333984375\n",
      "loss :  99.08436584472656\n",
      "loss :  39.621604919433594\n",
      "loss :  44.56597900390625\n",
      "loss :  101.7708969116211\n",
      "loss :  109.81502532958984\n",
      "loss :  98.9970703125\n",
      "loss :  39.41324996948242\n",
      "loss :  44.5216178894043\n",
      "loss :  101.5537109375\n",
      "loss :  109.73807525634766\n",
      "loss :  98.885986328125\n",
      "loss :  39.287940979003906\n",
      "loss :  44.50465393066406\n",
      "loss :  101.31278228759766\n",
      "loss :  109.60452270507812\n",
      "loss :  98.7586441040039\n",
      "loss :  39.17979431152344\n",
      "loss :  44.468162536621094\n",
      "loss :  101.10103607177734\n",
      "loss :  109.57954406738281\n",
      "loss :  98.642333984375\n",
      "loss :  39.01344299316406\n",
      "loss :  44.45121765136719\n",
      "loss :  100.88053131103516\n",
      "loss :  109.42737579345703\n",
      "loss :  98.51509857177734\n",
      "loss :  38.91463088989258\n",
      "loss :  44.428218841552734\n",
      "loss :  100.67692565917969\n",
      "loss :  109.4146957397461\n",
      "loss :  98.36200714111328\n",
      "loss :  38.70391845703125\n",
      "loss :  44.36705017089844\n",
      "loss :  100.4902572631836\n",
      "loss :  109.37569427490234\n",
      "loss :  98.18560028076172\n",
      "loss :  38.586097717285156\n",
      "loss :  44.37503433227539\n",
      "loss :  100.3056869506836\n",
      "loss :  109.24032592773438\n",
      "loss :  98.02240753173828\n",
      "loss :  38.49397659301758\n",
      "loss :  44.37690353393555\n",
      "loss :  100.1317367553711\n",
      "loss :  109.15036010742188\n",
      "loss :  97.89299774169922\n",
      "loss :  38.37527847290039\n",
      "loss :  44.382911682128906\n",
      "loss :  99.93193817138672\n",
      "loss :  109.04106140136719\n",
      "loss :  97.76498413085938\n",
      "loss :  38.345829010009766\n",
      "loss :  44.36363220214844\n",
      "loss :  99.68107604980469\n",
      "loss :  109.0018310546875\n",
      "loss :  97.6726303100586\n",
      "loss :  38.32975387573242\n",
      "loss :  44.32604217529297\n",
      "loss :  99.39869689941406\n",
      "loss :  108.969970703125\n",
      "loss :  97.60575866699219\n",
      "loss :  38.29710388183594\n",
      "loss :  44.28456115722656\n",
      "loss :  99.07882690429688\n",
      "loss :  108.88422393798828\n",
      "loss :  97.57677459716797\n",
      "loss :  38.36245346069336\n",
      "loss :  44.2373161315918\n",
      "loss :  98.76763153076172\n",
      "loss :  108.94332122802734\n",
      "loss :  97.55328369140625\n",
      "loss :  38.20718765258789\n",
      "loss :  44.110130310058594\n",
      "loss :  98.5155029296875\n",
      "loss :  109.01934814453125\n",
      "loss :  97.49555969238281\n",
      "loss :  38.242271423339844\n",
      "loss :  44.08591842651367\n",
      "loss :  98.2419662475586\n",
      "loss :  108.9324951171875\n",
      "loss :  97.44843292236328\n",
      "loss :  38.241207122802734\n",
      "loss :  44.041900634765625\n",
      "loss :  98.02888488769531\n",
      "loss :  108.97039794921875\n",
      "loss :  97.4185791015625\n",
      "loss :  38.138404846191406\n",
      "loss :  44.02851486206055\n",
      "loss :  97.80050659179688\n",
      "loss :  108.82382202148438\n",
      "loss :  97.33795928955078\n",
      "loss :  38.15838623046875\n",
      "loss :  43.97718811035156\n",
      "loss :  97.60749053955078\n",
      "loss :  108.95394897460938\n",
      "loss :  97.24311828613281\n",
      "loss :  37.994319915771484\n",
      "loss :  43.91472244262695\n",
      "loss :  97.42658233642578\n",
      "loss :  108.92256164550781\n",
      "loss :  97.12887573242188\n",
      "loss :  38.04082107543945\n",
      "loss :  43.92250442504883\n",
      "loss :  97.26473236083984\n",
      "loss :  108.95294952392578\n",
      "loss :  97.1015625\n",
      "loss :  37.880699157714844\n",
      "loss :  43.86336135864258\n",
      "loss :  97.12207794189453\n",
      "loss :  108.93836212158203\n",
      "loss :  97.08445739746094\n",
      "loss :  37.84974670410156\n",
      "loss :  43.84467697143555\n",
      "loss :  96.95169830322266\n",
      "loss :  108.83613586425781\n",
      "loss :  97.09337615966797\n",
      "loss :  37.8330192565918\n",
      "loss :  43.84617233276367\n",
      "loss :  96.79396057128906\n",
      "loss :  108.7022705078125\n",
      "loss :  97.11076354980469\n",
      "loss :  37.805824279785156\n",
      "loss :  43.82228088378906\n",
      "loss :  96.65679168701172\n",
      "loss :  108.69939422607422\n",
      "loss :  97.05931091308594\n",
      "loss :  37.77288818359375\n",
      "loss :  43.78221893310547\n",
      "loss :  96.5189437866211\n",
      "loss :  108.69902038574219\n",
      "loss :  96.9686508178711\n",
      "loss :  37.75590133666992\n",
      "loss :  43.75754165649414\n",
      "loss :  96.3878402709961\n",
      "loss :  108.66050720214844\n",
      "loss :  96.93573760986328\n",
      "loss :  37.71479415893555\n",
      "loss :  43.74274826049805\n",
      "loss :  96.2459487915039\n",
      "loss :  108.6142349243164\n",
      "loss :  96.91377258300781\n",
      "loss :  37.70286560058594\n",
      "loss :  43.72595977783203\n",
      "loss :  96.1151351928711\n",
      "loss :  108.61080932617188\n",
      "loss :  96.89130401611328\n",
      "loss :  37.63527297973633\n",
      "loss :  43.681419372558594\n",
      "loss :  95.98120880126953\n",
      "loss :  108.58856964111328\n",
      "loss :  96.83599853515625\n",
      "loss :  37.66230773925781\n",
      "loss :  43.66572570800781\n",
      "loss :  95.85897064208984\n",
      "loss :  108.55457305908203\n",
      "loss :  96.82791900634766\n",
      "loss :  37.59026336669922\n",
      "loss :  43.65687561035156\n",
      "loss :  95.7510986328125\n",
      "loss :  108.47545623779297\n",
      "loss :  96.78832244873047\n",
      "loss :  37.584842681884766\n",
      "loss :  43.664306640625\n",
      "loss :  95.64839935302734\n",
      "loss :  108.40733337402344\n",
      "loss :  96.76960754394531\n",
      "loss :  37.51274490356445\n",
      "loss :  43.64799880981445\n",
      "loss :  95.54627227783203\n",
      "loss :  108.35456848144531\n",
      "loss :  96.7094497680664\n",
      "loss :  37.58248519897461\n",
      "loss :  43.639225006103516\n",
      "loss :  95.43744659423828\n",
      "loss :  108.35514831542969\n",
      "loss :  96.65182495117188\n",
      "loss :  37.491172790527344\n",
      "loss :  43.56489944458008\n",
      "loss :  95.34053802490234\n",
      "loss :  108.39887237548828\n",
      "loss :  96.58915710449219\n",
      "loss :  37.48731231689453\n",
      "loss :  43.558753967285156\n",
      "loss :  95.22763061523438\n",
      "loss :  108.3293685913086\n",
      "loss :  96.58120727539062\n",
      "loss :  37.51652908325195\n",
      "loss :  43.597137451171875\n",
      "loss :  95.12225341796875\n",
      "loss :  108.18479919433594\n",
      "loss :  96.62451171875\n",
      "loss :  37.387760162353516\n",
      "loss :  43.57607650756836\n",
      "loss :  95.02841186523438\n",
      "loss :  108.13719940185547\n",
      "loss :  96.58448028564453\n",
      "loss :  37.41654586791992\n",
      "loss :  43.53990173339844\n",
      "loss :  94.91012573242188\n",
      "loss :  108.23554992675781\n",
      "loss :  96.52092742919922\n",
      "loss :  37.413150787353516\n",
      "loss :  43.49330139160156\n",
      "loss :  94.81443786621094\n",
      "loss :  108.21207427978516\n",
      "loss :  96.49009704589844\n",
      "loss :  37.39395523071289\n",
      "loss :  43.50181198120117\n",
      "loss :  94.71540832519531\n",
      "loss :  108.10617065429688\n",
      "loss :  96.44808959960938\n",
      "loss :  37.398780822753906\n",
      "loss :  43.457801818847656\n",
      "loss :  94.64022064208984\n",
      "loss :  108.18824768066406\n",
      "loss :  96.4716796875\n",
      "loss :  37.271785736083984\n",
      "loss :  43.47477340698242\n",
      "loss :  94.54985046386719\n",
      "loss :  107.94383239746094\n",
      "loss :  96.4202880859375\n",
      "loss :  37.41458511352539\n",
      "loss :  43.49227523803711\n",
      "loss :  94.45823669433594\n",
      "loss :  108.03763580322266\n",
      "loss :  96.41673278808594\n",
      "loss :  37.29201889038086\n",
      "loss :  43.409976959228516\n",
      "loss :  94.36730194091797\n",
      "loss :  108.03509521484375\n",
      "loss :  96.35286712646484\n",
      "loss :  37.35131072998047\n",
      "loss :  43.40435791015625\n",
      "loss :  94.28046417236328\n",
      "loss :  108.04344940185547\n",
      "loss :  96.34135437011719\n",
      "loss :  37.271244049072266\n",
      "loss :  43.39169692993164\n",
      "loss :  94.20018768310547\n",
      "loss :  107.93795013427734\n",
      "loss :  96.35936737060547\n",
      "loss :  37.28806686401367\n",
      "loss :  43.424625396728516\n",
      "loss :  94.11430358886719\n",
      "loss :  107.86650085449219\n",
      "loss :  96.32821655273438\n",
      "loss :  37.322288513183594\n",
      "loss :  43.39616012573242\n",
      "loss :  94.03094482421875\n",
      "loss :  107.90227508544922\n",
      "loss :  96.2879409790039\n",
      "loss :  37.25209426879883\n",
      "loss :  43.35416030883789\n",
      "loss :  93.95242309570312\n",
      "loss :  107.88289642333984\n",
      "loss :  96.26167297363281\n",
      "loss :  37.281700134277344\n",
      "loss :  43.34268569946289\n",
      "loss :  93.87293243408203\n",
      "loss :  107.885986328125\n",
      "loss :  96.23064422607422\n",
      "loss :  37.274662017822266\n",
      "loss :  43.33115005493164\n",
      "loss :  93.80194854736328\n",
      "loss :  107.83496856689453\n",
      "loss :  96.21428680419922\n",
      "loss :  37.2481803894043\n",
      "loss :  43.3338508605957\n",
      "loss :  93.7268295288086\n",
      "loss :  107.78504943847656\n",
      "loss :  96.1999740600586\n",
      "loss :  37.23457336425781\n",
      "loss :  43.32542037963867\n",
      "loss :  93.65931701660156\n",
      "loss :  107.75668334960938\n",
      "loss :  96.1817626953125\n",
      "loss :  37.2230224609375\n",
      "loss :  43.32960891723633\n",
      "loss :  93.58966064453125\n",
      "loss :  107.7031478881836\n",
      "loss :  96.17305755615234\n",
      "loss :  37.213924407958984\n",
      "loss :  43.30727767944336\n",
      "loss :  93.50287628173828\n",
      "loss :  107.71793365478516\n",
      "loss :  96.10641479492188\n",
      "loss :  37.2445068359375\n",
      "loss :  43.2666130065918\n",
      "loss :  93.43270111083984\n",
      "loss :  107.76676940917969\n",
      "loss :  96.08655548095703\n",
      "loss :  37.16571807861328\n",
      "loss :  43.282440185546875\n",
      "loss :  93.37297058105469\n",
      "loss :  107.6085205078125\n",
      "loss :  96.08290100097656\n",
      "loss :  37.239479064941406\n",
      "loss :  43.30392837524414\n",
      "loss :  93.30121612548828\n",
      "loss :  107.63941955566406\n",
      "loss :  96.08572387695312\n",
      "loss :  37.148075103759766\n",
      "loss :  43.25288772583008\n",
      "loss :  93.22799682617188\n",
      "loss :  107.59208679199219\n",
      "loss :  96.02196502685547\n",
      "loss :  37.230384826660156\n",
      "loss :  43.2672004699707\n",
      "loss :  93.16854858398438\n",
      "loss :  107.60354614257812\n",
      "loss :  96.01251220703125\n",
      "loss :  37.13692092895508\n",
      "loss :  43.26176834106445\n",
      "loss :  93.11302947998047\n",
      "loss :  107.5037841796875\n",
      "loss :  96.01184844970703\n",
      "loss :  37.16635513305664\n",
      "loss :  43.2739372253418\n",
      "loss :  93.04654693603516\n",
      "loss :  107.4831314086914\n",
      "loss :  95.98897552490234\n",
      "loss :  37.14065933227539\n",
      "loss :  43.24241638183594\n",
      "loss :  92.98631286621094\n",
      "loss :  107.47588348388672\n",
      "loss :  95.94210052490234\n",
      "loss :  37.1672248840332\n",
      "loss :  43.248470306396484\n",
      "loss :  92.93327331542969\n",
      "loss :  107.4397201538086\n",
      "loss :  95.95280456542969\n",
      "loss :  37.09725570678711\n",
      "loss :  43.245628356933594\n",
      "loss :  92.87891387939453\n",
      "loss :  107.36067199707031\n",
      "loss :  95.94766235351562\n",
      "loss :  37.098506927490234\n",
      "loss :  43.24186325073242\n",
      "loss :  92.81006622314453\n",
      "loss :  107.34539794921875\n",
      "loss :  95.90284729003906\n",
      "loss :  37.1397819519043\n",
      "loss :  43.2097053527832\n",
      "loss :  92.72823333740234\n",
      "loss :  107.41026306152344\n",
      "loss :  95.8569107055664\n",
      "loss :  37.123775482177734\n",
      "loss :  43.17707824707031\n",
      "loss :  92.63681030273438\n",
      "loss :  107.38794708251953\n",
      "loss :  95.82012176513672\n",
      "loss :  37.12290573120117\n",
      "loss :  43.16700744628906\n",
      "loss :  92.57695007324219\n",
      "loss :  107.37723541259766\n",
      "loss :  95.86170959472656\n",
      "loss :  37.07268524169922\n",
      "loss :  43.20771789550781\n",
      "loss :  92.51043701171875\n",
      "loss :  107.21659851074219\n",
      "loss :  95.8333511352539\n",
      "loss :  37.10542297363281\n",
      "loss :  43.18564987182617\n",
      "loss :  92.4638671875\n",
      "loss :  107.32331848144531\n",
      "loss :  95.80504608154297\n",
      "loss :  37.01895523071289\n",
      "loss :  43.16947555541992\n",
      "loss :  92.41181945800781\n",
      "loss :  107.18419647216797\n",
      "loss :  95.76911163330078\n",
      "loss :  37.091461181640625\n",
      "loss :  43.199485778808594\n",
      "loss :  92.35939025878906\n",
      "loss :  107.18902587890625\n",
      "loss :  95.79913330078125\n",
      "loss :  36.993831634521484\n",
      "loss :  43.18351745605469\n",
      "loss :  92.31644439697266\n",
      "loss :  107.10749816894531\n",
      "loss :  95.72996520996094\n",
      "loss :  37.02442932128906\n",
      "loss :  43.18437576293945\n",
      "loss :  92.28180694580078\n",
      "loss :  107.11754608154297\n",
      "loss :  95.70823669433594\n",
      "loss :  36.974029541015625\n",
      "loss :  43.164974212646484\n",
      "loss :  92.24649810791016\n",
      "loss :  107.09527587890625\n",
      "loss :  95.68800354003906\n",
      "loss :  37.00247573852539\n",
      "loss :  43.19021224975586\n",
      "loss :  92.19776916503906\n",
      "loss :  107.01006317138672\n",
      "loss :  95.65889739990234\n",
      "loss :  36.9821662902832\n",
      "loss :  43.136356353759766\n",
      "loss :  92.15235137939453\n",
      "loss :  107.10932159423828\n",
      "loss :  95.63041687011719\n",
      "loss :  36.966064453125\n",
      "loss :  43.124019622802734\n",
      "loss :  92.09608459472656\n",
      "loss :  107.03699493408203\n",
      "loss :  95.5938491821289\n",
      "loss :  36.987998962402344\n",
      "loss :  43.121803283691406\n",
      "loss :  92.04732513427734\n",
      "loss :  107.04137420654297\n",
      "loss :  95.6068344116211\n",
      "loss :  36.92763137817383\n",
      "loss :  43.139705657958984\n",
      "loss :  92.00505828857422\n",
      "loss :  106.8802719116211\n",
      "loss :  95.62240600585938\n",
      "loss :  36.97438049316406\n",
      "loss :  43.171653747558594\n",
      "loss :  91.96639251708984\n",
      "loss :  106.90294647216797\n",
      "loss :  95.58261108398438\n",
      "loss :  36.94544982910156\n",
      "loss :  43.10181427001953\n",
      "loss :  91.92011260986328\n",
      "loss :  106.97747039794922\n",
      "loss :  95.51663970947266\n",
      "loss :  36.95456314086914\n",
      "loss :  43.08073425292969\n",
      "loss :  91.89097595214844\n",
      "loss :  106.9209213256836\n",
      "loss :  95.51824951171875\n",
      "loss :  36.93788528442383\n",
      "loss :  43.11678695678711\n",
      "loss :  91.86127471923828\n",
      "loss :  106.81716918945312\n",
      "loss :  95.53116607666016\n",
      "loss :  36.941253662109375\n",
      "loss :  43.11977767944336\n",
      "loss :  91.82279205322266\n",
      "loss :  106.84717559814453\n",
      "loss :  95.5146484375\n",
      "loss :  36.90280532836914\n",
      "loss :  43.094512939453125\n",
      "loss :  91.78012084960938\n",
      "loss :  106.80457305908203\n",
      "loss :  95.4891586303711\n",
      "loss :  36.92575454711914\n",
      "loss :  43.080726623535156\n",
      "loss :  91.7549819946289\n",
      "loss :  106.81046295166016\n",
      "loss :  95.50051879882812\n",
      "loss :  36.87086868286133\n",
      "loss :  43.09659957885742\n",
      "loss :  91.71847534179688\n",
      "loss :  106.68879699707031\n",
      "loss :  95.46859741210938\n",
      "loss :  36.9433479309082\n",
      "loss :  43.09220504760742\n",
      "loss :  91.67743682861328\n",
      "loss :  106.7636489868164\n",
      "loss :  95.50577545166016\n",
      "loss :  36.828556060791016\n",
      "loss :  43.08434295654297\n",
      "loss :  91.6522445678711\n",
      "loss :  106.58259582519531\n",
      "loss :  95.47919464111328\n",
      "loss :  36.91665267944336\n",
      "loss :  43.11667251586914\n",
      "loss :  91.62506103515625\n",
      "loss :  106.6073989868164\n",
      "loss :  95.46354675292969\n",
      "loss :  36.85391616821289\n",
      "loss :  43.062538146972656\n",
      "loss :  91.58183288574219\n",
      "loss :  106.6334457397461\n",
      "loss :  95.4351577758789\n",
      "loss :  36.89452362060547\n",
      "loss :  43.045040130615234\n",
      "loss :  91.55107116699219\n",
      "loss :  106.63638305664062\n",
      "loss :  95.4329605102539\n",
      "loss :  36.853553771972656\n",
      "loss :  43.046836853027344\n",
      "loss :  91.5300064086914\n",
      "loss :  106.55722045898438\n",
      "loss :  95.39035034179688\n",
      "loss :  36.903831481933594\n",
      "loss :  43.04667282104492\n",
      "loss :  91.49368286132812\n",
      "loss :  106.59149932861328\n",
      "loss :  95.39533996582031\n",
      "loss :  36.85866928100586\n",
      "loss :  43.01875686645508\n",
      "loss :  91.45409393310547\n",
      "loss :  106.55404663085938\n",
      "loss :  95.40422821044922\n",
      "loss :  36.86250686645508\n",
      "loss :  43.0378532409668\n",
      "loss :  91.43232727050781\n",
      "loss :  106.47314453125\n",
      "loss :  95.3870849609375\n",
      "loss :  36.853824615478516\n",
      "loss :  43.04718780517578\n",
      "loss :  91.41116333007812\n",
      "loss :  106.43464660644531\n",
      "loss :  95.38749694824219\n",
      "loss :  36.84275436401367\n",
      "loss :  43.0255241394043\n",
      "loss :  91.37838745117188\n",
      "loss :  106.48064422607422\n",
      "loss :  95.38981628417969\n",
      "loss :  36.827335357666016\n",
      "loss :  43.01203918457031\n",
      "loss :  91.34339904785156\n",
      "loss :  106.43364715576172\n",
      "loss :  95.35675811767578\n",
      "loss :  36.831443786621094\n",
      "loss :  42.99934768676758\n",
      "loss :  91.32809448242188\n",
      "loss :  106.4354248046875\n",
      "loss :  95.3717041015625\n",
      "loss :  36.79240036010742\n",
      "loss :  43.02547836303711\n",
      "loss :  91.29972076416016\n",
      "loss :  106.3028335571289\n",
      "loss :  95.35540008544922\n",
      "loss :  36.856510162353516\n",
      "loss :  43.01116943359375\n",
      "loss :  91.27212524414062\n",
      "loss :  106.4235610961914\n",
      "loss :  95.36072540283203\n",
      "loss :  36.77973175048828\n",
      "loss :  42.98855209350586\n",
      "loss :  91.23890686035156\n",
      "loss :  106.29008483886719\n",
      "loss :  95.29071044921875\n",
      "loss :  36.87639236450195\n",
      "loss :  42.98533630371094\n",
      "loss :  91.22492218017578\n",
      "loss :  106.40248107910156\n",
      "loss :  95.32378387451172\n",
      "loss :  36.7244758605957\n",
      "loss :  42.98236846923828\n",
      "loss :  91.19872283935547\n",
      "loss :  106.2006607055664\n",
      "loss :  95.3011474609375\n",
      "loss :  36.883575439453125\n",
      "loss :  43.022029876708984\n",
      "loss :  91.17237091064453\n",
      "loss :  106.3084716796875\n",
      "loss :  95.33992767333984\n",
      "loss :  36.702335357666016\n",
      "loss :  42.969268798828125\n",
      "loss :  91.14910125732422\n",
      "loss :  106.1807861328125\n",
      "loss :  95.2590103149414\n",
      "loss :  36.861602783203125\n",
      "loss :  42.99573516845703\n",
      "loss :  91.13575744628906\n",
      "loss :  106.27130889892578\n",
      "loss :  95.32093048095703\n",
      "loss :  36.69210433959961\n",
      "loss :  42.972537994384766\n",
      "loss :  91.1061782836914\n",
      "loss :  106.10746002197266\n",
      "loss :  95.26668548583984\n",
      "loss :  36.84246063232422\n",
      "loss :  42.98127365112305\n",
      "loss :  91.09608459472656\n",
      "loss :  106.24364471435547\n",
      "loss :  95.32202911376953\n",
      "loss :  36.64434814453125\n",
      "loss :  42.95786666870117\n",
      "loss :  91.0802993774414\n",
      "loss :  106.04359436035156\n",
      "loss :  95.22818756103516\n",
      "loss :  36.853363037109375\n",
      "loss :  42.9857063293457\n",
      "loss :  91.05923461914062\n",
      "loss :  106.18037414550781\n",
      "loss :  95.31178283691406\n",
      "loss :  36.639373779296875\n",
      "loss :  42.95447540283203\n",
      "loss :  91.04069519042969\n",
      "loss :  106.00264739990234\n",
      "loss :  95.25630950927734\n",
      "loss :  36.78843688964844\n",
      "loss :  43.00030517578125\n",
      "loss :  91.02973175048828\n",
      "loss :  106.06458282470703\n",
      "loss :  95.28353881835938\n",
      "loss :  36.64556121826172\n",
      "loss :  42.945762634277344\n",
      "loss :  91.00689697265625\n",
      "loss :  106.01757049560547\n",
      "loss :  95.2372817993164\n",
      "loss :  36.768043518066406\n",
      "loss :  42.941654205322266\n",
      "loss :  90.98882293701172\n",
      "loss :  106.08463287353516\n",
      "loss :  95.23666381835938\n",
      "loss :  36.646141052246094\n",
      "loss :  42.924522399902344\n",
      "loss :  90.98253631591797\n",
      "loss :  105.96015930175781\n",
      "loss :  95.20938873291016\n",
      "loss :  36.75022888183594\n",
      "loss :  42.96321105957031\n",
      "loss :  90.96550750732422\n",
      "loss :  105.96419525146484\n",
      "loss :  95.25698852539062\n",
      "loss :  36.64262008666992\n",
      "loss :  42.94032669067383\n",
      "loss :  90.9455337524414\n",
      "loss :  105.90467071533203\n",
      "loss :  95.20966339111328\n",
      "loss :  36.69711685180664\n",
      "loss :  42.92496871948242\n",
      "loss :  90.93242645263672\n",
      "loss :  105.98551940917969\n",
      "loss :  95.21031951904297\n",
      "loss :  36.62791061401367\n",
      "loss :  42.906368255615234\n",
      "loss :  90.91546630859375\n",
      "loss :  105.89595031738281\n",
      "loss :  95.187744140625\n",
      "loss :  36.7128791809082\n",
      "loss :  42.93169021606445\n",
      "loss :  90.89723205566406\n",
      "loss :  105.91915893554688\n",
      "loss :  95.19241333007812\n",
      "loss :  36.6130256652832\n",
      "loss :  42.88991165161133\n",
      "loss :  90.885986328125\n",
      "loss :  105.88152313232422\n",
      "loss :  95.1793441772461\n",
      "loss :  36.65117263793945\n",
      "loss :  42.93295669555664\n",
      "loss :  90.87482452392578\n",
      "loss :  105.78842163085938\n",
      "loss :  95.21526336669922\n",
      "loss :  36.62715530395508\n",
      "loss :  42.94127655029297\n",
      "loss :  90.86126708984375\n",
      "loss :  105.77937316894531\n",
      "loss :  95.18346405029297\n",
      "loss :  36.616127014160156\n",
      "loss :  42.91271209716797\n",
      "loss :  90.84766387939453\n",
      "loss :  105.80198669433594\n",
      "loss :  95.17069244384766\n",
      "loss :  36.6163215637207\n",
      "loss :  42.92354202270508\n",
      "loss :  90.83422088623047\n",
      "loss :  105.70704650878906\n",
      "loss :  95.1817626953125\n",
      "loss :  36.61868667602539\n",
      "loss :  42.91525650024414\n",
      "loss :  90.82062530517578\n",
      "loss :  105.75684356689453\n",
      "loss :  95.15411376953125\n",
      "loss :  36.590816497802734\n",
      "loss :  42.890403747558594\n",
      "loss :  90.80403900146484\n",
      "loss :  105.73212432861328\n",
      "loss :  95.1460952758789\n",
      "loss :  36.59770965576172\n",
      "loss :  42.88658142089844\n",
      "loss :  90.79568481445312\n",
      "loss :  105.72472381591797\n",
      "loss :  95.15544128417969\n",
      "loss :  36.567840576171875\n",
      "loss :  42.911048889160156\n",
      "loss :  90.78509521484375\n",
      "loss :  105.5813980102539\n",
      "loss :  95.1325454711914\n",
      "loss :  36.62629318237305\n",
      "loss :  42.90616226196289\n",
      "loss :  90.77326965332031\n",
      "loss :  105.69220733642578\n",
      "loss :  95.16058349609375\n",
      "loss :  36.506492614746094\n",
      "loss :  42.85700607299805\n",
      "loss :  90.75701141357422\n",
      "loss :  105.63489532470703\n",
      "loss :  95.08718872070312\n",
      "loss :  36.60004425048828\n",
      "loss :  42.87378692626953\n",
      "loss :  90.75714111328125\n",
      "loss :  105.63796997070312\n",
      "loss :  95.13890838623047\n",
      "loss :  36.49871063232422\n",
      "loss :  42.896934509277344\n",
      "loss :  90.74592590332031\n",
      "loss :  105.47235107421875\n",
      "loss :  95.11913299560547\n",
      "loss :  36.594573974609375\n",
      "loss :  42.88591003417969\n",
      "loss :  90.72480773925781\n",
      "loss :  105.63774871826172\n",
      "loss :  95.0995864868164\n",
      "loss :  36.48115158081055\n",
      "loss :  42.82023620605469\n",
      "loss :  90.70854949951172\n",
      "loss :  105.58570098876953\n",
      "loss :  95.06046295166016\n",
      "loss :  36.57205581665039\n",
      "loss :  42.86237716674805\n",
      "loss :  90.70177459716797\n",
      "loss :  105.52931213378906\n",
      "loss :  95.10649871826172\n",
      "loss :  36.488956451416016\n",
      "loss :  42.8681755065918\n",
      "loss :  90.69725036621094\n",
      "loss :  105.4552993774414\n",
      "loss :  95.0728988647461\n",
      "loss :  36.544647216796875\n",
      "loss :  42.846466064453125\n",
      "loss :  90.68595886230469\n",
      "loss :  105.55655670166016\n",
      "loss :  95.04935455322266\n",
      "loss :  36.47212219238281\n",
      "loss :  42.81865692138672\n",
      "loss :  90.67483520507812\n",
      "loss :  105.4611587524414\n",
      "loss :  95.0447769165039\n",
      "loss :  36.537391662597656\n",
      "loss :  42.85113525390625\n",
      "loss :  90.66413116455078\n",
      "loss :  105.46249389648438\n",
      "loss :  95.08582305908203\n",
      "loss :  36.43262481689453\n",
      "loss :  42.83449172973633\n",
      "loss :  90.6575698852539\n",
      "loss :  105.37982177734375\n",
      "loss :  95.04100036621094\n",
      "loss :  36.51122283935547\n",
      "loss :  42.836734771728516\n",
      "loss :  90.64982604980469\n",
      "loss :  105.4499740600586\n",
      "loss :  95.04243469238281\n",
      "loss :  36.43498229980469\n",
      "loss :  42.80070114135742\n",
      "loss :  90.63475799560547\n",
      "loss :  105.41614532470703\n",
      "loss :  95.02892303466797\n",
      "loss :  36.469730377197266\n",
      "loss :  42.81265640258789\n",
      "loss :  90.62309265136719\n",
      "loss :  105.38336944580078\n",
      "loss :  95.02413177490234\n",
      "loss :  36.441497802734375\n",
      "loss :  42.799747467041016\n",
      "loss :  90.61953735351562\n",
      "loss :  105.35902404785156\n",
      "loss :  95.02205657958984\n",
      "loss :  36.46027755737305\n",
      "loss :  42.82337188720703\n",
      "loss :  90.60536193847656\n",
      "loss :  105.30915069580078\n",
      "loss :  95.0296401977539\n",
      "loss :  36.44477462768555\n",
      "loss :  42.80002212524414\n",
      "loss :  90.5888442993164\n",
      "loss :  105.35018157958984\n",
      "loss :  95.00508880615234\n",
      "loss :  36.419681549072266\n",
      "loss :  42.77111053466797\n",
      "loss :  90.58676147460938\n",
      "loss :  105.3449935913086\n",
      "loss :  94.97689819335938\n",
      "loss :  36.42646789550781\n",
      "loss :  42.78402328491211\n",
      "loss :  90.58562469482422\n",
      "loss :  105.26696014404297\n",
      "loss :  94.98747253417969\n",
      "loss :  36.42662048339844\n",
      "loss :  42.792728424072266\n",
      "loss :  90.57501983642578\n",
      "loss :  105.25552368164062\n",
      "loss :  95.0062255859375\n",
      "loss :  36.389617919921875\n",
      "loss :  42.77748489379883\n",
      "loss :  90.56382751464844\n",
      "loss :  105.22386932373047\n",
      "loss :  94.96029663085938\n",
      "loss :  36.432952880859375\n",
      "loss :  42.74664306640625\n",
      "loss :  90.55425262451172\n",
      "loss :  105.33096313476562\n",
      "loss :  94.9812240600586\n",
      "loss :  36.34444808959961\n",
      "loss :  42.750553131103516\n",
      "loss :  90.54208374023438\n",
      "loss :  105.18020629882812\n",
      "loss :  94.94715118408203\n",
      "loss :  36.41072463989258\n",
      "loss :  42.751033782958984\n",
      "loss :  90.54327392578125\n",
      "loss :  105.25047302246094\n",
      "loss :  94.96015930175781\n",
      "loss :  36.33343505859375\n",
      "loss :  42.737430572509766\n",
      "loss :  90.5351791381836\n",
      "loss :  105.14247131347656\n",
      "loss :  94.908203125\n",
      "loss :  36.43974685668945\n",
      "loss :  42.74836349487305\n",
      "loss :  90.52645111083984\n",
      "loss :  105.21211242675781\n",
      "loss :  94.95655822753906\n",
      "loss :  36.30390548706055\n",
      "loss :  42.72481918334961\n",
      "loss :  90.51411437988281\n",
      "loss :  105.12786865234375\n",
      "loss :  94.92125701904297\n",
      "loss :  36.383148193359375\n",
      "loss :  42.726924896240234\n",
      "loss :  90.50634765625\n",
      "loss :  105.201171875\n",
      "loss :  94.94144439697266\n",
      "loss :  36.28953170776367\n",
      "loss :  42.705562591552734\n",
      "loss :  90.49962615966797\n",
      "loss :  105.08441925048828\n",
      "loss :  94.88025665283203\n",
      "loss :  36.406253814697266\n",
      "loss :  42.71213150024414\n",
      "loss :  90.49951934814453\n",
      "loss :  105.1756362915039\n",
      "loss :  94.93206024169922\n",
      "loss :  36.261985778808594\n",
      "loss :  42.702064514160156\n",
      "loss :  90.48759460449219\n",
      "loss :  105.00569915771484\n",
      "loss :  94.87992095947266\n",
      "loss :  36.392425537109375\n",
      "loss :  42.70261001586914\n",
      "loss :  90.48249816894531\n",
      "loss :  105.15895080566406\n",
      "loss :  94.94071197509766\n",
      "loss :  36.2230110168457\n",
      "loss :  42.682594299316406\n",
      "loss :  90.47177124023438\n",
      "loss :  104.9557113647461\n",
      "loss :  94.85710906982422\n",
      "loss :  36.41061019897461\n",
      "loss :  42.688228607177734\n",
      "loss :  90.47079467773438\n",
      "loss :  105.15670013427734\n",
      "loss :  94.9383316040039\n",
      "loss :  36.173152923583984\n",
      "loss :  42.67478942871094\n",
      "loss :  90.4599838256836\n",
      "loss :  104.8419418334961\n",
      "loss :  94.85277557373047\n",
      "loss :  36.44481658935547\n",
      "loss :  42.70844650268555\n",
      "loss :  90.45518493652344\n",
      "loss :  105.12594604492188\n",
      "loss :  94.93456268310547\n",
      "loss :  36.130523681640625\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "optim = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 200\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for temp in range(len(X_train) // batch_size):\n",
    "        \n",
    "        s = temp * batch_size # 0 * 100\n",
    "        e = s + batch_size    # 100\n",
    "        \n",
    "        X = X_train[s:e]\n",
    "        y = y_train[s:e]\n",
    "    \n",
    "        optim.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        print('loss : ', loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
