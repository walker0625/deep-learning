{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df1f71a",
   "metadata": {},
   "source": [
    "# 파이토치\n",
    "### 이미지 처리에 특화\n",
    "### CUDA 버전 너무 높으면 다른 의존성과 충돌 - 낮은 것이 안정적일 수 있음\n",
    "### 훈련과 추론 data간에 차원을 맞춰야 함(학습 : 모델 4차원  -> 추론 : 4차원 입력)\n",
    "##### https://pytorch.org/get-started/locally/\n",
    "##### cmd : uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126(cuda 버전)\n",
    "##### cmd : nvidia-smi(cuda 버전확인)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca981840",
   "metadata": {},
   "source": [
    "### 1. 기본 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ab9e307a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "20262995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0+cu126'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "106d7bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor라는 type == array \n",
    "torch.tensor(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ff792038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "575f908b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "21a8de1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8bcf0a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1,2,3,4,5])\n",
    "torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eceb3252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 5) # 1로 채운 3 x 5개의 data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cdfebfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(512, 512) # 이미지 : 1(흰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eacd3a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(512, 512) # 이미지 : 0(검)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "27663c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(5) # 단위행렬(대각선의 요소가 1 나머지는 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a5e0fb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fc5eccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7, 9])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0d5a106e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9306, 0.1654, 0.5932, 0.1055, 0.9525])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f8275706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7855, 0.9344, 0.9691],\n",
       "         [0.2090, 0.8834, 0.6439],\n",
       "         [0.7475, 0.8471, 0.1604]],\n",
       "\n",
       "        [[0.6072, 0.7863, 0.0119],\n",
       "         [0.3712, 0.0155, 0.5355],\n",
       "         [0.6226, 0.4872, 0.8430]],\n",
       "\n",
       "        [[0.3238, 0.6757, 0.4946],\n",
       "         [0.5372, 0.3537, 0.1209],\n",
       "         [0.0098, 0.1240, 0.1122]]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a7436bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련과 추론 data간에 차원을 맞춰야 함\n",
    "data = torch.rand(3, 3, 3) # 3*3*3 = 27\n",
    "data.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a9f55073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0150, 0.4982, 0.6671, 0.2635, 0.5322, 0.3222, 0.1606, 0.2517, 0.9910],\n",
       "        [0.8049, 0.4751, 0.6013, 0.7166, 0.1626, 0.4786, 0.7037, 0.0994, 0.1265],\n",
       "        [0.1036, 0.1701, 0.0511, 0.3545, 0.4341, 0.9867, 0.0782, 0.9377, 0.4752]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(9, 3) # 9*3 = 27\n",
    "data.view(3, 9) # 3*9 = 27(행렬 전환 - 이미지 반전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6e9f9317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0150, 0.4982, 0.6671, 0.2635, 0.5322, 0.3222, 0.1606, 0.2517, 0.9910,\n",
       "        0.8049, 0.4751, 0.6013, 0.7166, 0.1626, 0.4786, 0.7037, 0.0994, 0.1265,\n",
       "        0.1036, 0.1701, 0.0511, 0.3545, 0.4341, 0.9867, 0.0782, 0.9377, 0.4752])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(27) # 3차원을 1차원으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f053d9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0150, 0.4982, 0.6671, 0.2635, 0.5322, 0.3222, 0.1606, 0.2517, 0.9910],\n",
       "        [0.8049, 0.4751, 0.6013, 0.7166, 0.1626, 0.4786, 0.7037, 0.0994, 0.1265],\n",
       "        [0.1036, 0.1701, 0.0511, 0.3545, 0.4341, 0.9867, 0.0782, 0.9377, 0.4752]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(3, -1) # 3행으로 하고 열은 자동으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "949b2eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0150, 0.4982, 0.6671, 0.2635, 0.5322, 0.3222, 0.1606, 0.2517, 0.9910,\n",
       "        0.8049, 0.4751, 0.6013, 0.7166, 0.1626, 0.4786, 0.7037, 0.0994, 0.1265,\n",
       "        0.1036, 0.1701, 0.0511, 0.3545, 0.4341, 0.9867, 0.0782, 0.9377, 0.4752])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(-1) # 1차원으로 변환(주로 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "05be3ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape # 모양"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4f46397b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dim() # 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a61e9034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype # 타입(중요)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520fca00",
   "metadata": {},
   "source": [
    "### 2. 하드웨어 설정(중요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b280f141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "50301c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4070 Laptop GPU'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "de609f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "80243b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([1,2,3])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6e20dc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to('cuda') # 1. gpu에 data 올리기(model/data 모두 같은 공간에서 처리해야 함 - 파라미터 적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0b9682e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = torch.tensor([1,2,3]).cuda() # 2. gpu에 data 올리기\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "95805c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.to('cpu') # gpu에서 cpu로 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8c071615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6], device='cuda:0')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cuda() + data1 # 같은 공간에 옮겨서 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fa5f059f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.cuda()\n",
    "data - data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45203d",
   "metadata": {},
   "source": [
    "### 3. 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "62e3037e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([[1,2,3],[4,5,6]])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "97fd0650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "56e87c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([1, 2, 3]),\n",
       "indices=tensor([0, 0, 0]))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min(dim=0) # 열(0)기준으로 작은 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "48679b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([1, 4]),\n",
       "indices=tensor([0, 0]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min(dim=1) # 행(1)기준으로 작은 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "df4d7faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([4, 5, 6]),\n",
       "indices=tensor([1, 1, 1]))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max(dim=0) # 열(0)기준으로 큰 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e4a3b4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([3, 6]),\n",
       "indices=tensor([2, 2]))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max(dim=1) # 행(1)기준으로 큰 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f68bef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e4fdb553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6adcdced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f704af9",
   "metadata": {},
   "source": [
    "### 4. 차원 편집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "36ee22af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(3, 128, 128) # 3차원 이미지\n",
    "image = image.view(1, 3, 128, 128) # 4차원 이미지\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c711eabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(3, 128, 128) \n",
    "image = image.unsqueeze(dim=0) # 맨 앞 차원 추가\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "bae875af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.rand(1, 128, 128)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f4b76c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.squeeze() # 맨 앞 차원 제거\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0a79d",
   "metadata": {},
   "source": [
    "### 5. 모델 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f33f609f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/boston.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "aa077bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.iloc[:, :13].values\n",
    "\n",
    "# X_train = torch.tensor(X_train)\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "\n",
    "y_train  = df.iloc[:, -1].values # 마지막 값만 \n",
    "y_train = torch.FloatTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bfcb6e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=13, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn # 모델 생성에 필요한 함수 포함\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "# 모델 학습\n",
    "# 13 => 100 => ReLU => 50 => ReLU => 1\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 100),\n",
    "    nn.ReLU(), # 활성화 함수(비선형)\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 1)\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "894cf3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 0.weight\n",
      "Shape: torch.Size([100, 13])\n",
      "Values: Parameter containing:\n",
      "tensor([[ 0.2678, -0.0858, -0.1227,  ...,  0.1063,  0.1780,  0.0986],\n",
      "        [-0.0619,  0.2396, -0.2533,  ..., -0.2146, -0.1039,  0.0191],\n",
      "        [ 0.1487,  0.0104, -0.1483,  ..., -0.1522, -0.1509, -0.1409],\n",
      "        ...,\n",
      "        [ 0.0588,  0.0869, -0.0101,  ...,  0.2699,  0.0602,  0.2079],\n",
      "        [-0.1405,  0.2344,  0.0495,  ..., -0.0822,  0.0715,  0.0225],\n",
      "        [ 0.1876,  0.1566,  0.1838,  ..., -0.0011,  0.1771,  0.1670]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name: 0.bias\n",
      "Shape: torch.Size([100])\n",
      "Values: Parameter containing:\n",
      "tensor([ 0.0994,  0.1843, -0.0412, -0.2049,  0.0775,  0.2287, -0.0667,  0.1358,\n",
      "         0.1152,  0.1284, -0.1881, -0.2112, -0.0962, -0.1001,  0.1493, -0.2513,\n",
      "        -0.0921,  0.1427, -0.0603, -0.1886,  0.1239, -0.0554, -0.0473, -0.0945,\n",
      "         0.2428, -0.2761,  0.1597, -0.1493, -0.0070,  0.2302, -0.1298,  0.2208,\n",
      "        -0.2568, -0.1261, -0.1597,  0.0049, -0.0128,  0.0041,  0.0433,  0.0185,\n",
      "        -0.0681, -0.0507, -0.2529,  0.1651,  0.1138,  0.0651,  0.0383, -0.1178,\n",
      "         0.2318,  0.1039, -0.1828, -0.0028,  0.1720,  0.0137, -0.2613, -0.2474,\n",
      "         0.0283,  0.0438,  0.0425, -0.1228, -0.2476,  0.0375, -0.2198, -0.0041,\n",
      "         0.1270,  0.2367,  0.0537,  0.2332,  0.1071,  0.2238,  0.1309, -0.1278,\n",
      "         0.1462, -0.0561,  0.1966, -0.1021,  0.2045, -0.1215, -0.0454,  0.1584,\n",
      "         0.2359, -0.1081, -0.2413,  0.2170, -0.0973, -0.0490, -0.1104,  0.2291,\n",
      "         0.1403, -0.0882,  0.2755,  0.2390, -0.2003, -0.1857,  0.0724, -0.1429,\n",
      "        -0.1330, -0.1024, -0.1720,  0.2126], requires_grad=True)\n",
      "\n",
      "Name: 2.weight\n",
      "Shape: torch.Size([50, 100])\n",
      "Values: Parameter containing:\n",
      "tensor([[-0.0161, -0.0986, -0.0785,  ..., -0.0121, -0.0328,  0.0671],\n",
      "        [-0.0298, -0.0748, -0.0512,  ..., -0.0387, -0.0461,  0.0318],\n",
      "        [-0.0731,  0.0674, -0.0605,  ...,  0.0168, -0.0441,  0.0876],\n",
      "        ...,\n",
      "        [ 0.0068,  0.0759,  0.0730,  ...,  0.0350, -0.0027,  0.0076],\n",
      "        [-0.0106,  0.0477, -0.0876,  ...,  0.0448,  0.0336, -0.0091],\n",
      "        [-0.0292, -0.0749, -0.0652,  ..., -0.0400,  0.0634,  0.0795]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name: 2.bias\n",
      "Shape: torch.Size([50])\n",
      "Values: Parameter containing:\n",
      "tensor([-0.0813,  0.0886, -0.0841,  0.0547,  0.0887, -0.0642,  0.0586, -0.0035,\n",
      "        -0.0246,  0.0348, -0.0645,  0.0832, -0.0937,  0.0147, -0.0740,  0.0303,\n",
      "        -0.0763,  0.0618,  0.0510, -0.0885, -0.0003,  0.0751,  0.0907,  0.0018,\n",
      "         0.0601, -0.0197, -0.0801,  0.0059,  0.0289,  0.0449,  0.0631,  0.0943,\n",
      "         0.0291,  0.0933, -0.0733, -0.0409,  0.0445, -0.0822,  0.0401, -0.0302,\n",
      "        -0.0338, -0.0312, -0.0785,  0.0201,  0.0649,  0.0130,  0.0244,  0.0323,\n",
      "        -0.0823, -0.0955], requires_grad=True)\n",
      "\n",
      "Name: 4.weight\n",
      "Shape: torch.Size([1, 50])\n",
      "Values: Parameter containing:\n",
      "tensor([[-0.1375,  0.0069,  0.0442, -0.0725,  0.0154, -0.1371,  0.1346,  0.0183,\n",
      "          0.1121, -0.0182, -0.0554,  0.1395,  0.0487,  0.0638,  0.1296,  0.0407,\n",
      "          0.1356, -0.0188, -0.0630,  0.0981, -0.0704, -0.0568, -0.1284, -0.1159,\n",
      "          0.0871,  0.0500, -0.0848, -0.0189, -0.0308,  0.0849, -0.0974, -0.0757,\n",
      "          0.0964,  0.1332, -0.0354,  0.1320,  0.0449,  0.1253, -0.0183, -0.1297,\n",
      "         -0.0284,  0.1005, -0.0321,  0.0727,  0.0465,  0.0230, -0.0771, -0.1272,\n",
      "         -0.0532, -0.0818]], requires_grad=True)\n",
      "\n",
      "Name: 4.bias\n",
      "Shape: torch.Size([1])\n",
      "Values: Parameter containing:\n",
      "tensor([-0.1092], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Values: {param}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "287de76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  241.25270080566406\n",
      "loss :  142.2334747314453\n",
      "loss :  151.1941375732422\n",
      "loss :  134.6925811767578\n",
      "loss :  95.9632568359375\n",
      "loss :  73.39851379394531\n",
      "loss :  77.30633544921875\n",
      "loss :  87.97480010986328\n",
      "loss :  87.84689331054688\n",
      "loss :  79.9390640258789\n",
      "loss :  76.0277328491211\n",
      "loss :  79.80854034423828\n",
      "loss :  82.26969909667969\n",
      "loss :  77.45519256591797\n",
      "loss :  69.63533020019531\n",
      "loss :  65.0558853149414\n",
      "loss :  65.23880004882812\n",
      "loss :  66.91819763183594\n",
      "loss :  66.79747772216797\n",
      "loss :  64.93596649169922\n",
      "loss :  63.78530502319336\n",
      "loss :  64.81431579589844\n",
      "loss :  66.80905151367188\n",
      "loss :  67.47039031982422\n",
      "loss :  66.1012954711914\n",
      "loss :  64.09013366699219\n",
      "loss :  63.05476379394531\n",
      "loss :  63.12393569946289\n",
      "loss :  63.17192840576172\n",
      "loss :  62.42697525024414\n",
      "loss :  61.37428665161133\n",
      "loss :  60.996517181396484\n",
      "loss :  61.45295333862305\n",
      "loss :  61.9716796875\n",
      "loss :  61.750701904296875\n",
      "loss :  60.96827697753906\n",
      "loss :  60.414634704589844\n",
      "loss :  60.352481842041016\n",
      "loss :  60.310455322265625\n",
      "loss :  59.86597442626953\n",
      "loss :  59.281002044677734\n",
      "loss :  59.04878616333008\n",
      "loss :  59.19853591918945\n",
      "loss :  59.28683090209961\n",
      "loss :  59.062564849853516\n",
      "loss :  58.75356674194336\n",
      "loss :  58.64597702026367\n",
      "loss :  58.62995147705078\n",
      "loss :  58.412925720214844\n",
      "loss :  58.01935958862305\n",
      "loss :  57.7165641784668\n",
      "loss :  57.60175323486328\n",
      "loss :  57.513946533203125\n",
      "loss :  57.30982208251953\n",
      "loss :  57.09759521484375\n",
      "loss :  57.00188064575195\n",
      "loss :  56.94584655761719\n",
      "loss :  56.78937530517578\n",
      "loss :  56.54590606689453\n",
      "loss :  56.3370246887207\n",
      "loss :  56.184146881103516\n",
      "loss :  56.01443099975586\n",
      "loss :  55.80710220336914\n",
      "loss :  55.633304595947266\n",
      "loss :  55.51803207397461\n",
      "loss :  55.38816452026367\n",
      "loss :  55.20598602294922\n",
      "loss :  55.017051696777344\n",
      "loss :  54.852783203125\n",
      "loss :  54.67709732055664\n",
      "loss :  54.477394104003906\n",
      "loss :  54.29023361206055\n",
      "loss :  54.133766174316406\n",
      "loss :  53.972721099853516\n",
      "loss :  53.78778839111328\n",
      "loss :  53.60080337524414\n",
      "loss :  53.424129486083984\n",
      "loss :  53.23802185058594\n",
      "loss :  53.03432083129883\n",
      "loss :  52.8337516784668\n",
      "loss :  52.645477294921875\n",
      "loss :  52.46363067626953\n",
      "loss :  52.27710723876953\n",
      "loss :  52.09061050415039\n",
      "loss :  51.89881896972656\n",
      "loss :  51.697715759277344\n",
      "loss :  51.49514389038086\n",
      "loss :  51.300907135009766\n",
      "loss :  51.10859680175781\n",
      "loss :  50.9119758605957\n",
      "loss :  50.71526336669922\n",
      "loss :  50.517906188964844\n",
      "loss :  50.31620407104492\n",
      "loss :  50.11408233642578\n",
      "loss :  49.91228485107422\n",
      "loss :  49.704627990722656\n",
      "loss :  49.49275207519531\n",
      "loss :  49.28032302856445\n",
      "loss :  49.05893325805664\n",
      "loss :  48.82302474975586\n",
      "loss :  48.597312927246094\n",
      "loss :  48.37773132324219\n",
      "loss :  48.15729522705078\n",
      "loss :  47.93706512451172\n",
      "loss :  47.72002029418945\n",
      "loss :  47.504085540771484\n",
      "loss :  47.28578567504883\n",
      "loss :  47.062294006347656\n",
      "loss :  46.836952209472656\n",
      "loss :  46.61357116699219\n",
      "loss :  46.39130401611328\n",
      "loss :  46.16654586791992\n",
      "loss :  45.94101333618164\n",
      "loss :  45.7138671875\n",
      "loss :  45.486019134521484\n",
      "loss :  45.258609771728516\n",
      "loss :  45.03084945678711\n",
      "loss :  44.80363464355469\n",
      "loss :  44.57594299316406\n",
      "loss :  44.3466911315918\n",
      "loss :  44.11687469482422\n",
      "loss :  43.88665771484375\n",
      "loss :  43.65475845336914\n",
      "loss :  43.422916412353516\n",
      "loss :  43.19075012207031\n",
      "loss :  42.957706451416016\n",
      "loss :  42.725135803222656\n",
      "loss :  42.492862701416016\n",
      "loss :  42.26103591918945\n",
      "loss :  42.029911041259766\n",
      "loss :  41.79731369018555\n",
      "loss :  41.564979553222656\n",
      "loss :  41.331634521484375\n",
      "loss :  41.09724044799805\n",
      "loss :  40.863067626953125\n",
      "loss :  40.62989044189453\n",
      "loss :  40.39585494995117\n",
      "loss :  40.159637451171875\n",
      "loss :  39.925968170166016\n",
      "loss :  39.69304656982422\n",
      "loss :  39.4610710144043\n",
      "loss :  39.22966766357422\n",
      "loss :  38.99745559692383\n",
      "loss :  38.76486587524414\n",
      "loss :  38.53306579589844\n",
      "loss :  38.30077362060547\n",
      "loss :  38.06718063354492\n",
      "loss :  37.83372116088867\n",
      "loss :  37.60056686401367\n",
      "loss :  37.36856460571289\n",
      "loss :  37.138404846191406\n",
      "loss :  36.909156799316406\n",
      "loss :  36.682064056396484\n",
      "loss :  36.457340240478516\n",
      "loss :  36.2324104309082\n",
      "loss :  36.00844955444336\n",
      "loss :  35.78896713256836\n",
      "loss :  35.56707763671875\n",
      "loss :  35.34658432006836\n",
      "loss :  35.129215240478516\n",
      "loss :  34.910301208496094\n",
      "loss :  34.6871337890625\n",
      "loss :  34.45783996582031\n",
      "loss :  34.22695541381836\n",
      "loss :  33.9989128112793\n",
      "loss :  33.75925827026367\n",
      "loss :  33.522789001464844\n",
      "loss :  33.303138732910156\n",
      "loss :  33.095001220703125\n",
      "loss :  32.894752502441406\n",
      "loss :  32.6972541809082\n",
      "loss :  32.503360748291016\n",
      "loss :  32.311973571777344\n",
      "loss :  32.12282943725586\n",
      "loss :  31.93583869934082\n",
      "loss :  31.747758865356445\n",
      "loss :  31.558300018310547\n",
      "loss :  31.370853424072266\n",
      "loss :  31.185747146606445\n",
      "loss :  31.00121307373047\n",
      "loss :  30.818923950195312\n",
      "loss :  30.639577865600586\n",
      "loss :  30.46044921875\n",
      "loss :  30.282041549682617\n",
      "loss :  30.105621337890625\n",
      "loss :  29.930532455444336\n",
      "loss :  29.757543563842773\n",
      "loss :  29.589075088500977\n",
      "loss :  29.422496795654297\n",
      "loss :  29.25927734375\n",
      "loss :  29.099857330322266\n",
      "loss :  28.940048217773438\n",
      "loss :  28.779796600341797\n",
      "loss :  28.619789123535156\n",
      "loss :  28.461240768432617\n",
      "loss :  28.303882598876953\n",
      "loss :  28.14848518371582\n",
      "loss :  27.994722366333008\n",
      "loss :  27.842426300048828\n",
      "loss :  27.689834594726562\n"
     ]
    }
   ],
   "source": [
    "optim = Adam(model.parameters(), lr=0.001) # 최적화(탐색 크기 및 속도)함수 선택\n",
    "criterion = nn.MSELoss() # 손실함수 선택\n",
    "epochs = 200 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    optim.zero_grad() # 기울기 값을 0으로 초기화(누적 방지)\n",
    "    \n",
    "    y_pred = model(X_train) # 예측\n",
    "    \n",
    "    loss = criterion(y_pred, y_train.view(-1, 1)) # loss 값 계산\n",
    "    \n",
    "    loss.backward() # 역전파\n",
    "    \n",
    "    optim.step() # 가중치 업데이트(다음 진행)\n",
    "    \n",
    "    print('loss : ', loss.item()) # loss 값 확인(계속 감소하면 epoch 증가 고려)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e67e3c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.4416, -0.0816, -0.1731,  ...,  0.0794,  0.1873, -0.0137],\n",
       "                      [-0.0798,  0.2187, -0.2885,  ..., -0.3209, -0.0670, -0.1988],\n",
       "                      [ 0.0644, -0.0416, -0.1089,  ..., -0.1904, -0.1277, -0.3355],\n",
       "                      ...,\n",
       "                      [ 0.0600,  0.1458, -0.1198,  ...,  0.2422,  0.0652,  0.1159],\n",
       "                      [-0.0821,  0.1568, -0.0477,  ..., -0.1333,  0.0545, -0.0744],\n",
       "                      [ 0.1647,  0.1416,  0.1406,  ..., -0.0404,  0.1652,  0.0832]])),\n",
       "             ('0.bias',\n",
       "              tensor([ 0.1125,  0.2596,  0.0401, -0.2172,  0.0533,  0.2072, -0.0579,  0.1478,\n",
       "                       0.1264,  0.1317, -0.1786, -0.2137, -0.0962, -0.1369,  0.1400, -0.2279,\n",
       "                      -0.0921,  0.1271, -0.0616, -0.1326,  0.1060, -0.0436, -0.0322, -0.1039,\n",
       "                       0.2525, -0.2761,  0.2012, -0.1327, -0.0117,  0.2302, -0.1298,  0.2208,\n",
       "                      -0.2568, -0.1314, -0.1845,  0.0075, -0.0349, -0.0025,  0.0289,  0.0061,\n",
       "                      -0.0936, -0.0507, -0.2444,  0.1680,  0.1138,  0.0651,  0.0383, -0.2033,\n",
       "                       0.2208,  0.1101, -0.1879, -0.0357,  0.1646,  0.0278, -0.2496, -0.2559,\n",
       "                       0.0321,  0.0533,  0.0240, -0.1228, -0.2542,  0.0375, -0.1365, -0.0041,\n",
       "                       0.0950,  0.2182,  0.0659,  0.2067,  0.1141,  0.2101,  0.1412, -0.1385,\n",
       "                       0.1466, -0.0461,  0.1966, -0.1150,  0.2045, -0.0145, -0.0338,  0.1722,\n",
       "                       0.2277, -0.1007, -0.2413,  0.2341, -0.1069, -0.0050, -0.1104,  0.2355,\n",
       "                       0.1403, -0.0896,  0.2755,  0.2390, -0.2138, -0.1895,  0.0724, -0.1336,\n",
       "                      -0.1330, -0.1002, -0.1890,  0.2020])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.0204, -0.1104, -0.0882,  ..., -0.0220, -0.0447,  0.0680],\n",
       "                      [-0.0247, -0.0703, -0.0479,  ..., -0.0239, -0.0341,  0.0301],\n",
       "                      [-0.0688,  0.0718, -0.0579,  ...,  0.0273, -0.0322,  0.0862],\n",
       "                      ...,\n",
       "                      [ 0.0025,  0.0641,  0.0633,  ...,  0.0250, -0.0146,  0.0085],\n",
       "                      [-0.0106,  0.0477, -0.0876,  ...,  0.0448,  0.0336, -0.0091],\n",
       "                      [-0.0406, -0.0867, -0.0723,  ..., -0.0662,  0.0515,  0.0693]])),\n",
       "             ('2.bias',\n",
       "              tensor([-0.0930,  0.0960, -0.0780,  0.0424,  0.1246, -0.0760,  0.0806,  0.0087,\n",
       "                      -0.0246,  0.0348, -0.0722,  0.0947, -0.0937,  0.0178, -0.0624,  0.0497,\n",
       "                      -0.0647,  0.0618,  0.0510, -0.0885, -0.0099,  0.0637,  0.0787, -0.0045,\n",
       "                       0.0715, -0.0105, -0.0801, -0.0063,  0.0186,  0.0706,  0.0571,  0.0913,\n",
       "                       0.0327,  0.0874, -0.0733, -0.0294,  0.0552, -0.0754,  0.0280, -0.0385,\n",
       "                      -0.0423, -0.0198, -0.0853,  0.0315,  0.0761,  0.0250,  0.0244,  0.0205,\n",
       "                      -0.0823, -0.1060])),\n",
       "             ('4.weight',\n",
       "              tensor([[-0.1430,  0.0026,  0.0450, -0.0777,  0.0859, -0.1481,  0.1934,  0.0268,\n",
       "                        0.1121, -0.0182, -0.0618,  0.1424,  0.0487,  0.0706,  0.1365,  0.0524,\n",
       "                        0.1444, -0.0188, -0.0630,  0.0981, -0.0588, -0.0552, -0.1233, -0.1098,\n",
       "                        0.0912,  0.0579, -0.0848, -0.0208, -0.0197,  0.1447, -0.0914, -0.0701,\n",
       "                        0.0957,  0.1280, -0.0354,  0.1364,  0.0414,  0.1740, -0.0198, -0.1229,\n",
       "                       -0.0209,  0.1028, -0.0255,  0.0732,  0.0466,  0.0290, -0.0771, -0.1311,\n",
       "                       -0.0532, -0.0718]])),\n",
       "             ('4.bias', tensor([-0.0976]))])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "77c2d7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  63.90715408325195\n",
      "loss :  260.35955810546875\n",
      "loss :  154.4896697998047\n",
      "loss :  210.74359130859375\n",
      "loss :  239.41749572753906\n",
      "loss :  68.94908905029297\n",
      "loss :  129.64222717285156\n",
      "loss :  162.2895965576172\n",
      "loss :  150.14764404296875\n",
      "loss :  73.15821075439453\n",
      "loss :  59.696197509765625\n",
      "loss :  134.8520965576172\n",
      "loss :  118.31626892089844\n",
      "loss :  121.11754608154297\n",
      "loss :  74.82767486572266\n",
      "loss :  73.53008270263672\n",
      "loss :  123.50568389892578\n",
      "loss :  103.26597595214844\n",
      "loss :  113.54100036621094\n",
      "loss :  51.41214370727539\n",
      "loss :  46.996490478515625\n",
      "loss :  120.8689956665039\n",
      "loss :  151.30172729492188\n",
      "loss :  104.08299255371094\n",
      "loss :  39.97507858276367\n",
      "loss :  44.93535614013672\n",
      "loss :  110.28019714355469\n",
      "loss :  116.70259857177734\n",
      "loss :  106.20465087890625\n",
      "loss :  73.62776184082031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\walker\\code\\deep-learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:616: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  49.18620681762695\n",
      "loss :  108.07474517822266\n",
      "loss :  116.54122161865234\n",
      "loss :  100.59918212890625\n",
      "loss :  41.27511978149414\n",
      "loss :  43.55937576293945\n",
      "loss :  111.96174621582031\n",
      "loss :  127.24317169189453\n",
      "loss :  102.52485656738281\n",
      "loss :  40.2081184387207\n",
      "loss :  46.29469299316406\n",
      "loss :  107.91692352294922\n",
      "loss :  107.00477600097656\n",
      "loss :  104.90272521972656\n",
      "loss :  47.30099105834961\n",
      "loss :  47.74473571777344\n",
      "loss :  107.31649780273438\n",
      "loss :  115.40717315673828\n",
      "loss :  102.9870376586914\n",
      "loss :  37.02814483642578\n",
      "loss :  43.75472640991211\n",
      "loss :  107.01167297363281\n",
      "loss :  114.9861068725586\n",
      "loss :  101.31128692626953\n",
      "loss :  43.9122428894043\n",
      "loss :  46.25493621826172\n",
      "loss :  104.89346313476562\n",
      "loss :  111.79027557373047\n",
      "loss :  100.64571380615234\n",
      "loss :  41.401493072509766\n",
      "loss :  44.14778518676758\n",
      "loss :  105.17837524414062\n",
      "loss :  117.34410858154297\n",
      "loss :  100.53702545166016\n",
      "loss :  39.50503921508789\n",
      "loss :  44.641319274902344\n",
      "loss :  104.1772689819336\n",
      "loss :  111.81107330322266\n",
      "loss :  100.62442016601562\n",
      "loss :  42.395450592041016\n",
      "loss :  45.41462707519531\n",
      "loss :  103.87532043457031\n",
      "loss :  114.20571899414062\n",
      "loss :  100.51933288574219\n",
      "loss :  38.95587921142578\n",
      "loss :  44.265708923339844\n",
      "loss :  103.57534790039062\n",
      "loss :  113.47241973876953\n",
      "loss :  100.03571319580078\n",
      "loss :  41.321449279785156\n",
      "loss :  45.046173095703125\n",
      "loss :  102.91139221191406\n",
      "loss :  113.4677734375\n",
      "loss :  99.64659881591797\n",
      "loss :  40.11437225341797\n",
      "loss :  44.13201904296875\n",
      "loss :  102.6937255859375\n",
      "loss :  114.80133819580078\n",
      "loss :  99.35224914550781\n",
      "loss :  40.41048812866211\n",
      "loss :  44.42795944213867\n",
      "loss :  102.21717071533203\n",
      "loss :  113.60059356689453\n",
      "loss :  99.25102233886719\n",
      "loss :  40.59406280517578\n",
      "loss :  44.36481857299805\n",
      "loss :  102.02529907226562\n",
      "loss :  114.44330596923828\n",
      "loss :  99.25468444824219\n",
      "loss :  39.83130645751953\n",
      "loss :  44.31320571899414\n",
      "loss :  101.71763610839844\n",
      "loss :  113.55342102050781\n",
      "loss :  99.12882995605469\n",
      "loss :  40.44415283203125\n",
      "loss :  44.43784713745117\n",
      "loss :  101.43394470214844\n",
      "loss :  114.08682250976562\n",
      "loss :  99.03636932373047\n",
      "loss :  39.8014030456543\n",
      "loss :  44.20907211303711\n",
      "loss :  101.15331268310547\n",
      "loss :  113.79548645019531\n",
      "loss :  98.86282348632812\n",
      "loss :  40.303218841552734\n",
      "loss :  44.323875427246094\n",
      "loss :  100.87139892578125\n",
      "loss :  113.95604705810547\n",
      "loss :  98.79570007324219\n",
      "loss :  39.86819076538086\n",
      "loss :  44.16102981567383\n",
      "loss :  100.64720916748047\n",
      "loss :  113.84412384033203\n",
      "loss :  98.6995620727539\n",
      "loss :  40.0826301574707\n",
      "loss :  44.26360321044922\n",
      "loss :  100.41104125976562\n",
      "loss :  113.75521087646484\n",
      "loss :  98.66173553466797\n",
      "loss :  39.86361312866211\n",
      "loss :  44.207340240478516\n",
      "loss :  100.20953369140625\n",
      "loss :  113.65779876708984\n",
      "loss :  98.58279418945312\n",
      "loss :  39.975215911865234\n",
      "loss :  44.26105499267578\n",
      "loss :  99.9820327758789\n",
      "loss :  113.59394836425781\n",
      "loss :  98.49137115478516\n",
      "loss :  39.894378662109375\n",
      "loss :  44.1939697265625\n",
      "loss :  99.77043151855469\n",
      "loss :  113.59741973876953\n",
      "loss :  98.40086364746094\n",
      "loss :  39.90792465209961\n",
      "loss :  44.21410369873047\n",
      "loss :  99.55619812011719\n",
      "loss :  113.45013427734375\n",
      "loss :  98.34468841552734\n",
      "loss :  39.90791320800781\n",
      "loss :  44.22163772583008\n",
      "loss :  99.35926818847656\n",
      "loss :  113.40608215332031\n",
      "loss :  98.28663635253906\n",
      "loss :  39.836036682128906\n",
      "loss :  44.19045639038086\n",
      "loss :  99.1615219116211\n",
      "loss :  113.32644653320312\n",
      "loss :  98.20401000976562\n",
      "loss :  39.905731201171875\n",
      "loss :  44.19681930541992\n",
      "loss :  98.9837646484375\n",
      "loss :  113.30249786376953\n",
      "loss :  98.18155670166016\n",
      "loss :  39.813297271728516\n",
      "loss :  44.21525955200195\n",
      "loss :  98.83014678955078\n",
      "loss :  113.10384368896484\n",
      "loss :  98.15875244140625\n",
      "loss :  39.83575439453125\n",
      "loss :  44.26696014404297\n",
      "loss :  98.67903137207031\n",
      "loss :  112.97559356689453\n",
      "loss :  98.08686065673828\n",
      "loss :  39.77900695800781\n",
      "loss :  44.196292877197266\n",
      "loss :  98.52503204345703\n",
      "loss :  112.99845886230469\n",
      "loss :  97.97756958007812\n",
      "loss :  39.829044342041016\n",
      "loss :  44.177330017089844\n",
      "loss :  98.36341094970703\n",
      "loss :  112.9429702758789\n",
      "loss :  97.90794372558594\n",
      "loss :  39.82578659057617\n",
      "loss :  44.17731857299805\n",
      "loss :  98.21671295166016\n",
      "loss :  112.85528564453125\n",
      "loss :  97.8703842163086\n",
      "loss :  39.80060958862305\n",
      "loss :  44.189842224121094\n",
      "loss :  98.08452606201172\n",
      "loss :  112.74436950683594\n",
      "loss :  97.84637451171875\n",
      "loss :  39.76533126831055\n",
      "loss :  44.211734771728516\n",
      "loss :  97.96361541748047\n",
      "loss :  112.62237548828125\n",
      "loss :  97.81038665771484\n",
      "loss :  39.73069381713867\n",
      "loss :  44.2034912109375\n",
      "loss :  97.83684539794922\n",
      "loss :  112.56526947021484\n",
      "loss :  97.73374938964844\n",
      "loss :  39.74208068847656\n",
      "loss :  44.17233657836914\n",
      "loss :  97.70687866210938\n",
      "loss :  112.56336212158203\n",
      "loss :  97.66651916503906\n",
      "loss :  39.705841064453125\n",
      "loss :  44.161415100097656\n",
      "loss :  97.58241271972656\n",
      "loss :  112.45806121826172\n",
      "loss :  97.61286163330078\n",
      "loss :  39.737735748291016\n",
      "loss :  44.18153381347656\n",
      "loss :  97.46373748779297\n",
      "loss :  112.40263366699219\n",
      "loss :  97.56558990478516\n",
      "loss :  39.689537048339844\n",
      "loss :  44.15928268432617\n",
      "loss :  97.32968139648438\n",
      "loss :  112.32249450683594\n",
      "loss :  97.44058227539062\n",
      "loss :  39.76656723022461\n",
      "loss :  44.110347747802734\n",
      "loss :  97.19831085205078\n",
      "loss :  112.3971939086914\n",
      "loss :  97.34573364257812\n",
      "loss :  39.73300552368164\n",
      "loss :  44.08122253417969\n",
      "loss :  97.06294250488281\n",
      "loss :  112.25947570800781\n",
      "loss :  97.26026916503906\n",
      "loss :  39.76279830932617\n",
      "loss :  44.090667724609375\n",
      "loss :  96.94656372070312\n",
      "loss :  112.21439361572266\n",
      "loss :  97.21521759033203\n",
      "loss :  39.7075309753418\n",
      "loss :  44.111244201660156\n",
      "loss :  96.81883239746094\n",
      "loss :  112.04959869384766\n",
      "loss :  97.13713836669922\n",
      "loss :  39.73075485229492\n",
      "loss :  44.09403991699219\n",
      "loss :  96.7015380859375\n",
      "loss :  112.0839614868164\n",
      "loss :  97.06514739990234\n",
      "loss :  39.67060089111328\n",
      "loss :  44.089176177978516\n",
      "loss :  96.57374572753906\n",
      "loss :  111.93711853027344\n",
      "loss :  96.96959686279297\n",
      "loss :  39.70799255371094\n",
      "loss :  44.07097625732422\n",
      "loss :  96.45513153076172\n",
      "loss :  111.9709701538086\n",
      "loss :  96.90103149414062\n",
      "loss :  39.62971115112305\n",
      "loss :  44.06207275390625\n",
      "loss :  96.332275390625\n",
      "loss :  111.82418823242188\n",
      "loss :  96.84654998779297\n",
      "loss :  39.6298942565918\n",
      "loss :  44.073028564453125\n",
      "loss :  96.2275619506836\n",
      "loss :  111.78797149658203\n",
      "loss :  96.80879211425781\n",
      "loss :  39.525543212890625\n",
      "loss :  44.035865783691406\n",
      "loss :  96.11811828613281\n",
      "loss :  111.69097137451172\n",
      "loss :  96.72431945800781\n",
      "loss :  39.577125549316406\n",
      "loss :  43.99969482421875\n",
      "loss :  96.02255249023438\n",
      "loss :  111.74777221679688\n",
      "loss :  96.68587493896484\n",
      "loss :  39.48882293701172\n",
      "loss :  43.978851318359375\n",
      "loss :  95.9267349243164\n",
      "loss :  111.58548736572266\n",
      "loss :  96.6395263671875\n",
      "loss :  39.5265007019043\n",
      "loss :  43.95344161987305\n",
      "loss :  95.85050964355469\n",
      "loss :  111.63824462890625\n",
      "loss :  96.64041900634766\n",
      "loss :  39.41270065307617\n",
      "loss :  43.96330642700195\n",
      "loss :  95.76338195800781\n",
      "loss :  111.37275695800781\n",
      "loss :  96.60419464111328\n",
      "loss :  39.495094299316406\n",
      "loss :  43.9422721862793\n",
      "loss :  95.69247436523438\n",
      "loss :  111.50313568115234\n",
      "loss :  96.60533142089844\n",
      "loss :  39.37281799316406\n",
      "loss :  43.938316345214844\n",
      "loss :  95.5991439819336\n",
      "loss :  111.2337875366211\n",
      "loss :  96.55905151367188\n",
      "loss :  39.48086929321289\n",
      "loss :  43.9151496887207\n",
      "loss :  95.53211975097656\n",
      "loss :  111.40082550048828\n",
      "loss :  96.5694351196289\n",
      "loss :  39.323455810546875\n",
      "loss :  43.918556213378906\n",
      "loss :  95.44116973876953\n",
      "loss :  111.07483673095703\n",
      "loss :  96.5074691772461\n",
      "loss :  39.47813415527344\n",
      "loss :  43.89100646972656\n",
      "loss :  95.3809814453125\n",
      "loss :  111.31489562988281\n",
      "loss :  96.50812530517578\n",
      "loss :  39.31371307373047\n",
      "loss :  43.873565673828125\n",
      "loss :  95.28359985351562\n",
      "loss :  110.97254943847656\n",
      "loss :  96.42311096191406\n",
      "loss :  39.51325988769531\n",
      "loss :  43.826141357421875\n",
      "loss :  95.2280502319336\n",
      "loss :  111.30931091308594\n",
      "loss :  96.44860076904297\n",
      "loss :  39.27162551879883\n",
      "loss :  43.84342575073242\n",
      "loss :  95.13604736328125\n",
      "loss :  110.79815673828125\n",
      "loss :  96.390869140625\n",
      "loss :  39.49220657348633\n",
      "loss :  43.82883071899414\n",
      "loss :  95.10577392578125\n",
      "loss :  111.18742370605469\n",
      "loss :  96.46366882324219\n",
      "loss :  39.187259674072266\n",
      "loss :  43.85835266113281\n",
      "loss :  95.00861358642578\n",
      "loss :  110.57967376708984\n",
      "loss :  96.36312103271484\n",
      "loss :  39.4964599609375\n",
      "loss :  43.77888488769531\n",
      "loss :  94.98344421386719\n",
      "loss :  111.2083740234375\n",
      "loss :  96.44093322753906\n",
      "loss :  39.13199234008789\n",
      "loss :  43.84298324584961\n",
      "loss :  94.88036346435547\n",
      "loss :  110.37474822998047\n",
      "loss :  96.34676361083984\n",
      "loss :  39.49311065673828\n",
      "loss :  43.7637939453125\n",
      "loss :  94.87742614746094\n",
      "loss :  111.18090057373047\n",
      "loss :  96.44768524169922\n",
      "loss :  39.06447219848633\n",
      "loss :  43.83800506591797\n",
      "loss :  94.75983428955078\n",
      "loss :  110.17620849609375\n",
      "loss :  96.28994750976562\n",
      "loss :  39.55006790161133\n",
      "loss :  43.712127685546875\n",
      "loss :  94.77447509765625\n",
      "loss :  111.24244689941406\n",
      "loss :  96.42855072021484\n",
      "loss :  39.00031280517578\n",
      "loss :  43.83412551879883\n",
      "loss :  94.64571380615234\n",
      "loss :  109.9328384399414\n",
      "loss :  96.24324798583984\n",
      "loss :  39.598121643066406\n",
      "loss :  43.671905517578125\n",
      "loss :  94.67816925048828\n",
      "loss :  111.32898712158203\n",
      "loss :  96.41131591796875\n",
      "loss :  38.900848388671875\n",
      "loss :  43.80644989013672\n",
      "loss :  94.53172302246094\n",
      "loss :  109.71221923828125\n",
      "loss :  96.15554809570312\n",
      "loss :  39.704978942871094\n",
      "loss :  43.60089874267578\n",
      "loss :  94.59098815917969\n",
      "loss :  111.5179443359375\n",
      "loss :  96.39512634277344\n",
      "loss :  38.7742805480957\n",
      "loss :  43.814979553222656\n",
      "loss :  94.42842102050781\n",
      "loss :  109.34712219238281\n",
      "loss :  96.1113510131836\n",
      "loss :  39.800045013427734\n",
      "loss :  43.58868408203125\n",
      "loss :  94.52120208740234\n",
      "loss :  111.68622589111328\n",
      "loss :  96.44244384765625\n",
      "loss :  38.59102249145508\n",
      "loss :  43.83549118041992\n",
      "loss :  94.32484436035156\n",
      "loss :  108.92945098876953\n",
      "loss :  96.041748046875\n",
      "loss :  39.98061752319336\n",
      "loss :  43.515045166015625\n",
      "loss :  94.4665756225586\n",
      "loss :  112.0198974609375\n",
      "loss :  96.5016860961914\n",
      "loss :  38.38843536376953\n",
      "loss :  43.87443161010742\n",
      "loss :  94.22981262207031\n",
      "loss :  108.33639526367188\n",
      "loss :  95.98623657226562\n",
      "loss :  40.22331619262695\n",
      "loss :  43.433250427246094\n",
      "loss :  94.42955780029297\n",
      "loss :  112.49076080322266\n",
      "loss :  96.60187530517578\n",
      "loss :  38.12260055541992\n",
      "loss :  43.91999435424805\n",
      "loss :  94.13609313964844\n",
      "loss :  107.59403228759766\n",
      "loss :  95.93123626708984\n",
      "loss :  40.58488845825195\n",
      "loss :  43.34121322631836\n",
      "loss :  94.42162322998047\n",
      "loss :  113.18902587890625\n",
      "loss :  96.77030944824219\n",
      "loss :  37.795719146728516\n",
      "loss :  43.99298095703125\n",
      "loss :  94.0435791015625\n",
      "loss :  106.67697143554688\n",
      "loss :  95.86671447753906\n",
      "loss :  41.14408874511719\n",
      "loss :  43.191009521484375\n",
      "loss :  94.45662689208984\n",
      "loss :  114.26725006103516\n",
      "loss :  97.0328598022461\n",
      "loss :  37.40169906616211\n",
      "loss :  44.096580505371094\n",
      "loss :  93.97407531738281\n",
      "loss :  105.46574401855469\n",
      "loss :  95.85990905761719\n",
      "loss :  41.93867874145508\n",
      "loss :  43.014522552490234\n",
      "loss :  94.55703735351562\n",
      "loss :  115.76290893554688\n",
      "loss :  97.4861831665039\n",
      "loss :  36.905269622802734\n",
      "loss :  44.233558654785156\n",
      "loss :  93.95638275146484\n",
      "loss :  103.9053726196289\n",
      "loss :  95.98463439941406\n",
      "loss :  43.07598114013672\n",
      "loss :  42.805274963378906\n",
      "loss :  94.78963470458984\n",
      "loss :  117.84559631347656\n",
      "loss :  98.30728912353516\n",
      "loss :  36.311256408691406\n",
      "loss :  44.446197509765625\n",
      "loss :  94.03364562988281\n",
      "loss :  101.90764617919922\n",
      "loss :  96.37454986572266\n",
      "loss :  44.76336669921875\n",
      "loss :  42.565860748291016\n",
      "loss :  95.2000961303711\n",
      "loss :  120.71009826660156\n",
      "loss :  99.62569427490234\n",
      "loss :  35.728904724121094\n",
      "loss :  44.70282745361328\n",
      "loss :  94.2606201171875\n",
      "loss :  99.55333709716797\n",
      "loss :  97.20674896240234\n",
      "loss :  47.21031188964844\n",
      "loss :  42.22123336791992\n",
      "loss :  95.9006118774414\n",
      "loss :  124.77495574951172\n",
      "loss :  101.87431335449219\n",
      "loss :  35.29447937011719\n",
      "loss :  45.02880096435547\n",
      "loss :  94.76776885986328\n",
      "loss :  96.6823501586914\n",
      "loss :  98.99205780029297\n",
      "loss :  51.16211700439453\n",
      "loss :  41.86180877685547\n",
      "loss :  97.05274963378906\n",
      "loss :  130.45321655273438\n",
      "loss :  105.39495849609375\n",
      "loss :  35.3193244934082\n",
      "loss :  45.18976593017578\n",
      "loss :  95.61907958984375\n",
      "loss :  93.68712615966797\n",
      "loss :  102.23927307128906\n",
      "loss :  57.36727523803711\n",
      "loss :  41.57628631591797\n",
      "loss :  98.57289123535156\n",
      "loss :  137.5597686767578\n",
      "loss :  111.15058135986328\n",
      "loss :  36.58882522583008\n",
      "loss :  45.18081283569336\n",
      "loss :  96.87596893310547\n",
      "loss :  90.68331909179688\n",
      "loss :  107.885986328125\n",
      "loss :  66.47046661376953\n",
      "loss :  41.558250427246094\n",
      "loss :  100.31373596191406\n",
      "loss :  145.95225524902344\n",
      "loss :  119.00259399414062\n",
      "loss :  39.44890594482422\n",
      "loss :  44.32017517089844\n",
      "loss :  97.77003479003906\n",
      "loss :  88.56786346435547\n",
      "loss :  115.56074523925781\n",
      "loss :  79.5325927734375\n",
      "loss :  42.25993728637695\n",
      "loss :  101.02784729003906\n",
      "loss :  152.34715270996094\n",
      "loss :  127.27069854736328\n",
      "loss :  44.66815948486328\n",
      "loss :  42.60968017578125\n",
      "loss :  97.48890686035156\n",
      "loss :  87.66299438476562\n",
      "loss :  122.38658142089844\n",
      "loss :  92.91212463378906\n",
      "loss :  44.29599380493164\n",
      "loss :  99.276123046875\n",
      "loss :  151.94744873046875\n",
      "loss :  130.6407928466797\n",
      "loss :  49.268375396728516\n",
      "loss :  40.34534454345703\n",
      "loss :  95.47978210449219\n",
      "loss :  88.77337646484375\n",
      "loss :  121.64839935302734\n",
      "loss :  98.34416198730469\n",
      "loss :  47.7143440246582\n",
      "loss :  95.8697738647461\n",
      "loss :  141.11474609375\n",
      "loss :  122.73284912109375\n",
      "loss :  47.261356353759766\n",
      "loss :  38.943748474121094\n",
      "loss :  94.00335693359375\n",
      "loss :  93.38888549804688\n",
      "loss :  110.70722198486328\n",
      "loss :  86.17254638671875\n",
      "loss :  50.37775802612305\n",
      "loss :  93.92033386230469\n",
      "loss :  124.35272216796875\n",
      "loss :  108.17659759521484\n",
      "loss :  39.16720199584961\n",
      "loss :  38.74918746948242\n",
      "loss :  94.44171905517578\n",
      "loss :  101.20621490478516\n",
      "loss :  99.19566345214844\n",
      "loss :  62.904884338378906\n",
      "loss :  49.476192474365234\n",
      "loss :  93.81610107421875\n",
      "loss :  111.5802230834961\n",
      "loss :  98.35083770751953\n",
      "loss :  34.243064880371094\n",
      "loss :  39.562381744384766\n",
      "loss :  94.86412048339844\n",
      "loss :  107.84880065917969\n",
      "loss :  95.10763549804688\n",
      "loss :  45.997188568115234\n",
      "loss :  46.34294128417969\n",
      "loss :  93.77775573730469\n",
      "loss :  106.02069854736328\n",
      "loss :  95.45919036865234\n",
      "loss :  36.1231803894043\n",
      "loss :  41.17970657348633\n",
      "loss :  94.45932006835938\n",
      "loss :  110.40807342529297\n",
      "loss :  95.1964111328125\n",
      "loss :  39.00221252441406\n",
      "loss :  43.87540054321289\n",
      "loss :  93.64678192138672\n",
      "loss :  104.98833465576172\n",
      "loss :  95.12042236328125\n",
      "loss :  39.178199768066406\n",
      "loss :  42.57201385498047\n",
      "loss :  94.00059509277344\n",
      "loss :  110.21601867675781\n",
      "loss :  95.46385955810547\n",
      "loss :  37.090003967285156\n",
      "loss :  42.73689270019531\n",
      "loss :  93.6554946899414\n",
      "loss :  105.5542984008789\n",
      "loss :  95.03150939941406\n",
      "loss :  40.24645233154297\n",
      "loss :  43.240386962890625\n",
      "loss :  93.76016998291016\n",
      "loss :  109.08026885986328\n",
      "loss :  95.40814971923828\n",
      "loss :  36.948387145996094\n",
      "loss :  42.44809341430664\n",
      "loss :  93.67939758300781\n",
      "loss :  106.346923828125\n",
      "loss :  94.98939514160156\n",
      "loss :  39.94785690307617\n",
      "loss :  43.316749572753906\n",
      "loss :  93.63209533691406\n",
      "loss :  108.14295959472656\n",
      "loss :  95.22832489013672\n",
      "loss :  37.385162353515625\n",
      "loss :  42.457733154296875\n",
      "loss :  93.65736389160156\n",
      "loss :  106.97702026367188\n",
      "loss :  94.97731018066406\n",
      "loss :  39.29180145263672\n",
      "loss :  43.147361755371094\n",
      "loss :  93.58235931396484\n",
      "loss :  107.54942321777344\n",
      "loss :  95.13172149658203\n",
      "loss :  37.83564758300781\n",
      "loss :  42.58690643310547\n",
      "loss :  93.62863159179688\n",
      "loss :  107.1458969116211\n",
      "loss :  95.0228271484375\n",
      "loss :  38.76930618286133\n",
      "loss :  42.9680061340332\n",
      "loss :  93.57066345214844\n",
      "loss :  107.18854522705078\n",
      "loss :  95.0755386352539\n",
      "loss :  38.141117095947266\n",
      "loss :  42.64384078979492\n",
      "loss :  93.60387420654297\n",
      "loss :  107.2153091430664\n",
      "loss :  95.0140380859375\n",
      "loss :  38.472503662109375\n",
      "loss :  42.79252243041992\n",
      "loss :  93.56424713134766\n",
      "loss :  107.04973602294922\n",
      "loss :  95.00527954101562\n",
      "loss :  38.29496765136719\n",
      "loss :  42.65193176269531\n",
      "loss :  93.58300018310547\n",
      "loss :  107.1738510131836\n",
      "loss :  94.99783325195312\n",
      "loss :  38.294578552246094\n",
      "loss :  42.69704818725586\n",
      "loss :  93.56587219238281\n",
      "loss :  106.93954467773438\n",
      "loss :  95.00141143798828\n",
      "loss :  38.30681610107422\n",
      "loss :  42.667724609375\n",
      "loss :  93.57196044921875\n",
      "loss :  107.02762603759766\n",
      "loss :  95.01318359375\n",
      "loss :  38.20024108886719\n",
      "loss :  42.63813781738281\n",
      "loss :  93.5599365234375\n",
      "loss :  106.88328552246094\n",
      "loss :  94.99589538574219\n",
      "loss :  38.268157958984375\n",
      "loss :  42.64094161987305\n",
      "loss :  93.55592346191406\n",
      "loss :  106.91707611083984\n",
      "loss :  95.01707458496094\n",
      "loss :  38.14728546142578\n",
      "loss :  42.62873840332031\n",
      "loss :  93.55269622802734\n",
      "loss :  106.75667572021484\n",
      "loss :  95.0209732055664\n",
      "loss :  38.18411636352539\n",
      "loss :  42.65434265136719\n",
      "loss :  93.54533386230469\n",
      "loss :  106.73198699951172\n",
      "loss :  95.02132415771484\n",
      "loss :  38.100284576416016\n",
      "loss :  42.617034912109375\n",
      "loss :  93.5425796508789\n",
      "loss :  106.6916732788086\n",
      "loss :  95.01936340332031\n",
      "loss :  38.117095947265625\n",
      "loss :  42.62515640258789\n",
      "loss :  93.53118133544922\n",
      "loss :  106.64301300048828\n",
      "loss :  95.00975799560547\n",
      "loss :  38.05629348754883\n",
      "loss :  42.584197998046875\n",
      "loss :  93.52898406982422\n",
      "loss :  106.65078735351562\n",
      "loss :  95.00611114501953\n",
      "loss :  38.022605895996094\n",
      "loss :  42.59854507446289\n",
      "loss :  93.52157592773438\n",
      "loss :  106.5469741821289\n",
      "loss :  95.01241302490234\n",
      "loss :  37.994972229003906\n",
      "loss :  42.60978698730469\n",
      "loss :  93.51444244384766\n",
      "loss :  106.49571990966797\n",
      "loss :  95.00191497802734\n",
      "loss :  37.933311462402344\n",
      "loss :  42.58896255493164\n",
      "loss :  93.50946807861328\n",
      "loss :  106.47673797607422\n",
      "loss :  94.98005676269531\n",
      "loss :  37.92049789428711\n",
      "loss :  42.58098602294922\n",
      "loss :  93.5006103515625\n",
      "loss :  106.45829772949219\n",
      "loss :  94.9698486328125\n",
      "loss :  37.879947662353516\n",
      "loss :  42.593475341796875\n",
      "loss :  93.49185180664062\n",
      "loss :  106.36064910888672\n",
      "loss :  94.95999908447266\n",
      "loss :  37.871070861816406\n",
      "loss :  42.58237838745117\n",
      "loss :  93.4773941040039\n",
      "loss :  106.38279724121094\n",
      "loss :  94.92884826660156\n",
      "loss :  37.87065505981445\n",
      "loss :  42.55135726928711\n",
      "loss :  93.44898223876953\n",
      "loss :  106.38294982910156\n",
      "loss :  94.8973388671875\n",
      "loss :  37.89147186279297\n",
      "loss :  42.541831970214844\n",
      "loss :  93.42782592773438\n",
      "loss :  106.36995697021484\n",
      "loss :  94.913330078125\n",
      "loss :  37.85521697998047\n",
      "loss :  42.56564712524414\n",
      "loss :  93.40670013427734\n",
      "loss :  106.26603698730469\n",
      "loss :  94.91503143310547\n",
      "loss :  37.8602409362793\n",
      "loss :  42.554256439208984\n",
      "loss :  93.3862075805664\n",
      "loss :  106.29679870605469\n",
      "loss :  94.90418243408203\n",
      "loss :  37.833274841308594\n",
      "loss :  42.54616165161133\n",
      "loss :  93.36431884765625\n",
      "loss :  106.23222351074219\n",
      "loss :  94.88858795166016\n",
      "loss :  37.85747146606445\n",
      "loss :  42.54528045654297\n",
      "loss :  93.34415435791016\n",
      "loss :  106.23844909667969\n",
      "loss :  94.88687896728516\n",
      "loss :  37.826236724853516\n",
      "loss :  42.53102493286133\n",
      "loss :  93.31890869140625\n",
      "loss :  106.18797302246094\n",
      "loss :  94.88758850097656\n",
      "loss :  37.82923126220703\n",
      "loss :  42.537010192871094\n",
      "loss :  93.30404663085938\n",
      "loss :  106.16911315917969\n",
      "loss :  94.91316223144531\n",
      "loss :  37.77119827270508\n",
      "loss :  42.5502815246582\n",
      "loss :  93.28318786621094\n",
      "loss :  106.07264709472656\n",
      "loss :  94.8902816772461\n",
      "loss :  37.8141975402832\n",
      "loss :  42.53130340576172\n",
      "loss :  93.2655258178711\n",
      "loss :  106.15512084960938\n",
      "loss :  94.87329864501953\n",
      "loss :  37.73191452026367\n",
      "loss :  42.50301742553711\n",
      "loss :  93.25227355957031\n",
      "loss :  106.07632446289062\n",
      "loss :  94.86051177978516\n",
      "loss :  37.7492561340332\n",
      "loss :  42.5231819152832\n",
      "loss :  93.23797607421875\n",
      "loss :  106.06367492675781\n",
      "loss :  94.85249328613281\n",
      "loss :  37.708988189697266\n",
      "loss :  42.49597930908203\n",
      "loss :  93.21678161621094\n",
      "loss :  106.05384826660156\n",
      "loss :  94.82876586914062\n",
      "loss :  37.74001693725586\n",
      "loss :  42.500831604003906\n",
      "loss :  93.20195007324219\n",
      "loss :  106.05154418945312\n",
      "loss :  94.84038543701172\n",
      "loss :  37.6703987121582\n",
      "loss :  42.51536560058594\n",
      "loss :  93.18389892578125\n",
      "loss :  105.9334716796875\n",
      "loss :  94.83354949951172\n",
      "loss :  37.68435287475586\n",
      "loss :  42.49348831176758\n",
      "loss :  93.16849517822266\n",
      "loss :  106.00824737548828\n",
      "loss :  94.8179702758789\n",
      "loss :  37.66557312011719\n",
      "loss :  42.485084533691406\n",
      "loss :  93.14129638671875\n",
      "loss :  105.91289520263672\n",
      "loss :  94.78616333007812\n",
      "loss :  37.700477600097656\n",
      "loss :  42.46263122558594\n",
      "loss :  93.13448333740234\n",
      "loss :  105.98434448242188\n",
      "loss :  94.80646514892578\n",
      "loss :  37.624000549316406\n",
      "loss :  42.48869705200195\n",
      "loss :  93.11141967773438\n",
      "loss :  105.78594970703125\n",
      "loss :  94.78308868408203\n",
      "loss :  37.703025817871094\n",
      "loss :  42.47195053100586\n",
      "loss :  93.10067749023438\n",
      "loss :  105.9344482421875\n",
      "loss :  94.7922592163086\n",
      "loss :  37.5864372253418\n",
      "loss :  42.467647552490234\n",
      "loss :  93.07733154296875\n",
      "loss :  105.72187042236328\n",
      "loss :  94.76499938964844\n",
      "loss :  37.70125961303711\n",
      "loss :  42.47110366821289\n",
      "loss :  93.0621566772461\n",
      "loss :  105.86934661865234\n",
      "loss :  94.77021026611328\n",
      "loss :  37.57776641845703\n",
      "loss :  42.44420623779297\n",
      "loss :  93.03974914550781\n",
      "loss :  105.71439361572266\n",
      "loss :  94.72823333740234\n",
      "loss :  37.6809196472168\n",
      "loss :  42.462669372558594\n",
      "loss :  93.03087615966797\n",
      "loss :  105.79750061035156\n",
      "loss :  94.7400131225586\n",
      "loss :  37.56787872314453\n",
      "loss :  42.472877502441406\n",
      "loss :  93.0086441040039\n",
      "loss :  105.59344482421875\n",
      "loss :  94.71036529541016\n",
      "loss :  37.67252731323242\n",
      "loss :  42.45345687866211\n",
      "loss :  92.990234375\n",
      "loss :  105.77824401855469\n",
      "loss :  94.70453643798828\n",
      "loss :  37.55128860473633\n",
      "loss :  42.42105484008789\n",
      "loss :  92.9631118774414\n",
      "loss :  105.60899353027344\n",
      "loss :  94.66845703125\n",
      "loss :  37.66260528564453\n",
      "loss :  42.431968688964844\n",
      "loss :  92.94953155517578\n",
      "loss :  105.71595764160156\n",
      "loss :  94.68416595458984\n",
      "loss :  37.54017639160156\n",
      "loss :  42.41722869873047\n",
      "loss :  92.92657470703125\n",
      "loss :  105.55921936035156\n",
      "loss :  94.65463256835938\n",
      "loss :  37.63697052001953\n",
      "loss :  42.419822692871094\n",
      "loss :  92.9106216430664\n",
      "loss :  105.66124725341797\n",
      "loss :  94.66049194335938\n",
      "loss :  37.5489501953125\n",
      "loss :  42.396671295166016\n",
      "loss :  92.88573455810547\n",
      "loss :  105.51204681396484\n",
      "loss :  94.64556884765625\n",
      "loss :  37.634185791015625\n",
      "loss :  42.3965950012207\n",
      "loss :  92.8731918334961\n",
      "loss :  105.59602355957031\n",
      "loss :  94.67091369628906\n",
      "loss :  37.54876708984375\n",
      "loss :  42.403385162353516\n",
      "loss :  92.85090637207031\n",
      "loss :  105.41133117675781\n",
      "loss :  94.65760040283203\n",
      "loss :  37.626712799072266\n",
      "loss :  42.3997688293457\n",
      "loss :  92.83927917480469\n",
      "loss :  105.50987243652344\n",
      "loss :  94.67108154296875\n",
      "loss :  37.5367317199707\n",
      "loss :  42.397457122802734\n",
      "loss :  92.8127212524414\n",
      "loss :  105.33678436279297\n",
      "loss :  94.62802124023438\n",
      "loss :  37.651668548583984\n",
      "loss :  42.38194274902344\n",
      "loss :  92.7994155883789\n",
      "loss :  105.49689483642578\n",
      "loss :  94.62373352050781\n",
      "loss :  37.541717529296875\n",
      "loss :  42.389312744140625\n",
      "loss :  92.77338409423828\n",
      "loss :  105.26514434814453\n",
      "loss :  94.57152557373047\n",
      "loss :  37.668487548828125\n",
      "loss :  42.382137298583984\n",
      "loss :  92.76508331298828\n",
      "loss :  105.44684600830078\n",
      "loss :  94.58096313476562\n",
      "loss :  37.54283905029297\n",
      "loss :  42.3907470703125\n",
      "loss :  92.73538208007812\n",
      "loss :  105.18684387207031\n",
      "loss :  94.5219955444336\n",
      "loss :  37.693138122558594\n",
      "loss :  42.36711502075195\n",
      "loss :  92.72776794433594\n",
      "loss :  105.42859649658203\n",
      "loss :  94.54351043701172\n",
      "loss :  37.53053665161133\n",
      "loss :  42.35194778442383\n",
      "loss :  92.69734191894531\n",
      "loss :  105.1325454711914\n",
      "loss :  94.51123809814453\n",
      "loss :  37.707725524902344\n",
      "loss :  42.36574172973633\n",
      "loss :  92.69092559814453\n",
      "loss :  105.35669708251953\n",
      "loss :  94.5344009399414\n",
      "loss :  37.55099868774414\n",
      "loss :  42.34659194946289\n",
      "loss :  92.65371704101562\n",
      "loss :  105.10269927978516\n",
      "loss :  94.46097564697266\n",
      "loss :  37.72573471069336\n",
      "loss :  42.3030891418457\n",
      "loss :  92.64498138427734\n",
      "loss :  105.4052734375\n",
      "loss :  94.4981689453125\n",
      "loss :  37.53923797607422\n",
      "loss :  42.32080078125\n",
      "loss :  92.6086196899414\n",
      "loss :  105.03773498535156\n",
      "loss :  94.45735168457031\n",
      "loss :  37.7159309387207\n",
      "loss :  42.29763412475586\n",
      "loss :  92.60408782958984\n",
      "loss :  105.35922241210938\n",
      "loss :  94.50879669189453\n",
      "loss :  37.50138473510742\n",
      "loss :  42.32230758666992\n",
      "loss :  92.56779479980469\n",
      "loss :  104.9167251586914\n",
      "loss :  94.44891357421875\n",
      "loss :  37.74049377441406\n",
      "loss :  42.29549789428711\n",
      "loss :  92.56706237792969\n",
      "loss :  105.34252166748047\n",
      "loss :  94.49342346191406\n",
      "loss :  37.47035217285156\n",
      "loss :  42.304771423339844\n",
      "loss :  92.52735137939453\n",
      "loss :  104.82569122314453\n",
      "loss :  94.42240142822266\n",
      "loss :  37.757076263427734\n",
      "loss :  42.27180480957031\n",
      "loss :  92.52967071533203\n",
      "loss :  105.32711029052734\n",
      "loss :  94.49918365478516\n",
      "loss :  37.447696685791016\n",
      "loss :  42.2906379699707\n",
      "loss :  92.48978424072266\n",
      "loss :  104.73977661132812\n",
      "loss :  94.4388427734375\n",
      "loss :  37.76016616821289\n",
      "loss :  42.296199798583984\n",
      "loss :  92.4975814819336\n",
      "loss :  105.21873474121094\n",
      "loss :  94.52434539794922\n",
      "loss :  37.4161376953125\n",
      "loss :  42.29746627807617\n",
      "loss :  92.45413208007812\n",
      "loss :  104.63209533691406\n",
      "loss :  94.40926361083984\n",
      "loss :  37.80051803588867\n",
      "loss :  42.25953674316406\n",
      "loss :  92.4609375\n",
      "loss :  105.26559448242188\n",
      "loss :  94.49077606201172\n",
      "loss :  37.40683364868164\n",
      "loss :  42.286067962646484\n",
      "loss :  92.41883850097656\n",
      "loss :  104.54259490966797\n",
      "loss :  94.38859558105469\n",
      "loss :  37.79384994506836\n",
      "loss :  42.258304595947266\n",
      "loss :  92.43253326416016\n",
      "loss :  105.23355102539062\n",
      "loss :  94.47525024414062\n",
      "loss :  37.38229751586914\n",
      "loss :  42.27466583251953\n",
      "loss :  92.37957000732422\n",
      "loss :  104.4637222290039\n",
      "loss :  94.34441375732422\n",
      "loss :  37.84522247314453\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "optim = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 200\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for temp in range(len(X_train) // batch_size):\n",
    "        \n",
    "        s = temp * batch_size # 0 * 100\n",
    "        e = s + batch_size    # 100\n",
    "        \n",
    "        X = X_train[s:e]\n",
    "        y = y_train[s:e]\n",
    "    \n",
    "        optim.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        print('loss : ', loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
