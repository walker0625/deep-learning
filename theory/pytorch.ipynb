{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df1f71a",
   "metadata": {},
   "source": [
    "# 파이토치\n",
    "### 이미지 처리에 특화\n",
    "### CUDA 버전 너무 높으면 다른 의존성과 충돌 - 낮은 것이 안정적일 수 있음\n",
    "### 훈련과 추론 data간에 차원을 맞춰야 함(학습 : 모델 4차원  -> 추론 : 4차원 입력)\n",
    "##### https://pytorch.org/get-started/locally/\n",
    "##### cmd : uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126(cuda 버전)\n",
    "##### cmd : nvidia-smi(cuda 버전확인)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca981840",
   "metadata": {},
   "source": [
    "### 1. 기본 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab9e307a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20262995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106d7bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor라는 type == array \n",
    "torch.tensor(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff792038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "575f908b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a8de1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bcf0a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1,2,3,4,5])\n",
    "torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eceb3252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 5) # 1로 채운 3 x 5개의 data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdfebfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(512, 512) # 이미지 : 1(흰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eacd3a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(512, 512) # 이미지 : 0(검)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27663c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(5) # 단위행렬(대각선의 요소가 1 나머지는 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5e0fb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc5eccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d5a106e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8881, 0.4669, 0.3050, 0.0500, 0.5202])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8275706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9314, 0.3802, 0.4968],\n",
       "         [0.3627, 0.5701, 0.8059],\n",
       "         [0.3776, 0.3463, 0.1191]],\n",
       "\n",
       "        [[0.6815, 0.3223, 0.3935],\n",
       "         [0.5688, 0.9092, 0.0432],\n",
       "         [0.9693, 0.5520, 0.3610]],\n",
       "\n",
       "        [[0.0199, 0.8769, 0.5897],\n",
       "         [0.9735, 0.1481, 0.3475],\n",
       "         [0.5659, 0.2220, 0.8644]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7436bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련과 추론 data간에 차원을 맞춰야 함\n",
    "data = torch.rand(3, 3, 3) # 3*3*3 = 27\n",
    "data.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9f55073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2596, 0.7923, 0.1214, 0.3026, 0.6047, 0.4404, 0.5416, 0.6734, 0.6712],\n",
       "        [0.5668, 0.9474, 0.2193, 0.5983, 0.7454, 0.1315, 0.5452, 0.2913, 0.1481],\n",
       "        [0.2993, 0.2478, 0.1716, 0.4591, 0.5671, 0.5786, 0.1082, 0.4254, 0.1229]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(9, 3) # 9*3 = 27\n",
    "data.view(3, 9) # 3*9 = 27(행렬 전환 - 이미지 반전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e9f9317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2596, 0.7923, 0.1214, 0.3026, 0.6047, 0.4404, 0.5416, 0.6734, 0.6712,\n",
       "        0.5668, 0.9474, 0.2193, 0.5983, 0.7454, 0.1315, 0.5452, 0.2913, 0.1481,\n",
       "        0.2993, 0.2478, 0.1716, 0.4591, 0.5671, 0.5786, 0.1082, 0.4254, 0.1229])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(27) # 3차원을 1차원으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f053d9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2596, 0.7923, 0.1214, 0.3026, 0.6047, 0.4404, 0.5416, 0.6734, 0.6712],\n",
       "        [0.5668, 0.9474, 0.2193, 0.5983, 0.7454, 0.1315, 0.5452, 0.2913, 0.1481],\n",
       "        [0.2993, 0.2478, 0.1716, 0.4591, 0.5671, 0.5786, 0.1082, 0.4254, 0.1229]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(3, -1) # 3행으로 하고 열은 자동으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "949b2eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2596, 0.7923, 0.1214, 0.3026, 0.6047, 0.4404, 0.5416, 0.6734, 0.6712,\n",
       "        0.5668, 0.9474, 0.2193, 0.5983, 0.7454, 0.1315, 0.5452, 0.2913, 0.1481,\n",
       "        0.2993, 0.2478, 0.1716, 0.4591, 0.5671, 0.5786, 0.1082, 0.4254, 0.1229])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(-1) # 1차원으로 변환(주로 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05be3ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape # 모양"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f46397b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dim() # 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a61e9034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype # 타입(중요)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520fca00",
   "metadata": {},
   "source": [
    "### 2. 하드웨어 설정(중요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b280f141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50301c93",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/J/Code/deep-learning/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:584\u001b[39m, in \u001b[36mget_device_name\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_name\u001b[39m(device: \u001b[33m\"\u001b[39m\u001b[33mDevice\u001b[39m\u001b[33m\"\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    573\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[32m    574\u001b[39m \n\u001b[32m    575\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m \u001b[33;03m        str: the name of the device\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/J/Code/deep-learning/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:616\u001b[39m, in \u001b[36mget_device_properties\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    604\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_properties\u001b[39m(device: \u001b[33m\"\u001b[39m\u001b[33mDevice\u001b[39m\u001b[33m\"\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m) -> _CudaDeviceProperties:\n\u001b[32m    605\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[32m    606\u001b[39m \n\u001b[32m    607\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    614\u001b[39m \u001b[33;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[32m    617\u001b[39m     device = _get_device_index(device, optional=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m device < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device >= device_count():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/J/Code/deep-learning/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:403\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    400\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    401\u001b[39m     )\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "#torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de609f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80243b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([1,2,3])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e20dc95",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 1. gpu에 data 올리기(model/data 모두 같은 공간에서 처리해야 함 - 파라미터 적용)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/J/Code/deep-learning/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:403\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    400\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    401\u001b[39m     )\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "data.to('cuda') # 1. gpu에 data 올리기(model/data 모두 같은 공간에서 처리해야 함 - 파라미터 적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b9682e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data1 = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 2. gpu에 data 올리기\u001b[39;00m\n\u001b[32m      2\u001b[39m data1\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/J/Code/deep-learning/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:403\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    400\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    401\u001b[39m     )\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "data1 = torch.tensor([1,2,3]).cuda() # 2. gpu에 data 올리기\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95805c98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata1\u001b[49m.to(\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# gpu에서 cpu로 올리기\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'data1' is not defined"
     ]
    }
   ],
   "source": [
    "data1.to('cpu') # gpu에서 cpu로 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c071615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6], device='cuda:0')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cuda() + data1 # 같은 공간에 옮겨서 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f059f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.cuda()\n",
    "data - data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45203d",
   "metadata": {},
   "source": [
    "### 3. 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62e3037e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([[1,2,3],[4,5,6]])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97fd0650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56e87c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([1, 2, 3]),\n",
       "indices=tensor([0, 0, 0]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min(dim=0) # 열(0)기준으로 작은 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48679b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([1, 4]),\n",
       "indices=tensor([0, 0]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min(dim=1) # 행(1)기준으로 작은 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d7faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([4, 5, 6]),\n",
       "indices=tensor([1, 1, 1]))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max(dim=0) # 열(0)기준으로 큰 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3b4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([3, 6]),\n",
       "indices=tensor([2, 2]))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max(dim=1) # 행(1)기준으로 큰 숫자 / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f68bef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4fdb553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adcdced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f704af9",
   "metadata": {},
   "source": [
    "### 4. 차원 편집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36ee22af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(3, 128, 128) # 3차원 이미지\n",
    "image = image.view(1, 3, 128, 128) # 4차원 이미지\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c711eabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(3, 128, 128) \n",
    "image = image.unsqueeze(dim=0) # 맨 앞 차원 추가\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bae875af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.rand(1, 128, 128)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4b76c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.squeeze() # 맨 앞 차원 제거\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0a79d",
   "metadata": {},
   "source": [
    "### 5. 모델 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33f609f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/boston.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa077bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.iloc[:, :13].values\n",
    "\n",
    "# X_train = torch.tensor(X_train)\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "\n",
    "y_train  = df.iloc[:, -1].values # 마지막 값만 \n",
    "y_train = torch.FloatTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfcb6e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=13, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn # 모델 생성에 필요한 함수 포함\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "# 모델 학습\n",
    "# 13 => 100 => ReLU => 50 => ReLU => 1\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 100),\n",
    "    nn.ReLU(), # 활성화 함수(비선형)\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 1)\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894cf3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 0.weight\n",
      "Shape: torch.Size([100, 13])\n",
      "Values: Parameter containing:\n",
      "tensor([[ 0.1059,  0.1304, -0.2473,  ...,  0.1654, -0.2266, -0.0434],\n",
      "        [-0.2063,  0.2501, -0.1128,  ..., -0.0722, -0.2576, -0.1383],\n",
      "        [-0.0039, -0.0415,  0.0828,  ...,  0.0455, -0.0399,  0.0407],\n",
      "        ...,\n",
      "        [ 0.0774,  0.2099, -0.0816,  ...,  0.0180, -0.0346,  0.2214],\n",
      "        [ 0.1526, -0.1995,  0.1974,  ...,  0.1155, -0.0940,  0.0748],\n",
      "        [-0.2280, -0.0895,  0.1546,  ...,  0.1809,  0.0532, -0.1909]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name: 0.bias\n",
      "Shape: torch.Size([100])\n",
      "Values: Parameter containing:\n",
      "tensor([ 0.0135,  0.2295, -0.1923, -0.0198, -0.2210, -0.1793, -0.2170,  0.1956,\n",
      "        -0.1018, -0.1486, -0.0503,  0.2726, -0.1932,  0.0055,  0.1442,  0.2351,\n",
      "         0.2767, -0.0635, -0.0181,  0.0771, -0.1591, -0.1050,  0.0953,  0.1774,\n",
      "        -0.2754,  0.2690,  0.0167,  0.2450, -0.0018,  0.2761,  0.0673,  0.1560,\n",
      "        -0.0265, -0.0135,  0.2013, -0.2146,  0.2205, -0.0260, -0.2480, -0.0897,\n",
      "        -0.1406,  0.2031, -0.2562, -0.1464, -0.0431,  0.1524, -0.1028, -0.0756,\n",
      "         0.1794,  0.2557,  0.2066,  0.0005,  0.0542, -0.0522,  0.1091,  0.2241,\n",
      "        -0.0652, -0.1904,  0.1318, -0.1470,  0.1636,  0.0368,  0.1021,  0.2023,\n",
      "        -0.2028,  0.2151,  0.2340,  0.1659, -0.2509, -0.0182,  0.1657,  0.1881,\n",
      "        -0.0930, -0.1050, -0.0731, -0.2361, -0.2143,  0.0711,  0.2102,  0.0150,\n",
      "        -0.2643, -0.1427,  0.1446,  0.1997,  0.2683,  0.0610, -0.1073, -0.2367,\n",
      "         0.1047, -0.0172, -0.0248,  0.2723, -0.1433, -0.0822, -0.1545, -0.1044,\n",
      "         0.1826,  0.0953,  0.0524,  0.0870], requires_grad=True)\n",
      "\n",
      "Name: 2.weight\n",
      "Shape: torch.Size([50, 100])\n",
      "Values: Parameter containing:\n",
      "tensor([[-0.0612, -0.0630,  0.0599,  ..., -0.0743, -0.0393,  0.0064],\n",
      "        [-0.0929,  0.0896,  0.0007,  ...,  0.0280,  0.0498,  0.0065],\n",
      "        [ 0.0456,  0.0353, -0.0095,  ..., -0.0773, -0.0429, -0.0282],\n",
      "        ...,\n",
      "        [ 0.0480,  0.0507, -0.0814,  ...,  0.0029,  0.0652, -0.0862],\n",
      "        [-0.0382, -0.0412,  0.0983,  ...,  0.0098,  0.0676,  0.0142],\n",
      "        [ 0.0203, -0.0501, -0.0914,  ..., -0.0464, -0.0478,  0.0218]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name: 2.bias\n",
      "Shape: torch.Size([50])\n",
      "Values: Parameter containing:\n",
      "tensor([-0.0777, -0.0176, -0.0862,  0.0616,  0.0332, -0.0450, -0.0418,  0.0564,\n",
      "        -0.0337, -0.0263, -0.0472,  0.0894, -0.0048, -0.0054, -0.0850, -0.0043,\n",
      "         0.0854, -0.0141,  0.0392,  0.0911, -0.0299,  0.0194,  0.0145,  0.0343,\n",
      "        -0.0439,  0.0405, -0.0871, -0.0576, -0.0210, -0.0165, -0.0257,  0.0176,\n",
      "        -0.0097, -0.0944,  0.0695, -0.0629,  0.0048,  0.0341,  0.0307,  0.0704,\n",
      "        -0.0619,  0.0613,  0.0490,  0.0833, -0.0427,  0.0900,  0.0036,  0.0853,\n",
      "         0.0241,  0.0649], requires_grad=True)\n",
      "\n",
      "Name: 4.weight\n",
      "Shape: torch.Size([1, 50])\n",
      "Values: Parameter containing:\n",
      "tensor([[ 0.1372,  0.1141,  0.0381,  0.0479, -0.0032, -0.0522, -0.0132, -0.0730,\n",
      "         -0.0984,  0.0749, -0.0890,  0.0063, -0.1223, -0.0406,  0.0786,  0.0821,\n",
      "         -0.0392, -0.0946,  0.0989, -0.1210,  0.0501,  0.0393,  0.0965,  0.0602,\n",
      "         -0.1062, -0.1086, -0.0801,  0.0675,  0.0625, -0.0024, -0.0105,  0.0119,\n",
      "         -0.1045,  0.0813, -0.0557,  0.1057,  0.1078, -0.0139, -0.1195, -0.0386,\n",
      "         -0.0212, -0.0432, -0.0623, -0.1221, -0.0625, -0.0842, -0.0326,  0.1031,\n",
      "         -0.1115, -0.1121]], requires_grad=True)\n",
      "\n",
      "Name: 4.bias\n",
      "Shape: torch.Size([1])\n",
      "Values: Parameter containing:\n",
      "tensor([-0.1014], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Values: {param}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "287de76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  1718.5030517578125\n",
      "loss :  1178.234130859375\n",
      "loss :  791.2654418945312\n",
      "loss :  550.1005859375\n",
      "loss :  424.1222229003906\n",
      "loss :  367.2666320800781\n",
      "loss :  349.30419921875\n",
      "loss :  349.1273193359375\n",
      "loss :  347.3112487792969\n",
      "loss :  334.42059326171875\n",
      "loss :  310.58795166015625\n",
      "loss :  276.5933837890625\n",
      "loss :  237.5851593017578\n",
      "loss :  197.58737182617188\n",
      "loss :  161.08303833007812\n",
      "loss :  131.68991088867188\n",
      "loss :  110.4965591430664\n",
      "loss :  98.28783416748047\n",
      "loss :  93.9709243774414\n",
      "loss :  95.0888442993164\n",
      "loss :  98.50685119628906\n",
      "loss :  101.7686538696289\n",
      "loss :  103.12773132324219\n",
      "loss :  102.00838470458984\n",
      "loss :  98.59288787841797\n",
      "loss :  93.68972778320312\n",
      "loss :  88.39707946777344\n",
      "loss :  83.77790069580078\n",
      "loss :  80.61080169677734\n",
      "loss :  79.21497344970703\n",
      "loss :  79.44326782226562\n",
      "loss :  80.75498962402344\n",
      "loss :  82.41484832763672\n",
      "loss :  83.7080307006836\n",
      "loss :  84.09577178955078\n",
      "loss :  83.37556457519531\n",
      "loss :  81.65596771240234\n",
      "loss :  79.2805404663086\n",
      "loss :  76.69926452636719\n",
      "loss :  74.34967803955078\n",
      "loss :  72.53977966308594\n",
      "loss :  71.4031753540039\n",
      "loss :  70.85979461669922\n",
      "loss :  70.70413208007812\n",
      "loss :  70.6415786743164\n",
      "loss :  70.37674713134766\n",
      "loss :  69.77070617675781\n",
      "loss :  68.83036041259766\n",
      "loss :  67.68038177490234\n",
      "loss :  66.52515411376953\n",
      "loss :  65.54985046386719\n",
      "loss :  64.87129974365234\n",
      "loss :  64.51031494140625\n",
      "loss :  64.39279174804688\n",
      "loss :  64.3881607055664\n",
      "loss :  64.36540985107422\n",
      "loss :  64.22480773925781\n",
      "loss :  63.93296813964844\n",
      "loss :  63.52479934692383\n",
      "loss :  63.07206344604492\n",
      "loss :  62.653045654296875\n",
      "loss :  62.32868957519531\n",
      "loss :  62.115447998046875\n",
      "loss :  61.9919319152832\n",
      "loss :  61.902442932128906\n",
      "loss :  61.79191970825195\n",
      "loss :  61.6233024597168\n",
      "loss :  61.38960647583008\n",
      "loss :  61.131507873535156\n",
      "loss :  60.88970184326172\n",
      "loss :  60.693546295166016\n",
      "loss :  60.55620574951172\n",
      "loss :  60.46455764770508\n",
      "loss :  60.39596939086914\n",
      "loss :  60.32479476928711\n",
      "loss :  60.23237609863281\n",
      "loss :  60.113101959228516\n",
      "loss :  59.977203369140625\n",
      "loss :  59.841636657714844\n",
      "loss :  59.722679138183594\n",
      "loss :  59.62672805786133\n",
      "loss :  59.55002975463867\n",
      "loss :  59.4799690246582\n",
      "loss :  59.404178619384766\n",
      "loss :  59.31539535522461\n",
      "loss :  59.2138786315918\n",
      "loss :  59.105491638183594\n",
      "loss :  58.99912643432617\n",
      "loss :  58.90208053588867\n",
      "loss :  58.81637954711914\n",
      "loss :  58.73668670654297\n",
      "loss :  58.656211853027344\n",
      "loss :  58.57085418701172\n",
      "loss :  58.47654724121094\n",
      "loss :  58.37225341796875\n",
      "loss :  58.263710021972656\n",
      "loss :  58.15255355834961\n",
      "loss :  58.044742584228516\n",
      "loss :  57.937503814697266\n",
      "loss :  57.836212158203125\n",
      "loss :  57.73849105834961\n",
      "loss :  57.64906692504883\n",
      "loss :  57.568206787109375\n",
      "loss :  57.4943962097168\n",
      "loss :  57.420841217041016\n",
      "loss :  57.34931945800781\n",
      "loss :  57.278995513916016\n",
      "loss :  57.20924377441406\n",
      "loss :  57.1395378112793\n",
      "loss :  57.06999588012695\n",
      "loss :  57.00080871582031\n",
      "loss :  56.93262481689453\n",
      "loss :  56.865272521972656\n",
      "loss :  56.79779815673828\n",
      "loss :  56.72964859008789\n",
      "loss :  56.66011428833008\n",
      "loss :  56.58924865722656\n",
      "loss :  56.518611907958984\n",
      "loss :  56.44961166381836\n",
      "loss :  56.38188171386719\n",
      "loss :  56.313106536865234\n",
      "loss :  56.2444953918457\n",
      "loss :  56.175472259521484\n",
      "loss :  56.10343551635742\n",
      "loss :  56.031673431396484\n",
      "loss :  55.960750579833984\n",
      "loss :  55.89021682739258\n",
      "loss :  55.8209228515625\n",
      "loss :  55.75278091430664\n",
      "loss :  55.688480377197266\n",
      "loss :  55.627017974853516\n",
      "loss :  55.565704345703125\n",
      "loss :  55.5035400390625\n",
      "loss :  55.44084167480469\n",
      "loss :  55.37816619873047\n",
      "loss :  55.31578063964844\n",
      "loss :  55.25355529785156\n",
      "loss :  55.191627502441406\n",
      "loss :  55.13017272949219\n",
      "loss :  55.069374084472656\n",
      "loss :  55.0088996887207\n",
      "loss :  54.94850540161133\n",
      "loss :  54.88832473754883\n",
      "loss :  54.8286018371582\n",
      "loss :  54.76903533935547\n",
      "loss :  54.70938491821289\n",
      "loss :  54.649410247802734\n",
      "loss :  54.58903503417969\n",
      "loss :  54.528526306152344\n",
      "loss :  54.46786117553711\n",
      "loss :  54.407344818115234\n",
      "loss :  54.34685516357422\n",
      "loss :  54.286319732666016\n",
      "loss :  54.225807189941406\n",
      "loss :  54.165313720703125\n",
      "loss :  54.10491180419922\n",
      "loss :  54.044769287109375\n",
      "loss :  53.9846305847168\n",
      "loss :  53.92450714111328\n",
      "loss :  53.86421203613281\n",
      "loss :  53.803768157958984\n",
      "loss :  53.743282318115234\n",
      "loss :  53.682777404785156\n",
      "loss :  53.622196197509766\n",
      "loss :  53.561641693115234\n",
      "loss :  53.500797271728516\n",
      "loss :  53.43984603881836\n",
      "loss :  53.37870788574219\n",
      "loss :  53.31741714477539\n",
      "loss :  53.25586700439453\n",
      "loss :  53.194156646728516\n",
      "loss :  53.13236999511719\n",
      "loss :  53.070526123046875\n",
      "loss :  53.0084114074707\n",
      "loss :  52.94636917114258\n",
      "loss :  52.88442611694336\n",
      "loss :  52.82233810424805\n",
      "loss :  52.7602653503418\n",
      "loss :  52.697975158691406\n",
      "loss :  52.63536834716797\n",
      "loss :  52.572689056396484\n",
      "loss :  52.50985336303711\n",
      "loss :  52.44684982299805\n",
      "loss :  52.38364791870117\n",
      "loss :  52.32024383544922\n",
      "loss :  52.2565803527832\n",
      "loss :  52.19279479980469\n",
      "loss :  52.128787994384766\n",
      "loss :  52.064544677734375\n",
      "loss :  52.000186920166016\n",
      "loss :  51.93559646606445\n",
      "loss :  51.87056350708008\n",
      "loss :  51.805213928222656\n",
      "loss :  51.73980712890625\n",
      "loss :  51.67411804199219\n",
      "loss :  51.60816192626953\n",
      "loss :  51.54194641113281\n",
      "loss :  51.475433349609375\n",
      "loss :  51.40864944458008\n",
      "loss :  51.34183883666992\n"
     ]
    }
   ],
   "source": [
    "optim = Adam(model.parameters(), lr=0.001) # 최적화(탐색 크기 및 속도)함수 선택\n",
    "criterion = nn.MSELoss() # 손실함수 선택\n",
    "epochs = 200 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    optim.zero_grad() # 기울기 값을 0으로 초기화(누적 방지)\n",
    "    \n",
    "    y_pred = model(X_train) # 예측\n",
    "    \n",
    "    loss = criterion(y_pred, y_train.view(-1, 1)) # loss 값 계산\n",
    "    \n",
    "    loss.backward() # 역전파\n",
    "    \n",
    "    optim.step() # 가중치 업데이트(다음 진행)\n",
    "    \n",
    "    print('loss : ', loss.item()) # loss 값 확인(계속 감소하면 epoch 증가 고려)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e67e3c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.1155,  0.1100, -0.2732,  ...,  0.1456, -0.2396, -0.0290],\n",
       "                      [-0.2221,  0.2501, -0.1248,  ..., -0.0838, -0.2634, -0.1532],\n",
       "                      [-0.0039, -0.0415,  0.0828,  ...,  0.0455, -0.0399,  0.0407],\n",
       "                      ...,\n",
       "                      [ 0.0774,  0.2099, -0.0816,  ...,  0.0180, -0.0346,  0.2214],\n",
       "                      [ 0.1348, -0.1934,  0.1760,  ...,  0.1026, -0.1052,  0.0673],\n",
       "                      [-0.2280, -0.0895,  0.1546,  ...,  0.1809,  0.0532, -0.1909]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.0105,  0.2183, -0.1923,  0.0083, -0.2165, -0.1783, -0.2281,  0.1782,\n",
       "                      -0.1018, -0.1671, -0.0257,  0.2546, -0.1882,  0.0309,  0.1305,  0.2351,\n",
       "                       0.2950, -0.0819, -0.0181,  0.0771, -0.1495, -0.1050,  0.0802,  0.1774,\n",
       "                      -0.2936,  0.2793,  0.0167,  0.2301, -0.0018,  0.2747,  0.0782,  0.1722,\n",
       "                      -0.0555, -0.0162,  0.1965, -0.2178,  0.2205, -0.0260, -0.2480, -0.0854,\n",
       "                      -0.1466,  0.2031, -0.2562, -0.1337, -0.0208,  0.1763, -0.1028, -0.0903,\n",
       "                       0.1794,  0.2416,  0.2252,  0.0005,  0.0542, -0.0361,  0.1220,  0.2738,\n",
       "                      -0.0652, -0.2042,  0.1168, -0.1268,  0.1832,  0.0526,  0.0929,  0.2023,\n",
       "                      -0.2028,  0.2151,  0.2340,  0.1671, -0.2231, -0.0313,  0.1657,  0.1838,\n",
       "                      -0.0748, -0.1050, -0.0711, -0.2361, -0.2238,  0.0731,  0.2046,  0.0250,\n",
       "                      -0.2623, -0.1427,  0.1446,  0.1997,  0.2683,  0.0610, -0.1073, -0.2367,\n",
       "                       0.1047,  0.0084, -0.0428,  0.2723, -0.1433, -0.1064, -0.1476, -0.1290,\n",
       "                       0.2000,  0.0953,  0.0384,  0.0870])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.0610, -0.0657,  0.0599,  ..., -0.0743, -0.0369,  0.0064],\n",
       "                      [-0.0929,  0.0868,  0.0007,  ...,  0.0280,  0.0522,  0.0065],\n",
       "                      [ 0.0451,  0.0315, -0.0095,  ..., -0.0773, -0.0413, -0.0282],\n",
       "                      ...,\n",
       "                      [ 0.0385,  0.0358, -0.0814,  ...,  0.0029,  0.0572, -0.0862],\n",
       "                      [-0.0388, -0.0391,  0.0983,  ...,  0.0098,  0.0644,  0.0142],\n",
       "                      [ 0.0198, -0.0478, -0.0914,  ..., -0.0464, -0.0512,  0.0218]])),\n",
       "             ('2.bias',\n",
       "              tensor([-0.0598,  0.0001, -0.0682,  0.0616,  0.0404, -0.0450, -0.0575,  0.0475,\n",
       "                      -0.0475, -0.0073, -0.0546,  0.1082, -0.0048, -0.0146, -0.0850,  0.0118,\n",
       "                       0.0854, -0.0175,  0.0392,  0.0784, -0.0159,  0.0374,  0.0323,  0.0523,\n",
       "                      -0.0439,  0.0230, -0.0871, -0.0409, -0.0031, -0.0165, -0.0350,  0.0360,\n",
       "                      -0.0123, -0.1012,  0.0521, -0.0647,  0.0048,  0.0316,  0.0167,  0.0568,\n",
       "                      -0.0756,  0.0449,  0.0335,  0.0725, -0.0439,  0.0815, -0.0137,  0.0779,\n",
       "                       0.0087,  0.0475])),\n",
       "             ('4.weight',\n",
       "              tensor([[ 0.1468,  0.1215,  0.0467,  0.0479,  0.0063, -0.0522,  0.0019, -0.0642,\n",
       "                       -0.0865,  0.0728, -0.0816,  0.0161, -0.1223, -0.0317,  0.0786,  0.0776,\n",
       "                       -0.0392, -0.1154,  0.0989, -0.1089,  0.0718,  0.0436,  0.1029,  0.0720,\n",
       "                       -0.1062, -0.0941, -0.0801,  0.0765,  0.0736, -0.0024, -0.0016,  0.0202,\n",
       "                       -0.1021,  0.0701, -0.0427,  0.0955,  0.1078, -0.0116, -0.1074, -0.0248,\n",
       "                       -0.0115, -0.0324, -0.0479, -0.1124, -0.0688, -0.0755, -0.0166,  0.0916,\n",
       "                       -0.1017, -0.0997]])),\n",
       "             ('4.bias', tensor([-0.0835]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77c2d7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minwoojeon/J/Code/deep-learning/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  55.77470779418945\n",
      "loss :  181.104736328125\n",
      "loss :  138.63299560546875\n",
      "loss :  110.1680908203125\n",
      "loss :  104.98490142822266\n",
      "loss :  67.50037384033203\n",
      "loss :  110.71512603759766\n",
      "loss :  110.31269836425781\n",
      "loss :  102.3504409790039\n",
      "loss :  36.832801818847656\n",
      "loss :  46.06622314453125\n",
      "loss :  125.60594940185547\n",
      "loss :  128.44873046875\n",
      "loss :  101.14525604248047\n",
      "loss :  40.6799430847168\n",
      "loss :  48.708580017089844\n",
      "loss :  109.6517105102539\n",
      "loss :  104.44902801513672\n",
      "loss :  105.7396011352539\n",
      "loss :  59.61597442626953\n",
      "loss :  49.12688064575195\n",
      "loss :  109.31787872314453\n",
      "loss :  116.74935913085938\n",
      "loss :  97.41619110107422\n",
      "loss :  39.127967834472656\n",
      "loss :  42.855873107910156\n",
      "loss :  113.64601135253906\n",
      "loss :  121.91422271728516\n",
      "loss :  96.5797348022461\n",
      "loss :  45.05255889892578\n",
      "loss :  44.812992095947266\n",
      "loss :  107.10563659667969\n",
      "loss :  110.18820190429688\n",
      "loss :  98.68466186523438\n",
      "loss :  50.553550720214844\n",
      "loss :  45.0528450012207\n",
      "loss :  107.99862670898438\n",
      "loss :  115.86479949951172\n",
      "loss :  96.76392364501953\n",
      "loss :  40.42467498779297\n",
      "loss :  43.10487365722656\n",
      "loss :  109.21150207519531\n",
      "loss :  114.84272766113281\n",
      "loss :  97.20318603515625\n",
      "loss :  44.104949951171875\n",
      "loss :  45.12471389770508\n",
      "loss :  106.52517700195312\n",
      "loss :  109.7187271118164\n",
      "loss :  97.76705932617188\n",
      "loss :  44.517250061035156\n",
      "loss :  44.730735778808594\n",
      "loss :  107.15174865722656\n",
      "loss :  113.26370239257812\n",
      "loss :  97.07058715820312\n",
      "loss :  40.5927619934082\n",
      "loss :  43.83682632446289\n",
      "loss :  106.88894653320312\n",
      "loss :  112.04975128173828\n",
      "loss :  97.04306030273438\n",
      "loss :  43.32988739013672\n",
      "loss :  44.75081253051758\n",
      "loss :  105.66504669189453\n",
      "loss :  111.00392150878906\n",
      "loss :  96.93685913085938\n",
      "loss :  42.14748764038086\n",
      "loss :  44.136844635009766\n",
      "loss :  105.98092651367188\n",
      "loss :  112.61417388916016\n",
      "loss :  96.8078384399414\n",
      "loss :  40.98128890991211\n",
      "loss :  44.12493896484375\n",
      "loss :  105.38592529296875\n",
      "loss :  111.13449096679688\n",
      "loss :  96.92394256591797\n",
      "loss :  42.223289489746094\n",
      "loss :  44.550010681152344\n",
      "loss :  104.93546295166016\n",
      "loss :  111.19595336914062\n",
      "loss :  96.90453338623047\n",
      "loss :  40.94343948364258\n",
      "loss :  44.1688346862793\n",
      "loss :  104.91398620605469\n",
      "loss :  111.51131439208984\n",
      "loss :  96.87114715576172\n",
      "loss :  41.04682540893555\n",
      "loss :  44.33746337890625\n",
      "loss :  104.39315032958984\n",
      "loss :  110.85802459716797\n",
      "loss :  96.82484436035156\n",
      "loss :  41.26699447631836\n",
      "loss :  44.29887008666992\n",
      "loss :  104.19517517089844\n",
      "loss :  111.33518981933594\n",
      "loss :  96.73318481445312\n",
      "loss :  40.617462158203125\n",
      "loss :  44.11929702758789\n",
      "loss :  103.96144104003906\n",
      "loss :  111.1873779296875\n",
      "loss :  96.6650619506836\n",
      "loss :  40.93973159790039\n",
      "loss :  44.2349853515625\n",
      "loss :  103.62431335449219\n",
      "loss :  111.06330108642578\n",
      "loss :  96.60948944091797\n",
      "loss :  40.67490768432617\n",
      "loss :  44.11864471435547\n",
      "loss :  103.46607208251953\n",
      "loss :  111.25614929199219\n",
      "loss :  96.55545043945312\n",
      "loss :  40.496673583984375\n",
      "loss :  44.120418548583984\n",
      "loss :  103.20758056640625\n",
      "loss :  111.04853820800781\n",
      "loss :  96.5135726928711\n",
      "loss :  40.5750732421875\n",
      "loss :  44.16383361816406\n",
      "loss :  102.99693298339844\n",
      "loss :  111.09507751464844\n",
      "loss :  96.4827651977539\n",
      "loss :  40.321800231933594\n",
      "loss :  44.12141799926758\n",
      "loss :  102.81461334228516\n",
      "loss :  111.06863403320312\n",
      "loss :  96.44927215576172\n",
      "loss :  40.32049560546875\n",
      "loss :  44.16963195800781\n",
      "loss :  102.589599609375\n",
      "loss :  110.99166107177734\n",
      "loss :  96.4228286743164\n",
      "loss :  40.23101043701172\n",
      "loss :  44.16458511352539\n",
      "loss :  102.41061401367188\n",
      "loss :  111.02876281738281\n",
      "loss :  96.4002685546875\n",
      "loss :  40.118099212646484\n",
      "loss :  44.165645599365234\n",
      "loss :  102.21405029296875\n",
      "loss :  110.97826385498047\n",
      "loss :  96.37283325195312\n",
      "loss :  40.09859085083008\n",
      "loss :  44.18572998046875\n",
      "loss :  102.02214050292969\n",
      "loss :  110.9756851196289\n",
      "loss :  96.34591674804688\n",
      "loss :  39.99497604370117\n",
      "loss :  44.18436813354492\n",
      "loss :  101.8437271118164\n",
      "loss :  110.95498657226562\n",
      "loss :  96.31535339355469\n",
      "loss :  39.97042465209961\n",
      "loss :  44.20479202270508\n",
      "loss :  101.6524658203125\n",
      "loss :  110.92594909667969\n",
      "loss :  96.2733154296875\n",
      "loss :  39.93119812011719\n",
      "loss :  44.194610595703125\n",
      "loss :  101.47758483886719\n",
      "loss :  110.9525146484375\n",
      "loss :  96.22608184814453\n",
      "loss :  39.88313674926758\n",
      "loss :  44.181907653808594\n",
      "loss :  101.30288696289062\n",
      "loss :  110.95118713378906\n",
      "loss :  96.17208862304688\n",
      "loss :  39.86335372924805\n",
      "loss :  44.16010665893555\n",
      "loss :  101.1339340209961\n",
      "loss :  110.99116516113281\n",
      "loss :  96.1182632446289\n",
      "loss :  39.8207893371582\n",
      "loss :  44.142398834228516\n",
      "loss :  100.96443939208984\n",
      "loss :  110.98979949951172\n",
      "loss :  96.06685638427734\n",
      "loss :  39.823238372802734\n",
      "loss :  44.140380859375\n",
      "loss :  100.79336547851562\n",
      "loss :  110.99714660644531\n",
      "loss :  96.0201187133789\n",
      "loss :  39.76667404174805\n",
      "loss :  44.10799026489258\n",
      "loss :  100.6372299194336\n",
      "loss :  111.02200317382812\n",
      "loss :  95.97401428222656\n",
      "loss :  39.69924545288086\n",
      "loss :  44.070804595947266\n",
      "loss :  100.48817443847656\n",
      "loss :  111.0139389038086\n",
      "loss :  95.93741607666016\n",
      "loss :  39.66569519042969\n",
      "loss :  44.05095672607422\n",
      "loss :  100.3305892944336\n",
      "loss :  110.92530059814453\n",
      "loss :  95.9141616821289\n",
      "loss :  39.618106842041016\n",
      "loss :  44.018795013427734\n",
      "loss :  100.20024871826172\n",
      "loss :  110.88504791259766\n",
      "loss :  95.91849517822266\n",
      "loss :  39.515804290771484\n",
      "loss :  44.011295318603516\n",
      "loss :  100.05946350097656\n",
      "loss :  110.6937255859375\n",
      "loss :  95.9349136352539\n",
      "loss :  39.50373840332031\n",
      "loss :  44.01225662231445\n",
      "loss :  99.93842315673828\n",
      "loss :  110.61710357666016\n",
      "loss :  95.97420501708984\n",
      "loss :  39.37312316894531\n",
      "loss :  43.981048583984375\n",
      "loss :  99.81024169921875\n",
      "loss :  110.49942779541016\n",
      "loss :  95.99137878417969\n",
      "loss :  39.42154312133789\n",
      "loss :  43.983489990234375\n",
      "loss :  99.659912109375\n",
      "loss :  110.44490051269531\n",
      "loss :  95.97323608398438\n",
      "loss :  39.39851379394531\n",
      "loss :  43.93345642089844\n",
      "loss :  99.53610229492188\n",
      "loss :  110.52936553955078\n",
      "loss :  95.93821716308594\n",
      "loss :  39.35415267944336\n",
      "loss :  43.90108108520508\n",
      "loss :  99.39701080322266\n",
      "loss :  110.51599884033203\n",
      "loss :  95.894287109375\n",
      "loss :  39.38906478881836\n",
      "loss :  43.88967514038086\n",
      "loss :  99.26676177978516\n",
      "loss :  110.55101013183594\n",
      "loss :  95.86659240722656\n",
      "loss :  39.341678619384766\n",
      "loss :  43.87383270263672\n",
      "loss :  99.14202880859375\n",
      "loss :  110.54088592529297\n",
      "loss :  95.83540344238281\n",
      "loss :  39.34583282470703\n",
      "loss :  43.870235443115234\n",
      "loss :  99.01862335205078\n",
      "loss :  110.57547760009766\n",
      "loss :  95.80359649658203\n",
      "loss :  39.31048583984375\n",
      "loss :  43.852115631103516\n",
      "loss :  98.8983383178711\n",
      "loss :  110.5864028930664\n",
      "loss :  95.77212524414062\n",
      "loss :  39.30793762207031\n",
      "loss :  43.840736389160156\n",
      "loss :  98.7858657836914\n",
      "loss :  110.60665130615234\n",
      "loss :  95.75849914550781\n",
      "loss :  39.25806427001953\n",
      "loss :  43.8364143371582\n",
      "loss :  98.68084716796875\n",
      "loss :  110.57286071777344\n",
      "loss :  95.7587661743164\n",
      "loss :  39.24113082885742\n",
      "loss :  43.850067138671875\n",
      "loss :  98.57707214355469\n",
      "loss :  110.53921508789062\n",
      "loss :  95.75836181640625\n",
      "loss :  39.22166061401367\n",
      "loss :  43.85293960571289\n",
      "loss :  98.47633361816406\n",
      "loss :  110.5302734375\n",
      "loss :  95.7542724609375\n",
      "loss :  39.18232345581055\n",
      "loss :  43.8498420715332\n",
      "loss :  98.37858581542969\n",
      "loss :  110.50125885009766\n",
      "loss :  95.75238037109375\n",
      "loss :  39.17814254760742\n",
      "loss :  43.85922622680664\n",
      "loss :  98.28099822998047\n",
      "loss :  110.46626281738281\n",
      "loss :  95.75064849853516\n",
      "loss :  39.153099060058594\n",
      "loss :  43.85478973388672\n",
      "loss :  98.18993377685547\n",
      "loss :  110.45277404785156\n",
      "loss :  95.74647521972656\n",
      "loss :  39.140560150146484\n",
      "loss :  43.8558235168457\n",
      "loss :  98.09526062011719\n",
      "loss :  110.42302703857422\n",
      "loss :  95.73831176757812\n",
      "loss :  39.14338684082031\n",
      "loss :  43.85486602783203\n",
      "loss :  98.00365447998047\n",
      "loss :  110.42835998535156\n",
      "loss :  95.73947143554688\n",
      "loss :  39.095558166503906\n",
      "loss :  43.85009765625\n",
      "loss :  97.9184341430664\n",
      "loss :  110.38539123535156\n",
      "loss :  95.74340057373047\n",
      "loss :  39.10093688964844\n",
      "loss :  43.872745513916016\n",
      "loss :  97.831787109375\n",
      "loss :  110.33102416992188\n",
      "loss :  95.74815368652344\n",
      "loss :  39.0910758972168\n",
      "loss :  43.874969482421875\n",
      "loss :  97.74828338623047\n",
      "loss :  110.3154525756836\n",
      "loss :  95.74038696289062\n",
      "loss :  39.06974792480469\n",
      "loss :  43.860755920410156\n",
      "loss :  97.66706848144531\n",
      "loss :  110.30872344970703\n",
      "loss :  95.7252197265625\n",
      "loss :  39.07927322387695\n",
      "loss :  43.85582733154297\n",
      "loss :  97.58171081542969\n",
      "loss :  110.29277801513672\n",
      "loss :  95.72338104248047\n",
      "loss :  39.06013870239258\n",
      "loss :  43.86051559448242\n",
      "loss :  97.50250244140625\n",
      "loss :  110.23507690429688\n",
      "loss :  95.72740936279297\n",
      "loss :  39.061641693115234\n",
      "loss :  43.870765686035156\n",
      "loss :  97.42572784423828\n",
      "loss :  110.21109008789062\n",
      "loss :  95.7233657836914\n",
      "loss :  39.047218322753906\n",
      "loss :  43.86075210571289\n",
      "loss :  97.34519958496094\n",
      "loss :  110.20158386230469\n",
      "loss :  95.71017456054688\n",
      "loss :  39.05279541015625\n",
      "loss :  43.85792922973633\n",
      "loss :  97.26629638671875\n",
      "loss :  110.17523956298828\n",
      "loss :  95.70660400390625\n",
      "loss :  39.04610061645508\n",
      "loss :  43.861595153808594\n",
      "loss :  97.18970489501953\n",
      "loss :  110.12157440185547\n",
      "loss :  95.70976257324219\n",
      "loss :  39.04857635498047\n",
      "loss :  43.86626434326172\n",
      "loss :  97.1167221069336\n",
      "loss :  110.10269927978516\n",
      "loss :  95.70353698730469\n",
      "loss :  39.0345344543457\n",
      "loss :  43.85548782348633\n",
      "loss :  97.04086303710938\n",
      "loss :  110.08867645263672\n",
      "loss :  95.6964340209961\n",
      "loss :  39.04866027832031\n",
      "loss :  43.85933303833008\n",
      "loss :  96.96531677246094\n",
      "loss :  110.04679870605469\n",
      "loss :  95.69844818115234\n",
      "loss :  39.03426742553711\n",
      "loss :  43.8651237487793\n",
      "loss :  96.8979721069336\n",
      "loss :  110.00173950195312\n",
      "loss :  95.71880340576172\n",
      "loss :  39.00374984741211\n",
      "loss :  43.887699127197266\n",
      "loss :  96.83165740966797\n",
      "loss :  110.29193878173828\n",
      "loss :  95.71600341796875\n",
      "loss :  39.097354888916016\n",
      "loss :  43.953033447265625\n",
      "loss :  96.7542724609375\n",
      "loss :  109.82955932617188\n",
      "loss :  95.7422866821289\n",
      "loss :  38.97511291503906\n",
      "loss :  43.87766647338867\n",
      "loss :  96.69737243652344\n",
      "loss :  109.91218566894531\n",
      "loss :  95.7040023803711\n",
      "loss :  39.02606964111328\n",
      "loss :  43.865623474121094\n",
      "loss :  96.61526489257812\n",
      "loss :  109.88561248779297\n",
      "loss :  95.69542694091797\n",
      "loss :  39.02438735961914\n",
      "loss :  43.869667053222656\n",
      "loss :  96.55806732177734\n",
      "loss :  109.86467742919922\n",
      "loss :  95.7083511352539\n",
      "loss :  39.0032958984375\n",
      "loss :  43.888755798339844\n",
      "loss :  96.48777770996094\n",
      "loss :  109.7718734741211\n",
      "loss :  95.69461059570312\n",
      "loss :  39.03447341918945\n",
      "loss :  43.88058853149414\n",
      "loss :  96.42610168457031\n",
      "loss :  109.81050109863281\n",
      "loss :  95.6888656616211\n",
      "loss :  39.005271911621094\n",
      "loss :  43.86595916748047\n",
      "loss :  96.35686492919922\n",
      "loss :  109.75323486328125\n",
      "loss :  95.67047119140625\n",
      "loss :  39.04105758666992\n",
      "loss :  43.864837646484375\n",
      "loss :  96.30178833007812\n",
      "loss :  109.76934051513672\n",
      "loss :  95.6845932006836\n",
      "loss :  39.001121520996094\n",
      "loss :  43.87474822998047\n",
      "loss :  96.23249816894531\n",
      "loss :  109.6542739868164\n",
      "loss :  95.68291473388672\n",
      "loss :  39.0399055480957\n",
      "loss :  43.887454986572266\n",
      "loss :  96.18193054199219\n",
      "loss :  109.66537475585938\n",
      "loss :  95.69818878173828\n",
      "loss :  38.98344802856445\n",
      "loss :  43.88447570800781\n",
      "loss :  96.11796569824219\n",
      "loss :  109.5756607055664\n",
      "loss :  95.67533874511719\n",
      "loss :  39.05005645751953\n",
      "loss :  43.886314392089844\n",
      "loss :  96.06510162353516\n",
      "loss :  109.61753845214844\n",
      "loss :  95.6875\n",
      "loss :  38.983558654785156\n",
      "loss :  43.86827850341797\n",
      "loss :  95.99806213378906\n",
      "loss :  109.55431365966797\n",
      "loss :  95.67427825927734\n",
      "loss :  39.04896926879883\n",
      "loss :  43.86943435668945\n",
      "loss :  95.94767761230469\n",
      "loss :  109.64508819580078\n",
      "loss :  95.72061920166016\n",
      "loss :  38.945526123046875\n",
      "loss :  43.90073013305664\n",
      "loss :  95.88977813720703\n",
      "loss :  109.3609619140625\n",
      "loss :  95.71797943115234\n",
      "loss :  39.053306579589844\n",
      "loss :  43.916481018066406\n",
      "loss :  95.83380889892578\n",
      "loss :  109.4566650390625\n",
      "loss :  95.70762634277344\n",
      "loss :  38.96724319458008\n",
      "loss :  43.859493255615234\n",
      "loss :  95.77349090576172\n",
      "loss :  109.44271087646484\n",
      "loss :  95.67369842529297\n",
      "loss :  39.03857421875\n",
      "loss :  43.87071228027344\n",
      "loss :  95.71622467041016\n",
      "loss :  109.43119049072266\n",
      "loss :  95.68111419677734\n",
      "loss :  38.99961853027344\n",
      "loss :  43.86930465698242\n",
      "loss :  95.6654281616211\n",
      "loss :  109.38387298583984\n",
      "loss :  95.68568420410156\n",
      "loss :  39.023277282714844\n",
      "loss :  43.89114761352539\n",
      "loss :  95.60627746582031\n",
      "loss :  109.31737518310547\n",
      "loss :  95.69605255126953\n",
      "loss :  39.00090026855469\n",
      "loss :  43.885807037353516\n",
      "loss :  95.55919647216797\n",
      "loss :  109.3199462890625\n",
      "loss :  95.692626953125\n",
      "loss :  39.01797103881836\n",
      "loss :  43.88945770263672\n",
      "loss :  95.49668884277344\n",
      "loss :  109.27970123291016\n",
      "loss :  95.69055938720703\n",
      "loss :  39.02394485473633\n",
      "loss :  43.88801193237305\n",
      "loss :  95.45069122314453\n",
      "loss :  109.2556381225586\n",
      "loss :  95.70286560058594\n",
      "loss :  39.00466537475586\n",
      "loss :  43.892845153808594\n",
      "loss :  95.39665985107422\n",
      "loss :  109.20121002197266\n",
      "loss :  95.68402099609375\n",
      "loss :  39.02142333984375\n",
      "loss :  43.88256072998047\n",
      "loss :  95.34611511230469\n",
      "loss :  109.20233917236328\n",
      "loss :  95.6781234741211\n",
      "loss :  39.025760650634766\n",
      "loss :  43.886680603027344\n",
      "loss :  95.29508209228516\n",
      "loss :  109.182861328125\n",
      "loss :  95.69506072998047\n",
      "loss :  39.00657653808594\n",
      "loss :  43.90370178222656\n",
      "loss :  95.2437515258789\n",
      "loss :  109.11891174316406\n",
      "loss :  95.69129180908203\n",
      "loss :  39.02753448486328\n",
      "loss :  43.910194396972656\n",
      "loss :  95.19332122802734\n",
      "loss :  109.08028411865234\n",
      "loss :  95.68431091308594\n",
      "loss :  39.03524398803711\n",
      "loss :  43.89131164550781\n",
      "loss :  95.14662170410156\n",
      "loss :  109.09729766845703\n",
      "loss :  95.68022155761719\n",
      "loss :  39.003604888916016\n",
      "loss :  43.88100814819336\n",
      "loss :  95.09762573242188\n",
      "loss :  109.03015899658203\n",
      "loss :  95.67947387695312\n",
      "loss :  39.040672302246094\n",
      "loss :  43.912017822265625\n",
      "loss :  95.04912567138672\n",
      "loss :  108.98003387451172\n",
      "loss :  95.69654083251953\n",
      "loss :  39.018402099609375\n",
      "loss :  43.90935134887695\n",
      "loss :  95.00450897216797\n",
      "loss :  108.96121215820312\n",
      "loss :  95.68153381347656\n",
      "loss :  39.04792022705078\n",
      "loss :  43.89181137084961\n",
      "loss :  94.95386505126953\n",
      "loss :  108.96659851074219\n",
      "loss :  95.67212677001953\n",
      "loss :  39.049312591552734\n",
      "loss :  43.87507247924805\n",
      "loss :  94.91230010986328\n",
      "loss :  108.93618774414062\n",
      "loss :  95.69328308105469\n",
      "loss :  39.00892639160156\n",
      "loss :  43.89627456665039\n",
      "loss :  94.870849609375\n",
      "loss :  108.81022644042969\n",
      "loss :  95.71661376953125\n",
      "loss :  39.042701721191406\n",
      "loss :  43.92378616333008\n",
      "loss :  94.8256607055664\n",
      "loss :  108.77347564697266\n",
      "loss :  95.72209930419922\n",
      "loss :  39.016563415527344\n",
      "loss :  43.89631652832031\n",
      "loss :  94.77948760986328\n",
      "loss :  108.76392364501953\n",
      "loss :  95.7118911743164\n",
      "loss :  39.0247802734375\n",
      "loss :  43.876319885253906\n",
      "loss :  94.73961639404297\n",
      "loss :  108.76725006103516\n",
      "loss :  95.72069549560547\n",
      "loss :  39.02731704711914\n",
      "loss :  43.88372039794922\n",
      "loss :  94.6837387084961\n",
      "loss :  108.69529724121094\n",
      "loss :  95.6988754272461\n",
      "loss :  39.06033706665039\n",
      "loss :  43.86626052856445\n",
      "loss :  94.64579010009766\n",
      "loss :  108.73975372314453\n",
      "loss :  95.70191955566406\n",
      "loss :  39.00252914428711\n",
      "loss :  43.86469650268555\n",
      "loss :  94.59431457519531\n",
      "loss :  108.63075256347656\n",
      "loss :  95.67418670654297\n",
      "loss :  39.10641098022461\n",
      "loss :  43.87779235839844\n",
      "loss :  94.55653381347656\n",
      "loss :  108.69104766845703\n",
      "loss :  95.69612884521484\n",
      "loss :  38.98397445678711\n",
      "loss :  43.859710693359375\n",
      "loss :  94.51060485839844\n",
      "loss :  108.55860137939453\n",
      "loss :  95.6766586303711\n",
      "loss :  39.09356689453125\n",
      "loss :  43.882205963134766\n",
      "loss :  94.4790267944336\n",
      "loss :  108.5821533203125\n",
      "loss :  95.7082290649414\n",
      "loss :  38.958683013916016\n",
      "loss :  43.874027252197266\n",
      "loss :  94.43498992919922\n",
      "loss :  108.40703582763672\n",
      "loss :  95.68097686767578\n",
      "loss :  39.06584548950195\n",
      "loss :  43.875118255615234\n",
      "loss :  94.40965270996094\n",
      "loss :  108.53031158447266\n",
      "loss :  95.68994903564453\n",
      "loss :  38.93326187133789\n",
      "loss :  43.833187103271484\n",
      "loss :  94.35774993896484\n",
      "loss :  108.42762756347656\n",
      "loss :  95.63704681396484\n",
      "loss :  39.088661193847656\n",
      "loss :  43.842323303222656\n",
      "loss :  94.33375549316406\n",
      "loss :  108.51834869384766\n",
      "loss :  95.69398498535156\n",
      "loss :  38.891151428222656\n",
      "loss :  43.845726013183594\n",
      "loss :  94.28604125976562\n",
      "loss :  108.27339935302734\n",
      "loss :  95.66434478759766\n",
      "loss :  39.05816650390625\n",
      "loss :  43.87313461303711\n",
      "loss :  94.26270294189453\n",
      "loss :  108.4094009399414\n",
      "loss :  95.6991195678711\n",
      "loss :  38.84593200683594\n",
      "loss :  43.8380126953125\n",
      "loss :  94.21085357666016\n",
      "loss :  108.23568725585938\n",
      "loss :  95.62162017822266\n",
      "loss :  39.08129119873047\n",
      "loss :  43.849727630615234\n",
      "loss :  94.18045043945312\n",
      "loss :  108.4417724609375\n",
      "loss :  95.66263580322266\n",
      "loss :  38.831153869628906\n",
      "loss :  43.83255386352539\n",
      "loss :  94.1217041015625\n",
      "loss :  108.17649841308594\n",
      "loss :  95.61225128173828\n",
      "loss :  39.093650817871094\n",
      "loss :  43.873661041259766\n",
      "loss :  94.09429931640625\n",
      "loss :  108.36035919189453\n",
      "loss :  95.66702270507812\n",
      "loss :  38.7909049987793\n",
      "loss :  43.821170806884766\n",
      "loss :  94.04612731933594\n",
      "loss :  108.10154724121094\n",
      "loss :  95.595703125\n",
      "loss :  39.104766845703125\n",
      "loss :  43.86737060546875\n",
      "loss :  94.0270767211914\n",
      "loss :  108.30743408203125\n",
      "loss :  95.66868591308594\n",
      "loss :  38.76641845703125\n",
      "loss :  43.82649230957031\n",
      "loss :  93.9678726196289\n",
      "loss :  108.00833892822266\n",
      "loss :  95.5848388671875\n",
      "loss :  39.11126708984375\n",
      "loss :  43.856937408447266\n",
      "loss :  93.9532241821289\n",
      "loss :  108.32154083251953\n",
      "loss :  95.67150115966797\n",
      "loss :  38.711082458496094\n",
      "loss :  43.82790756225586\n",
      "loss :  93.88764953613281\n",
      "loss :  107.91259002685547\n",
      "loss :  95.60124969482422\n",
      "loss :  39.13606643676758\n",
      "loss :  43.89945602416992\n",
      "loss :  93.8729248046875\n",
      "loss :  108.21588897705078\n",
      "loss :  95.68528747558594\n",
      "loss :  38.690982818603516\n",
      "loss :  43.81864929199219\n",
      "loss :  93.81097412109375\n",
      "loss :  107.85401153564453\n",
      "loss :  95.57400512695312\n",
      "loss :  39.1649169921875\n",
      "loss :  43.87357711791992\n",
      "loss :  93.79801177978516\n",
      "loss :  108.2063980102539\n",
      "loss :  95.6700210571289\n",
      "loss :  38.65892791748047\n",
      "loss :  43.80324935913086\n",
      "loss :  93.73908996582031\n",
      "loss :  107.77947235107422\n",
      "loss :  95.55793762207031\n",
      "loss :  39.191898345947266\n",
      "loss :  43.88053894042969\n",
      "loss :  93.7339096069336\n",
      "loss :  108.18888854980469\n",
      "loss :  95.67935943603516\n",
      "loss :  38.63163375854492\n",
      "loss :  43.81466293334961\n",
      "loss :  93.6637191772461\n",
      "loss :  107.6722640991211\n",
      "loss :  95.54966735839844\n",
      "loss :  39.241641998291016\n",
      "loss :  43.87276077270508\n",
      "loss :  93.66566467285156\n",
      "loss :  108.2052230834961\n",
      "loss :  95.7100830078125\n",
      "loss :  38.53023147583008\n",
      "loss :  43.796566009521484\n",
      "loss :  93.5879898071289\n",
      "loss :  107.55923461914062\n",
      "loss :  95.56897735595703\n",
      "loss :  39.30009460449219\n",
      "loss :  43.92020797729492\n",
      "loss :  93.60167694091797\n",
      "loss :  108.15591430664062\n",
      "loss :  95.76908874511719\n",
      "loss :  38.44445037841797\n",
      "loss :  43.8255615234375\n",
      "loss :  93.51412200927734\n",
      "loss :  107.36577606201172\n",
      "loss :  95.53417205810547\n",
      "loss :  39.418357849121094\n",
      "loss :  43.896854400634766\n",
      "loss :  93.53836059570312\n",
      "loss :  108.25646209716797\n",
      "loss :  95.72561645507812\n",
      "loss :  38.35151672363281\n",
      "loss :  43.725311279296875\n",
      "loss :  93.4430160522461\n",
      "loss :  107.36074829101562\n",
      "loss :  95.46482849121094\n",
      "loss :  39.578556060791016\n",
      "loss :  43.89870834350586\n",
      "loss :  93.47482299804688\n",
      "loss :  108.29602813720703\n",
      "loss :  95.77330017089844\n",
      "loss :  38.22856903076172\n",
      "loss :  43.74673080444336\n",
      "loss :  93.3740463256836\n",
      "loss :  107.1173095703125\n",
      "loss :  95.47691345214844\n",
      "loss :  39.721248626708984\n",
      "loss :  43.93479919433594\n",
      "loss :  93.43060302734375\n",
      "loss :  108.34680938720703\n",
      "loss :  95.84286499023438\n",
      "loss :  38.086669921875\n",
      "loss :  43.7200927734375\n",
      "loss :  93.3113021850586\n",
      "loss :  106.89561462402344\n",
      "loss :  95.42278289794922\n",
      "loss :  39.96612548828125\n",
      "loss :  43.9053840637207\n",
      "loss :  93.39838409423828\n",
      "loss :  108.46698760986328\n",
      "loss :  95.849365234375\n",
      "loss :  37.933509826660156\n",
      "loss :  43.635467529296875\n",
      "loss :  93.2607650756836\n",
      "loss :  106.66272735595703\n",
      "loss :  95.35862731933594\n",
      "loss :  40.31721878051758\n",
      "loss :  43.93657302856445\n",
      "loss :  93.36874389648438\n",
      "loss :  108.56179809570312\n",
      "loss :  95.98161315917969\n",
      "loss :  37.71297073364258\n",
      "loss :  43.6530876159668\n",
      "loss :  93.2044677734375\n",
      "loss :  106.22106170654297\n",
      "loss :  95.39572143554688\n",
      "loss :  40.64044189453125\n",
      "loss :  44.02267074584961\n",
      "loss :  93.3352279663086\n",
      "loss :  108.65272521972656\n",
      "loss :  96.1310043334961\n",
      "loss :  37.44813537597656\n",
      "loss :  43.59540557861328\n",
      "loss :  93.14044189453125\n",
      "loss :  105.93861389160156\n",
      "loss :  95.35686492919922\n",
      "loss :  41.1337890625\n",
      "loss :  44.04362487792969\n",
      "loss :  93.29876708984375\n",
      "loss :  108.9556884765625\n",
      "loss :  96.2568359375\n",
      "loss :  37.19111251831055\n",
      "loss :  43.52122497558594\n",
      "loss :  93.06932830810547\n",
      "loss :  105.60867309570312\n",
      "loss :  95.33192443847656\n",
      "loss :  41.68873596191406\n",
      "loss :  44.11280822753906\n",
      "loss :  93.25969696044922\n",
      "loss :  109.16773986816406\n",
      "loss :  96.43453216552734\n",
      "loss :  36.95159149169922\n",
      "loss :  43.464263916015625\n",
      "loss :  93.00616455078125\n",
      "loss :  105.21485900878906\n",
      "loss :  95.3193588256836\n",
      "loss :  42.39914321899414\n",
      "loss :  44.18047332763672\n",
      "loss :  93.22503662109375\n",
      "loss :  109.54811096191406\n",
      "loss :  96.62113952636719\n",
      "loss :  36.687530517578125\n",
      "loss :  43.357303619384766\n",
      "loss :  92.9397964477539\n",
      "loss :  104.89453887939453\n",
      "loss :  95.31175231933594\n",
      "loss :  43.14558410644531\n",
      "loss :  44.25838851928711\n",
      "loss :  93.18402862548828\n",
      "loss :  109.81964874267578\n",
      "loss :  96.82252502441406\n",
      "loss :  36.526885986328125\n",
      "loss :  43.257450103759766\n",
      "loss :  92.88831329345703\n",
      "loss :  104.55501556396484\n",
      "loss :  95.32095336914062\n",
      "loss :  43.89934539794922\n",
      "loss :  44.327938079833984\n",
      "loss :  93.14620971679688\n",
      "loss :  110.08505249023438\n",
      "loss :  96.97074127197266\n",
      "loss :  36.37958908081055\n",
      "loss :  43.10740661621094\n",
      "loss :  92.86744689941406\n",
      "loss :  104.37044525146484\n",
      "loss :  95.33505249023438\n",
      "loss :  44.460426330566406\n",
      "loss :  44.39236068725586\n",
      "loss :  93.11029815673828\n",
      "loss :  110.07618713378906\n",
      "loss :  96.9927978515625\n",
      "loss :  36.30939865112305\n",
      "loss :  42.991512298583984\n",
      "loss :  92.8780517578125\n",
      "loss :  104.32176208496094\n",
      "loss :  95.3475341796875\n",
      "loss :  44.58184814453125\n",
      "loss :  44.48336410522461\n",
      "loss :  93.05318450927734\n",
      "loss :  109.80642700195312\n",
      "loss :  96.95315551757812\n",
      "loss :  36.26971435546875\n",
      "loss :  42.97325897216797\n",
      "loss :  92.86371612548828\n",
      "loss :  104.4279556274414\n",
      "loss :  95.33574676513672\n",
      "loss :  44.22259521484375\n",
      "loss :  44.501380920410156\n",
      "loss :  92.96907043457031\n",
      "loss :  109.43807220458984\n",
      "loss :  96.80665588378906\n",
      "loss :  36.329383850097656\n",
      "loss :  42.957000732421875\n",
      "loss :  92.8398666381836\n",
      "loss :  104.69871520996094\n",
      "loss :  95.2957763671875\n",
      "loss :  43.553382873535156\n",
      "loss :  44.42893600463867\n",
      "loss :  92.9000244140625\n",
      "loss :  109.0496597290039\n",
      "loss :  96.63601684570312\n",
      "loss :  36.421878814697266\n",
      "loss :  43.0107536315918\n",
      "loss :  92.8130874633789\n",
      "loss :  104.91173553466797\n",
      "loss :  95.28607177734375\n",
      "loss :  42.85219955444336\n",
      "loss :  44.37666702270508\n",
      "loss :  92.84329986572266\n",
      "loss :  108.65442657470703\n",
      "loss :  96.4306411743164\n",
      "loss :  36.5885009765625\n",
      "loss :  43.08967971801758\n",
      "loss :  92.779296875\n",
      "loss :  105.14566040039062\n",
      "loss :  95.27034759521484\n",
      "loss :  42.13185119628906\n",
      "loss :  44.26782989501953\n",
      "loss :  92.79326629638672\n",
      "loss :  108.33057403564453\n",
      "loss :  96.26702880859375\n",
      "loss :  36.781585693359375\n",
      "loss :  43.16862487792969\n",
      "loss :  92.7408218383789\n",
      "loss :  105.32264709472656\n",
      "loss :  95.27664947509766\n",
      "loss :  41.516170501708984\n",
      "loss :  44.17869186401367\n",
      "loss :  92.75284576416016\n",
      "loss :  108.04763793945312\n",
      "loss :  96.130126953125\n",
      "loss :  36.97890090942383\n",
      "loss :  43.232112884521484\n",
      "loss :  92.70390319824219\n",
      "loss :  105.45672607421875\n",
      "loss :  95.28291320800781\n",
      "loss :  41.06560516357422\n",
      "loss :  44.112579345703125\n",
      "loss :  92.7143325805664\n",
      "loss :  107.76053619384766\n",
      "loss :  96.04523468017578\n",
      "loss :  37.16922378540039\n",
      "loss :  43.31465530395508\n",
      "loss :  92.6624984741211\n",
      "loss :  105.51942443847656\n",
      "loss :  95.3108139038086\n",
      "loss :  40.6705322265625\n",
      "loss :  44.035587310791016\n",
      "loss :  92.6759033203125\n",
      "loss :  107.5792465209961\n",
      "loss :  95.9413070678711\n",
      "loss :  37.34635925292969\n",
      "loss :  43.32830047607422\n",
      "loss :  92.62300872802734\n",
      "loss :  105.63238525390625\n",
      "loss :  95.29597473144531\n",
      "loss :  40.42888641357422\n",
      "loss :  43.95670700073242\n",
      "loss :  92.64073944091797\n",
      "loss :  107.43717193603516\n",
      "loss :  95.91033172607422\n",
      "loss :  37.45230484008789\n",
      "loss :  43.384029388427734\n",
      "loss :  92.58308410644531\n",
      "loss :  105.58589935302734\n",
      "loss :  95.33243560791016\n",
      "loss :  40.21950149536133\n",
      "loss :  43.922420501708984\n",
      "loss :  92.61028289794922\n",
      "loss :  107.32561492919922\n",
      "loss :  95.87401580810547\n",
      "loss :  37.544830322265625\n",
      "loss :  43.41107940673828\n",
      "loss :  92.54242706298828\n",
      "loss :  105.57154083251953\n",
      "loss :  95.33865356445312\n",
      "loss :  40.09201431274414\n",
      "loss :  43.88321304321289\n",
      "loss :  92.5761489868164\n",
      "loss :  107.23332214355469\n",
      "loss :  95.85038757324219\n",
      "loss :  37.605587005615234\n",
      "loss :  43.412879943847656\n",
      "loss :  92.50341033935547\n",
      "loss :  105.549072265625\n",
      "loss :  95.34260559082031\n",
      "loss :  40.023193359375\n",
      "loss :  43.86787796020508\n",
      "loss :  92.53887176513672\n",
      "loss :  107.1093978881836\n",
      "loss :  95.8527603149414\n",
      "loss :  37.62578201293945\n",
      "loss :  43.42340087890625\n",
      "loss :  92.46784973144531\n",
      "loss :  105.50023651123047\n",
      "loss :  95.34078979492188\n",
      "loss :  39.979217529296875\n",
      "loss :  43.83734893798828\n",
      "loss :  92.50862121582031\n",
      "loss :  107.0935287475586\n",
      "loss :  95.83656311035156\n",
      "loss :  37.644432067871094\n",
      "loss :  43.42194366455078\n",
      "loss :  92.43116760253906\n",
      "loss :  105.43612670898438\n",
      "loss :  95.32999420166016\n",
      "loss :  39.96889114379883\n",
      "loss :  43.8159065246582\n",
      "loss :  92.48170471191406\n",
      "loss :  107.04476165771484\n",
      "loss :  95.82953643798828\n",
      "loss :  37.65592575073242\n",
      "loss :  43.4209098815918\n",
      "loss :  92.39665985107422\n",
      "loss :  105.35009765625\n",
      "loss :  95.33149719238281\n",
      "loss :  39.97133255004883\n",
      "loss :  43.80963134765625\n",
      "loss :  92.4516372680664\n",
      "loss :  106.99727630615234\n",
      "loss :  95.8405990600586\n",
      "loss :  37.634883880615234\n",
      "loss :  43.41292953491211\n",
      "loss :  92.35980224609375\n",
      "loss :  105.27552795410156\n",
      "loss :  95.31970977783203\n",
      "loss :  40.0214958190918\n",
      "loss :  43.8038330078125\n",
      "loss :  92.4208755493164\n",
      "loss :  107.00547790527344\n",
      "loss :  95.87606811523438\n",
      "loss :  37.562923431396484\n",
      "loss :  43.40741729736328\n",
      "loss :  92.32330322265625\n",
      "loss :  105.14869689941406\n",
      "loss :  95.31385803222656\n",
      "loss :  40.099769592285156\n",
      "loss :  43.80990982055664\n",
      "loss :  92.39840698242188\n",
      "loss :  107.02496337890625\n",
      "loss :  95.87776184082031\n",
      "loss :  37.515987396240234\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "optim = Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 200\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for temp in range(len(X_train) // batch_size):\n",
    "        \n",
    "        s = temp * batch_size # 0 * 100\n",
    "        e = s + batch_size    # 100\n",
    "        \n",
    "        X = X_train[s:e]\n",
    "        y = y_train[s:e]\n",
    "    \n",
    "        optim.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        print('loss : ', loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
